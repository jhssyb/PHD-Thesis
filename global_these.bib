@book{pilkey,
author = {Pilkey, Orrin H. and Pilkey-Jarvis, Linda},
title = {{Useless Arithmetic: Why Environmental Scientists Can't Predict the Future}},
year = {2006}
}

@book{Goodfellow2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{Terraz2017,
author = {Terraz, Th{\'{e}}ophile and Ribes, Alejandro and Fournier, Yvan and Iooss, Bertrand and Raffin, Bruno},
doi = {10.1145/3126908.3126922},
isbn = {9781450351140},
journal = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis on - SC '17},
pages = {1--14},
title = {{Melissa: large scale in transit sensitivity analysis avoiding intermediate files}},
year = {2017}
}

@article{Wang2015,
archivePrefix = {arXiv},
arxivId = {1501.03189},
author = {Wang, Jian-xun and Roy, Christopher J. and Xiao, Heng},
doi = {10.1115/1.4037452},
eprint = {1501.03189},
issn = {2332-9017},
keywords = {aerospace,probability,uncertainty},
number = {c},
title = {{Propagation of Input Uncertainty in Presence of Model-Form Uncertainty: A Multi-fidelity Approach for CFD Applications}},
url = {http://arxiv.org/abs/1501.03189},
year = {2015}
}

@article{Saltelli2017,
author = {Saltelli, Andrea},
number = {March},
title = {{A new sample-based algorithms to compute the total sensitivity index}},
year = {2017}
}

@inproceedings{Wilson2015,
author = {Wilson, Andrew Gordon},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
file = {:Users/roy/Documents/Knowledge/Bibliographie/2015/Wilson{\_}Kernel Interpolation for Scalable Structured Gaussian Processes ( KISS-GP ).pdf:pdf},
title = {{Kernel Interpolation for Scalable Structured Gaussian Processes ( KISS-GP )}},
volume = {37},
year = {2015}
}

@inproceedings{Dupuis2018,
address = {Reston, Virginia},
author = {Dupuis, Romain and Jouhaud, Jean-Christophe and Sagaut, Pierre},
booktitle = {2018 AIAA/ASCE/AHS/ASC Structures, Structural Dynamics, and Materials Conference},
doi = {10.2514/6.2018-1905},
isbn = {978-1-62410-532-6},
month = {jan},
number = {January},
pages = {1--23},
publisher = {American Institute of Aeronautics and Astronautics},
title = {{Aerodynamic Data Predictions for Transonic Flows via a Machine-Learning-based Surrogate Model}},
year = {2018}
}

@article{Fernandez2017,
author = {Fernandez, Giselle and Park, Chanyoung and Kim, Nam H and Haftka, Raphael},
file = {:Users/roy/Documents/Knowledge/Bibliographie/2017/Fernandez et al.{\_}Review of multi-fidelity models.pdf:pdf},
number = {March},
title = {{Review of multi-fidelity models}},
year = {2017}
}

@article{Becker2018,
abstract = {Sensitivity analysis is an essential tool in the development of robust models for engineering, physical sciences, economics and policy-making, but typically requires running the model a large number of times in order to estimate sensitivity measures. While statistical emulators allow sensitivity analysis even on complex models, they only perform well with a moderately low number of model inputs: in higher dimensional problems they tend to require a restrictively high number of model runs unless the model is relatively linear. Therefore, an open question is how to tackle sensitivity problems in higher dimensionalities, at very low sample sizes. This article examines the relative performance of four sampling-based measures which can be used in such high-dimensional nonlinear problems. The measures tested are the Sobol' total sensitivity indices, the absolute mean of elementary effects, a derivative-based global sensitivity measure, and a modified derivative-based measure. Performance is assessed in a ‘screening' context, by assessing the ability of each measure to identify influential and non-influential inputs on a wide variety of test functions at different dimensionalities. The results show that the best-performing measure in the screening context is dependent on the model or function, but derivative-based measures have a significant potential at low sample sizes that is currently not widely recognised.},
author = {Becker, W. E. and Tarantola, S. and Deman, G.},
doi = {10.1080/00949655.2018.1450876},
file = {:Users/roy/Documents/Knowledge/Bibliographie/2018/Becker, Tarantola, Deman{\_}Sensitivity analysis approaches to high-dimensional screening problems at low sample size.pdf:pdf},
issn = {15635163},
journal = {Journal of Statistical Computation and Simulation},
keywords = {Derivative-based global sensitivity measures,Sensitivity analysis,Sobol' indices,elementary effects,function,low-discrepancy sequences,screening},
number = {11},
pages = {2089--2110},
publisher = {Taylor {\&} Francis},
title = {{Sensitivity analysis approaches to high-dimensional screening problems at low sample size}},
url = {https://doi.org/00949655.2018.1450876},
volume = {88},
year = {2018}
}


@article{Kolmogorov1941,
journal = {Doklady Akademiia Nauk SSSR},
volume = {30},
author = {Kolmogorov, A. N},
pages = {301-305},
title = {{The local structure of turbulence in incompressible viscous fluid for very large reynolds numbers}},
year = {1941}
}


@article{Plischke2012,
abstract = {We consider correlation ratios as estimators for first order sensitivity indices from given data. The computation is simplified by the introduction of the cumulative sum of the normalised reordered output. Ideas for the estimation using interpolation are also discussed. {\textcopyright} 2011 Elsevier Ltd.},
author = {Plischke, Elmar},
doi = {10.1016/j.ress.2011.12.007},
file = {:Users/roy/Documents/Knowledge/Bibliographie/2012/Plischke{\_}An adaptive correlation ratio method using the cumulative sum of the reordered output.pdf:pdf},
issn = {09518320},
journal = {Reliability Engineering and System Safety},
keywords = {Correlation ratio,Global sensitivity analysis,Sobol index},
pages = {149--156},
publisher = {Elsevier},
title = {{An adaptive correlation ratio method using the cumulative sum of the reordered output}},
url = {http://dx.doi.org/10.1016/j.ress.2011.12.007},
volume = {107},
year = {2012}
}

@article{borgonovo2007,
abstract = {Uncertainty in parameters is present in many risk assessment problems and leads to uncertainty in model predictions. In this work, we introduce a global sensitivity indicator which looks at the influence of input uncertainty on the entire output distribution without reference to a specific moment of the output (moment independence) and which can be defined also in the presence of correlations among the parameters. We discuss its mathematical properties and highlight the differences between the present indicator, variance-based uncertainty importance measures and a moment independent sensitivity indicator previously introduced in the literature. Numerical results are discussed with application to the probabilistic risk assessment model on which Iman [A matrix-based approach to uncertainty and sensitivity analysis for fault trees. Risk Anal 1987;7(1):22-33] first introduced uncertainty importance measures. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
author = {Borgonovo, E.},
doi = {10.1016/j.ress.2006.04.015},
file = {:Users/roy/Documents/Knowledge/Bibliographie/2007/Borgonovo{\_}A new uncertainty importance measure.pdf:pdf},
isbn = {0951-8320},
issn = {09518320},
journal = {Reliability Engineering and System Safety},
keywords = {Global sensitivity analysis,Importance measures,Probabilistic risk assessment,Uncertainty analysis,Uncertainty importance measures},
number = {6},
pages = {771--784},
title = {{A new uncertainty importance measure}},
volume = {92},
year = {2007}
}

@article{gicquel2011,
    Author = {L.Y.M. Gicquel and N. Gourdain and J.-F. Boussuge and H. Deniau and G. Staffelbach and P. Wolf and T. Poinsot},
    Journal = {Comptes Rendus M\'ecanique},
    Keywords = {AVBP, elsA, High Performance Computing},
    Number = {2-3},
    Pages = {104 - 124},
    Title = {High performance parallel computing of flows in complex geometries},
    Volume = {339},
    Year = {2011}}

@misc{quartapelle1993,
    Author = {L. Quartapelle and V. Selmin},
    Booktitle = {Finite Elements Fluids},
    Date-Added = {2007-04-19 18:45:58 +0200},
    Date-Modified = {2013-09-30 09:23:39 +0000},
    Keywords = {Numerics},
    Title = {High-order {T}aylor-{G}alerkin methods for non-linear multidimensional problems.},
    Year = {1993}}

@inproceedings{baudin2016,
address = {R{\'{e}}union Island},
author = {Baudin, Micha{\"{e}}l and Boumhaout, Khalid and Delage, Thibault and Iooss, Bertrand and Martinez, Jean-Marc},
booktitle = {8th International Conference on Sensitivity Analysis of Model Output,},
title = {{Numerical stability of Sobol' indices estimation formula}},
year = {2016}
}

@incollection{cavazzuti2013,
author = {Cavazzuti, Marco},
booktitle = {Optimization Methods: From Theory to Design},
doi = {10.1007/978-3-642-31187-1\_2},
pages = {13--42},
publisher = {Springer Berlin Heidelberg},
title = {{Design of Experiments}},
year = {2013}
}

@article{chatterjee2000,
author = {{Anindya Chatterjee}},
journal = {Current Science},
number = {7},
title = {{An introduction to the proper orthogonal decomposition}},
volume = {78},
year = {2000}
}

@article{damblin2013,
author = {Damblin, Guillaume and Couplet, Mathieu and Iooss, Bertrand and Damblin, Guillaume and Couplet, Mathieu and Iooss, Bertrand},
journal = {Journal of Simulation},
keywords = {Latin Hypercube Sampling,computer experiment,discrepancy,optimal design},
pages = {276--289},
title = {{Numerical studies of space filling designs : optimization of Latin Hypercube Samples and subprojection properties}},
year = {2013}
}

@article{ferretti2016,
author = {Ferretti, Federico and Saltelli, Andrea and Tarantola, Stefano},
doi = {10.1016/j.scitotenv.2016.02.133},
journal = {Science of The Total Environment},
keywords = {Bibliometric analysis,Chemical modelling,Global sensitivity analysis,Sensitivity analysis},
month = {oct},
pages = {666--670},
publisher = {British Geological Survey, NERC},
title = {{Trends in sensitivity analysis practice in the last decade}},
volume = {568},
year = {2016}
}

@article{forrester2009,
author = {Forrester, Alexander I J and Keane, Andy J.},
doi = {10.1016/j.paerosci.2008.11.001},
journal = {Progress in Aerospace Sciences},
number = {1-3},
pages = {50--79},
title = {{Recent advances in surrogate-based optimization}},
volume = {45},
year = {2009}
}

@article{iooss2010,
author = {Iooss, B. and Boussouf, L. and Feuillard, V. and Marrel, A.},
journal = {International Journal on Advances in Systems and Measurements},
number = {1},
pages = {11--21},
title = {{Numerical studies of the metamodel fitting and validation processes}},
volume = {3},
year = {2010}
}

@incollection{iooss2016,
author = {Iooss, B. and Saltelli, A.},
booktitle = {Handbook of Uncertainty Quantification},
doi = {10.1007/978-3-319-11259-6\_31-1},
keywords = {Computer experiments,Impact assessment,Risk assessment,Sensitivity analysis,Sensitivity auditing,Uncertainty analysis},
pages = {1--20},
publisher = {Springer International Publishing},
title = {{Introduction to Sensitivity Analysis}},
year = {2016}
}

@article{jones1998,
author = {Jones, Donald R and Schonlau, Matthias and William, J},
doi = {10.1023/a:1008306431147},
journal = {Journal of Global Optimization},
title = {{Efficient Global Optimization of Expensive Black-Box Functions}},
volume = {13},
year = {1998}
}

@inproceedings{kohavi1995,
author = {Kohavi, Ron},
booktitle = {International Joint Conference on Artificial Intelligence},
title = {{A Study of Cross-Validation and Bootstrap for Accuracy Estimaiton and Model Selection}},
year = {1995}
}

@article{krige1989,
author = {Krige, D G and Guarascio, M and Camisani-Calzolari, F A},
journal = {Geostatistics},
pages = {1--19},
title = {{Early South African geostatistical techniques in today's perspective}},
volume = {1},
year = {1989}
}

@article{marrel2009,
title = {Calculations of Sobol indices for the Gaussian process metamodel},
journal = {Reliability Engineering \& System Safety},
volume = {94},
number = {3},
pages = {742 - 751},
year = {2009},
doi = {http://dx.doi.org/10.1016/j.ress.2008.07.008},
author = {Amandine Marrel and Bertrand Iooss and Beatrice Laurent and Olivier Roustant}
}

@article{marrel2012,
author = {Marrel, Amandine and Iooss, Bertrand and {Da Veiga}, S{\'{e}}bastien and Ribatet, Mathieu},
doi = {10.1007/s11222-011-9274-8},
journal = {Statistics and Computing},
month = {may},
number = {3},
pages = {833--847},
title = {{Global sensitivity analysis of stochastic computer models with joint metamodels}},
volume = {22},
year = {2012}
}

@incollection{marrel2015,
author = {Marrel, Amandine and Saint-Geours, Nathalie and {De Lozzo}, Matthias},
booktitle = {Handbook of Uncertainty Quantification},
doi = {10.1007/978-3-319-11259-6\_39-1},
isbn = {978-3-319-11259-6},
pages = {1--31},
publisher = {Springer International Publishing},
title = {{Sensitivity Analysis of Spatial and/or Temporal Phenomena}},
year = {2015}
}

@book{rasmussen2006,
author = {Rasmussen, C.E. and Williams, C},
publisher = {MIT Press},
title = {{Gaussian processes for machine learning}},
year = {2006}
}

@article{sirovich1987,
author = {Sirovich, Lawrence},
journal = {Quarterly of Applied Mathematics},
number = {3},
pages = {561--571},
title = {{Turbulence and the dynamics of coherent structures part i: coherent structures}},
volume = {XLV},
year = {1987}
}

@article{sobol1993,
author = {Sobol', I.M},
journal = {Mathematical Modeling and Computational Experiment},
keywords = {Anova,Sobol' sensitivity indices,Sobol' variance decomposition},
number = {4},
pages = {407--414},
title = {{Sensitivity analysis for nonlinear mathematical models}},
volume = {1},
year = {1993}
}

@misc{schonlau1998,
author = {Schonlau, Matthias and Welch, William J and Jones, Donald R},
booktitle = {New Developments and Applications in Experimental Design},
pages = {11--25},
title = {{Global Versus Local Search in Constrained Optimization of Computer Models}},
volume = {34},
year = {1998}
}

@article{campet2018,
  author =  {Campet, R. AND Zhu, M. AND Riber, E. AND Cuenot, B.},
  title =   {Large Eddy Simulation of a Single-Started Ribbed Tube with heat Transfer},
  journal = {International Journal of Heat and Mass Transfer},
  year =    {2018}
}

@Article{perry1969,
  author =  {Perry, A. E. AND Schofield, W. H. AND Joubert, P. N.},
  title =   {Rough wall turbulent boundary layers},
  journal = {Journal of Fluid Mechanics},
  year =    {1969},
  volume =  {37},
  number =  {2},
  pages =   {383-413},
  month =   {June},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/perry1969.pdf:PDF}
}

@Article{jimenez2004,
  author =  {Jimenez, J.},
  title =   {Turbulent Flow over Rough Walls},
  journal = {Annual Review of Fluid Mechanics},
  year =    {2004},
  volume =  {36},
  pages =   {173-196},
  month =   {January},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/Jimenez2004.pdf:PDF}
}

@Article{nagano2004,
  author =  {Nagano, Y. AND Hattori, H. AND Yasui, S. Y. AND Houra, T.},
  title =   {DNS of Velocity and Thermal Field in Turbulent Channel Flow with Transverse-Rib Roughness},
  journal = {International Journal of Heat and Fluid Flow},
  year =    {2004},
  volume =  {25},
  number =  {3},
  pages =   {393-403},
  month =   {June},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/nagano2004.pdf:PDF}
}

@Article{ravigururajan1996,
  author =  {Ravigururajan, T. S. AND Bergles, A. E.},
  title =   {Development and Verification of General Correlations for Pressure Drop and Heat Transfer in Single-Phase Turbulent Flow in Enhanced Tubes},
  journal = {Experimental Thermal and Fluid Science},
  year =    {1996},
  volume =  {13},
  number =  {1},
  pages =   {55-70},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/ravigururajan1996.pdf:PDF}
}

@PhdThesis{Zhu2015,
  author =    {Zhu, M.},
  title =     {Large Eddy Simulation of thermal cracking in petroleum industry},
  school =    {Institut National Polytechnique de Toulouse},
  year =      {2015},
  note =      {Simulation aux grandes {\'e}chelles du craquage thermique dans l'industrie p{\'e}trochimique},
  file =      {:/Users/campet/Documents/Biblio/Thesis_Zhu_2015.pdf:PDF},
  publisher = {{\'E}cole Doctorale M{\'e}canique, {\'E}nerg{\'e}tique, G{\'e}nie civil et Proc{\'e}d{\'e}s (Toulouse); 154236012},
  review =    {PhD Manqi Zhu}
}

@Article{rogallo1984,
  author =  {Rogallo, R. S. AND Moin, P.},
  title =   {Numerical Simulation of Turbulent Flows},
  journal = {Annual Review of Fluid Mechanics},
  year =    {1984},
  volume =  {16},
  pages =   {99-137},
  month =   {January},
  file =    {:/Users/campet/Documents/Biblio/Smooth Tubes/rogallo1984.pdf:PDF}
}

@Article{kim1987,
  author =  {Kim, J. AND Moin, P. AND Moser, R. D.},
  title =   {Turbulence statistics in fully developed channel flow at low Reynolds number},
  journal = {Journal of Fluid Mechanics},
  year =    {1987},
  volume =  {177},
  pages =   {133-166},
  month =   {April},
  file =    {:/Users/campet/Documents/Biblio/Smooth Tubes/KMM-1987.pdf:PDF}
}

@Article{JimenezMoin1991,
  author =    {Jimenez, J. and Moin, P.},
  title =     {The minimal flow unit in near-wall turbulence},
  journal =   {Journal of Fluid Mechanics},
  year =      {1991},
  volume =    {225},
  pages =     {213-240},
  file =      {:/Users/campet/Documents/Biblio/Ribbed Tubes/Jimenez1990.pdf:PDF},
  publisher = {Cambridge Univ Press},
  review =    {Study of the minimal channel lenght in order to capture all the turbulent structures in LES.
Depends on the Reynolds.
Applications to the periodic channel (or pipe)}
}

@Article{webb1972,
  author =  {Webb, R. L. AND Eckert, E. R. G.},
  title =   {Application of Rough Surfaces to Heat Exchanger Design},
  journal = {International Journal of Heat and Mass Transfer},
  year =    {1972},
  volume =  {15},
  number =  {9},
  pages =   {1647-1658},
  month =   {September},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/webb1972.pdf:PDF}
}

@Article{GarciaSolanoVicenteEtAl2012,
  author =    {Garcia, A. and Solano, J.P. and Vicente, P.G. and Viedma, A.},
  title =     {The influence of artificial roughness shape on heat transfer enhancement: Corrugated tubes, dimpled tubes and wire coils},
  journal =   {Applied Thermal Engineering},
  year =      {2012},
  volume =    {35},
  pages =     {196-201},
  month =     {March},
  file =      {:/Users/campet/Documents/Biblio/Ribbed Tubes/garcia2011.pdf:PDF},
  publisher = {Elsevier}
}

@Article{aliaga1994,
  author =   {Aliaga, D. A. AND Lamb, J. P. AND Klein, D. E.},
  title =    {Convection heat transfer distributions over plates with square ribs from infrared thermography measurements},
  journal =  {International Journal of Hear and Mass Transfer},
  year =     {1994},
  volume =   {37},
  number =   {3},
  pages =    {363-374},
  month =    {February},
  file =     {:/Users/campet/Documents/Biblio/Ribbed Tubes/aliaga1994.pdf:PDF},
  keywords = {experiment, heat transfer, transverse rib}
}

@Article{cheng2006,
  author =   {Cheng, L. AND Chen, T.},
  title =    {Study of Single Phase Flow Heat Transfer and Friction Pressure Drop in a Spiral Internally Ribbed Tube},
  journal =  {Chemical Engineering \& Technology},
  year =     {2006},
  file =     {:/Users/campet/Documents/Biblio/Ribbed Tubes/cheng2006.pdf:PDF},
  keywords = {experiment, heat transfer, helical rib}
}

@Article{gee1980,
  author =  {Gee, D. L. AND Webb, R. L.},
  title =   {Forced convection heat transfer in helically rib-roughened tubes},
  journal = {Journal of heat transfer},
  year =    {1980},
  volume =  {23},
  number =  {8},
  pages =   {1127-1136},
  month =   {August},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/gee1980.pdf:PDF}
}

@Conference{Mayo2016,
  author =        {Mayo, I. AND Cernat, B. C. AND Virgilio, M. AND Pappa, A. AND Arts, T.},
  title =         {Aerothermal investigation on the flow and heat transfer in a helically corrugated cooling channel},
  booktitle =     {ASME Turbo Expo 2016: Turbine Technical Conference and Exposition},
  year =          {2016},
  date-added =    {2016-10-17 16:23:47 +0000},
  date-modified = {2016-10-17 16:37:00 +0000},
  file =          {:/Users/campet/Documents/Biblio/Ribbed Tubes/Mayo2016.pdf:PDF}
}

@Article{webb1971,
  author =  {Webb, R. L. AND Eckert, E. R. G. AND Goldstein, R. J.},
  title =   {Heat Transfer and Friction in Tubes with Repeated-Rib Roughness},
  journal = {International Journal of Heat and Mass Transfer},
  year =    {1971},
  volume =  {14},
  number =  {4},
  pages =   {601-617},
  month =   {April},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/webb1971.pdf:PDF}
}

@Article{VicenteGarciaViedma2002,
  author =    {Vicente, Pedro G and Garc{\i} a, Alberto and Viedma, Antonio},
  title =     {Heat transfer and pressure drop for low Reynolds turbulent flow in helically dimpled tubes},
  journal =   {International journal of heat and mass transfer},
  year =      {2002},
  volume =    {45},
  number =    {3},
  pages =     {543--553},
  file =      {:Biblio/Ribbed Tubes/vicente2002.pdf:PDF},
  publisher = {Elsevier}
}

@Article{VicenteGarciaViedma2004,
  author =    {Vicente, P.G. and Garcia, A. and Viedma, A.},
  title =     {Experimental investigation on heat transfer and frictional characteristics of spirally corrugated tubes in turbulent flow at different Prandtl numbers},
  journal =   {International Journal of Heat and Mass Transfer},
  year =      {2004},
  volume =    {47},
  number =    {4},
  pages =     {671-681},
  file =      {:/Users/campet/Documents/Biblio/Ribbed Tubes/vicente2004.pdf:PDF},
  publisher = {Elsevier}
}

@Article{jordan2003,
  author =  {Jordan, S. A.},
  title =   {The turbulent character and pressure loss produced by periodic symmetric ribs in a circular duct},
  journal = {International Journal of Heat and Fluid Flow},
  year =    {2003},
  volume =  {24},
  number =  {6},
  pages =   {795-806},
  month =   {December},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/jordan2003.pdf:PDF}
}

@Article{vijiapurapu2007,
  author =  {Vijiapurapu, S. V. AND Cui, J.},
  title =   {Simulation of Turbulent Flow in a Ribbed Pipe Using Large Eddy Simulation},
  journal = {Numerical Heat Transfer Part A},
  year =    {2007},
  volume =  {51},
  number =  {12},
  pages =   {1137-1165},
  month =   {September},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/vijiapurapu2007.pdf:PDF}
}

@Article{vijiapurapu2010,
  author =   {Vijiapurapu, S. AND Cui, J.},
  title =    {Performance of turbulence models for flows through rough pipes},
  journal =  {Applied Mathematical Modelling},
  year =     {2010},
  volume =   {34},
  number =   {6},
  pages =    {1458-1466},
  month =    {June},
  abstract = {Reynolds-Averaged Navier Stokes},
  file =     {:/Users/campet/Documents/Biblio/Ribbed Tubes/vijiapurapu2010.pdf:PDF}
}

@Article{liou1993,
  author =  {Liou, T.M. AND Hwang, J.J. AND Chen, S.H.},
  title =   {Simulation and measurement of enhanced turbulent heat transfer in a channel with periodic ribs on one principal wall},
  journal = {International Journal of Heat and Mass Transfer},
  year =    {1993},
  volume =  {36},
  number =  {2},
  pages =   {507-517},
  month =   {January},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/liou1993.pdf:PDF}
}

@Article{liou2002,
  author =  {Liou, T.M. AND Chen, S.H. AND Shih, K.C.},
  title =   {Numerical simulation of turbulent flow field and heat transfer in a two-dimensional channel with periodic slit ribs},
  journal = {International Journal of Heat and Mass Transfer},
  year =    {2002},
  volume =  {45},
  number =  {22},
  pages =   {4493-4505},
  month =   {October},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/liou2002.pdf:PDF}
}

@Article{liu2001,
  author =  {Liu, X. AND Jensen, M.K.},
  title =   {Geometry Effects on Turbulent Flow and Heat Transfer in Internally Finned Tubes},
  journal = {Journal of Heat Transfer},
  year =    {2001},
  volume =  {123},
  number =  {6},
  pages =   {1035-1044},
  month =   {May},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/liu2001.pdf:PDF}
}

@Article{shub1993,
  author =   {Shub, L.I.},
  title =    {Calculation of turbulent flow and heat transfer in a tube with a periodically varying cross-section},
  journal =  {International Journal of Heat Transfer},
  year =     {1993},
  volume =   {36},
  number =   {4},
  pages =    {1085-1095},
  month =    {March},
  abstract = {Reynolds-Averaged Navier Stokes},
  file =     {:/Users/campet/Documents/Biblio/Ribbed Tubes/shub1993.pdf:PDF}
}

@Article{iaccarino2002,
  author =  {Iaccarino, G. AND Ooi, A. AND Durbin, P.A. AND Behnia, M.},
  title =   {Conjugate heat transfer predictions in two-dimensional ribbed passages},
  journal = {International Journal of Heat and Fluid Flow},
  year =    {2002},
  volume =  {23},
  number =  {3},
  pages =   {340-345},
  month =   {June},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/iaccarino2002.pdf:PDF}
}

@Article{ooi2002,
  author =  {Ooi, A. AND Iaccarino, G. AND Durbin, P.A. AND Behnia, M.},
  title =   {Reynolds averaged simulation of flow and heat transfer in ribbed ducts},
  journal = {International Journal of Heat and Fluid Flow},
  year =    {2002},
  volume =  {23},
  number =  {6},
  pages =   {750-757},
  month =   {December},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/ooi2002.pdf:PDF}
}

@Article{kim_hm2004,
  author =  {Kim, H.M. AND Kim, K.Y.},
  title =   {Design optimization of rib-roughened channel to enhance turbulent heat transfer},
  journal = {International Journal of Heat and Mass Transfer},
  year =    {2004},
  volume =  {47},
  number =  {23},
  pages =   {5159-5168},
  month =   {November},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/kim_hm2004.pdf:PDF}
}

@Article{kim2004,
  author =  {Kim, J.H. AND Jansen, K.E. AND Jensen, M.K.},
  title =   {Simulation of Three-Dimensional Incompressible Turbulent Flow Inside Tubes with Helical Fins},
  journal = {Numerical Heat Transfer Part B},
  year =    {2004},
  volume =  {46},
  number =  {3},
  pages =   {195-221},
  month =   {January},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/kim2004.pdf:PDF}
}

@Article{ryu2007_a,
  author =  {Ryu, D.N. AND Choi, D.H. AND Patel, V.C.},
  title =   {Analysis of turbulent flow in channels roughened by two-dimensional ribs and three-dimensional blocks. Part I: Resistance},
  journal = {International Journal of Heat and Fluid Flow},
  year =    {2007},
  volume =  {28},
  number =  {5},
  pages =   {1098-1111},
  month =   {October},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/ryu2007_a.pdf:PDF}
}

@Article{ryu2007_b,
  author =  {Ryu, D.N. AND Choi, D.H. AND Patel, V.C.},
  title =   {Analysis of turbulent flow in channels roughened by two-dimensional ribs and three-dimensional blocks. Part II: Heat transfer},
  journal = {International Journal of heat and Fluid Flow},
  year =    {2007},
  volume =  {28},
  number =  {5},
  pages =   {1112-1124},
  month =   {October},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/ryu2007.pdf:PDF}
}

@Article{kamali2008,
  author =  {Kamali, R. AND Binesh, A.R.},
  title =   {The importance of rib shape effects on the local heat transfer and flow friction characteristics of square ducts with ribbed internal surfaces},
  journal = {International Communications in Heat and Mass Transfer},
  year =    {2008},
  volume =  {35},
  number =  {8},
  pages =   {1032-1040},
  month =   {October},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/kamali2008.pdf:PDF}
}

@Article{eiamsaard2008,
  author =  {Eiamsa-ard, S. AND Promvonge, P.},
  title =   {Numerical study on heat transfer of turbulent channel flow over periodic grooves},
  journal = {International Communications in Heat and Mass Transfer},
  year =    {2008},
  volume =  {35},
  number =  {7},
  pages =   {844-852},
  month =   {August},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/eiamsaard2008.pdf:PDF}
}

@Article{agra2011,
  author =   {Agra, O. AND Demir, H. AND Atayilmaz, S.O. AND Kantas, F. AND Dalkilic, A.S.},
  title =    {Numerical investigation of heat transfer and pressure drop in enhanced tubes},
  journal =  {International Communications in Heat and Mass Transfer},
  year =     {2011},
  volume =   {38},
  number =   {10},
  pages =    {1384-1391},
  month =    {December},
  file =     {:/Users/campet/Documents/Biblio/Ribbed Tubes/agra2011.pdf:PDF},
  keywords = {RANS, heat transfer, helical rib}
}

@Article{ma2012,
  author =  {Ma, T. AND Wang, Q.W. AND Zeng, M. AND Chen, Y.T. AND Liu, Y. AND Nagarajan, V.},
  title =   {Study on heat transfer and pressure drop performances of ribbed channel in the high temperature heat exchanger},
  journal = {Applied Energy},
  year =    {2012},
  volume =  {99},
  pages =   {393-401},
  month =   {November},
  file =    {:/Users/campet/Documents/Biblio/Ribbed Tubes/ma2012.pdf:PDF}
}

@Article{PetukhovPopov1963,
  author =  {Petukhov, B.S. and Popov, V.N.},
  title =   {Theoretical calculation of heat exchange and frictional resistance in turbulent flow in tubes of an incompressible fluid with variable physical properties},
  journal = {High Temperature Heat Physics},
  year =    {1963},
  volume =  {1},
  pages =   {69-83},
  file =    {:/Users/campet/Documents/Biblio/Livres meca flu/Petukhov1970.pdf:PDF}
}

@Article{DittusBoelter1930,
  author =    {Dittus, F.W. and Boelter, L.M.K.},
  title =     {Heat Transfer in Automobile Radiators of the Tubular Type},
  journal =   {University of California publications in Engineering},
  year =      {1930},
  volume =    {2},
  pages =     {371},
  file =      {:/Users/campet/Documents/Biblio/Smooth Tubes/dittus1930.pdf:PDF},
  publisher = {The University Press}
}



automatically generated by mendeley desktop 1.17.11-dev4
any changes to this file will be lost if it is regenerated by mendeley.

@article{kucherenko2015,
author = {Kucherenko, Sergei and Albrecht, Daniel and Saltelli, Andrea},
doi = {10.1016/j.ress.2017.04.003},
journal = {The 8th {IMACS} seminar on {Monte Carlo} methods},
keywords = {high dimensional integration,latin hypercube sampling,monte carlo,quasi monte carlo,sequences,sobol},
pages = {1--32},
title = {{Exploring multi-dimensional spaces: a comparison of latin hypercube and quasi monte carlo sampling techniques}},
year = {2015}
}

bibtex export options can be customized via preferences -> bibtex in mendeley desktop
@phdthesis{merkle2006,
author = {Merkle, K.},
school = {Universit{\"{a}}t Fridericiana Karlsruhe},
title = {{Einfluss gleich- und gegensinniger drehrichtung der verbrennungsluftstr{\"{o}}me auf die stabilisierung turbulenter doppeldrall-diffusionsflammen}},
year = {2006}
}

@phdthesis{Fransen2013,
author = {Fransen, R.},
school = {Institut National Polytechnique de Toulouse},
title = {{LES based aerothermal modeling of turbine blade cooling systems}},
year = {2013}
}

@article{dapogny2014358,
title = "Three-dimensional adaptive domain remeshing, implicit domain meshing, and applications to free and moving boundary problems ",
journal = "J. Comp. Physics",
volume = "262",
pages = "358--378",
year = "2014",
author = "Dapogny, C. and Dobrzynski, C. and Frey, P.",
}


@article{poinsot1992boundary,
  title={Boundary conditions for direct simulations of compressible viscous flows},
  author={Poinsot, T. and Lele, S. K.},
  journal={J. Comp. Physics},
  volume={101},
  number={1},
  pages={104--129},
  year={1992},
  publisher={Elsevier}
}

@article{sch1999steady,
  title={Steady and unsteady flow simulations using the hybrid flow solver avbp},
  author={Sch{\o}nfeld, T. and Rudgyard, M.},
  journal={{AIAA} J.},
  volume={37},
  number={11},
  pages={1378--1385},
  year={1999}
}s

@article{nicoud_pof_23_2011,
  author =   {Nicoud, F. and Baya Toda, H. and Cabrit, o. and Bose, S. and Lee, J.},
  title =    {Using singular values to build a subgrid-scale model for large eddy simulations},
  journal =    {Phys. Fluids},
  year =   2011,
  volume =   23}

@article{colin_jcp_2000,
  author =   {Colin, O. and Rudgyard, M.},
  title =    {Development of high-order {t}aylor-{g}alerkin schemes for unsteady calculation},
  journal =    {J. Comp. Physics},
  year =   2000,
  volume =   162,
  number =   2,
  pages =    {338--371}}s

@article{huang_jpp_19-5_2003,
  title={large-eddy simulation of combustion dynamics of lean-premixed swirl-stabilized combustor},
  author={huang, ying and sung, hong-gye and hsieh, shih-yang and yang, vigor},
  journal={j. prop. power},
  volume={19},
  number={5},
  pages={782--794},
  year={2003}
}

@article{moureau2011large,
  title={from large-eddy simulation to direct numerical simulation of a lean premixed swirl flame: filtered laminar flame-pdf modeling},
  author={moureau, v. and domingo, p. and vervisch, l.},
  journal={combust. flame},
  volume={158},
  number={7},
  pages={1340--1357},
  year={2011},
  publisher={elsevier}
}

@article{roux:2005,
  author = {s. roux and g. lartigue and t. poinsot and u. meier and c. b{\'e}rat},
  journal = {combust. flame},
  keywords = {acoustics, les; aero gas turbine},
  pages = {40--54},
  title = {studies of mean and unsteady flow in a swirled combustor using experiments, acoustic analysis and large eddy simulations},
  volume = {141},
  year = {2005}}

@book{sullivan2015b,
abstract = {this book is designed as a broad introduction to the mathematics of un- certainty quantification (uq) at the fourth year (senior) undergraduate or beginning postgraduate level. it is aimed primarily at readers from a math- ematical or statistical (rather than, say, engineering) background. the main mathematical prerequisite is familiarity with the language of linear functional analysis and measure / probability theory, and some familiarity with basic optimization theory. chapters 2–5 of the text provide a review of this mate- rial, generally without detailed proof. the aim of this book has been to give a survey of the main objectives in the field of uq and a few of the mathematical methods by which they can be achieved. however, this book is no exception to the old saying that books are never completed, only abandoned. there are many more uq problems and solution methods in the world than those covered here. for any grievous omissions, i ask for your indulgence, and would be happy to receive sugges- tions for improvements. with the exception of the preliminary material on measure theory and functional analysis, this book should serve as a basis for a course comprising 30–45 hours' worth of lectures, depending upon the instructor's choices in terms of selection of topics and depth of treatment. the examples and exercises in this book aim to be simple but informative about individual components of uq studies: practical applications almost always require some ad hoc combination of multiple techniques (e.g., gaus- sian process regression plus quadrature plus reduced-order modelling). such compound examples have been omitted in the interests of keeping the pre- sentation of the mathematical ideas clean, and in order to focus on examples and exercises that will be more useful to instructors and students. each chapter concludes with a bibliography, the aim of which is threefold: to give sources for results discussed but not proved in the text; to give some historical overview and context; and, most importantly, to give students a jumping-off point for further reading and research. this has led to a large bibliography, but hopefully a more useful text for budding researchers. i would like to thank achi dosanjh at springer for her stewardship of this project, and the anonymous reviewers for their thoughtful comments, which prompted many improvements to the manuscript. frominitial conception to nearly finished product, this book has benefitted from interactions with many people: they have given support and encourage- ment, offered stimulating perspectives on the text and the field of uq, and pointed out the inevitable typographical mistakes. in particular, i would like to thank paul constantine, zach dean, charlie elliott, zydrunas gimbutas, calvin khor, ilja klebanov, han cheng lie, milena kremakova, david mc- cormick, damon mcdougall, mike mckerns, akil narayan, michael ortiz, houman owhadi, adwaye rambojun, asbj{\o}rn nilsen riseth, clint scovel, colin sparrow, andrew stuart, florian theil, joy tolia, florian wechsung, thomas whitaker, and aim´ ee williams. finally, since the students on the 2013–14 iteration of the university of warwick mathematics module ma4k0 introduction to uncertainty quantifi- cation were curious and brave enough to be the initial ‘guinea pigs' for this material, they deserve a special note of thanks.},
address = {cham},
author = {sullivan, t.j.},
booktitle = {springer},
doi = {10.1007/978-3-319-23395-6},
file = {:users/roy/documents/knowledge/bibliographie/2015/sullivan{\_}introduction to uncertainty quantification.pdf:pdf},
isbn = {978-3-319-23394-9},
publisher = {springer international publishing},
series = {texts in applied mathematics},
title = {{introduction to uncertainty quantification}},
volume = {63},
year = {2015}
}
@article{sakov2012,
author = {sakov, pavel and oliver, dean s. and bertino, laurent},
doi = {10.1175/mwr-d-11-00176.1},
file = {:users/roy/documents/knowledge/bibliographie/2012/sakov, oliver, bertino{\_}an iterative enkf for strongly nonlinear systems.pdf:pdf},
journal = {monthly weather review},
pages = {1988--2004},
title = {{an iterative enkf for strongly nonlinear systems}},
volume = {140},
year = {2012}
}
@article{chaudhuri2016,
abstract = {fixed point iteration is a common strategy to handle interdisciplinary coupling within a coupled multidisciplinary analysis. for each coupled analysis, this requires a large num- ber of disciplinary high-fidelity simulations to resolve the interactions between different disciplines. when embedded within an uncertainty analysis loop (e.g., with monte carlo sampling over uncertain parameters) the number of high-fidelity disciplinary simulations quickly becomes prohibitive, since each sample requires a fixed point iteration and the uncertainty analysis typically involves thousands or even millions of samples. this paper develops a method for uncertainty analysis in feedback-coupled black-box systems that leverages adaptive surrogates to reduce the number of cases for which fixed point iteration is needed. the multifidelity coupled uncertainty propagation method is an iterative pro- cess that uses surrogates for approximating the coupling variables and adaptive sampling strategies to refine the surrogates. the adaptive sampling strategies explored in this work are residual error, information gain, and weighted information gain. the surrogate mod- els are adapted in a way that does not compromise accuracy of the uncertainty analysis relative to the original coupled high-fidelity problem.},
author = {chaudhuri, anirban and willcox, karen e.},
doi = {10.2514/6.2016-1442},
file = {:users/roy/documents/knowledge/bibliographie/2016/chaudhuri, willcox{\_}multifidelity uncertainty propagation in coupled multidisciplinary systems.pdf:pdf},
isbn = {978-1-62410-397-1},
journal = {18th aiaa non-deterministic approaches conference},
number = {january},
pages = {1--15},
title = {{multifidelity uncertainty propagation in coupled multidisciplinary systems}},
year = {2016}
}
@article{gmeiner2016,
abstract = {the computational complexity of naive, sampling-based uncertainty quantification for 3d partial differential equations is extremely high. multilevel approaches, such as multilevel monte carlo (mlmc), can reduce the complexity significantly, but to exploit them fully in a parallel environment, sophisticated scheduling strategies are needed. often fast algorithms that are executed in parallel are essential to compute fine level samples in 3d, whereas to compute individual coarse level samples only moderate numbers of processors can be employed efficiently. we make use of multiple instances of a parallel multigrid solver combined with advanced load balancing techniques. in particular, we optimize the concurrent execution across the three layers of the mlmc method: parallelization across levels, across samples, and across the spatial grid. the overall efficiency and performance of these methods will be analyzed. here the scalability window of the multigrid solver is revealed as being essential, i.e., the property that the solution can be computed with a range of process numbers while maintaining good parallel efficiency. we evaluate the new scheduling strategies in a series of numerical tests, and conclude the paper demonstrating large 3d scaling experiments.},
archiveprefix = {arxiv},
arxivid = {1607.03252},
author = {gmeiner, bj{\"{o}}rn and drzisga, daniel and ruede, ulrich and scheichl, robert and wohlmuth, barbara},
eprint = {1607.03252},
file = {:users/roy/documents/knowledge/bibliographie/2016/gmeiner et al.{\_}scheduling massively parallel multigrid for multilevel monte carlo methods.pdf:pdf},
month = {jul},
pages = {1--26},
title = {{scheduling massively parallel multigrid for multilevel monte carlo methods}},
volume = {1},
year = {2016}
}
@article{han2012,
author = {han, zhonghua and g{\"{o}}rtz, stefan},
doi = {10.2514/1.j051354},
file = {:users/roy/documents/knowledge/bibliographie/2012/han, g{\"{o}}rtz{\_}hierarchical kriging model for variable-fidelity surrogate modeling.pdf:pdf},
isbn = {0001-1452},
issn = {0001-1452},
journal = {aiaa journal},
number = {9},
pages = {1885--1896},
title = {{hierarchical kriging model for variable-fidelity surrogate modeling}},
volume = {50},
year = {2012}
}
@article{handcock1993,
abstract = {this article is concerned with predicting for gaussian random fields in a way that appropriately deals with uncertainty in the covariance function. to this end, we analyze the best linear unbiased prediction procedure within a bayesian framework. particular attention is paid to the treatment of parameters in the covariance structure and their effect on the quality, both real and perceived, of the prediction. these ideas are implemented using topographical data from davis.},
author = {handcock, mark s. and stein, michael l.},
doi = {10.2307/1270273},
file = {:users/roy/documents/knowledge/bibliographie/1993/handcock, stein{\_}a bayesian analysis of kriging.pdf:pdf},
isbn = {00401706},
issn = {00401706},
journal = {technometrics},
keywords = {bayesian statistics,interpolation,robustness,spatial statistics},
number = {4},
pages = {403--410},
pmid = {21976389},
title = {{a bayesian analysis of kriging}},
volume = {35},
year = {1993}
}
@article{potter2013,
abstract = {as dataset size and complexity steadily increase, uncertainty is becoming an important data aspect. so, today's visualizations need to incorporate indications of uncertainty. however, characterizing uncertainty for visualization isn't always straightforward. entropy, in the information-theoretic sense, can be a measure for uncertainty in categorical datasets. the authors discuss the mathematical formulation, interpretation, and use of entropy in visualizations. this research aims to demonstrate entropy as a metric and expand the vocabulary of uncertainty measures for visualization.},
author = {potter, kristin and gerber, samuel and anderson, erik w.},
doi = {10.1109/mcg.2013.14},
file = {:users/roy/documents/knowledge/bibliographie/2013/potter, gerber, anderson{\_}visualization of uncertainty without a mean.pdf:pdf},
issn = {02721716},
journal = {ieee computer graphics and applications},
keywords = {color mapping,computer graphics,entropy,uncertainty,volume rendering},
number = {1},
pages = {75--79},
pmid = {24807884},
title = {{visualization of uncertainty without a mean}},
volume = {33},
year = {2013}
}

@article{hoel2016b,
abstract = {this work embeds a multilevel monte carlo (mlmc) sampling strategy into the monte carlo step of the ensemble kalman filter (enkf), thereby yielding a multilevel ensemble kalman filter (mlenkf) which has provably superior asymptotic cost to a given accuracy level. the theoretical results are illustrated numerically.},
archiveprefix = {arxiv},
arxivid = {1608.08558},
author = {hoel, h{\aa}kon and law, kody j. h. and tempone, raul},
doi = {10.1137/15m100955x},
eprint = {1608.08558},
file = {:users/roy/documents/knowledge/bibliographie/2016/hoel, law, tempone{\_}multilevel ensemble kalman filtering(2).pdf:pdf},
issn = {0036-1429},
journal = {siam journal on numerical analysis},
keywords = {1,65c30,65y20,a system through sequential,ams subject classification,and,ensemble kalman filter,filtering,filtering refers to the,incorporation of online data,introduction,kalman filter,monte carlo,multilevel,or parameters p of,sequential estimation of the,state v,y},
month = {jan},
number = {3},
pages = {1813--1839},
title = {{multilevel ensemble kalman filtering}},
volume = {54},
year = {2016}
}
@article{ren2015,
abstract = {state-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. advances like sppnet and fast r-cnn have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. in this work, we introduce a region proposal network (rpn) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. an rpn is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. the rpn is trained end-to-end to generate high-quality region proposals, which are used by fast r-cnn for detection. we further merge rpn and fast r-cnn into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the rpn component tells the unified network where to look. for the very deep vgg-16 model, our detection system has a frame rate of 5fps (including all steps) on a gpu, while achieving state-of-the-art object detection accuracy on pascal voc 2007, 2012, and ms coco datasets with only 300 proposals per image. in ilsvrc and coco 2015 competitions, faster r-cnn and rpn are the foundations of the 1st-place winning entries in several tracks. code has been made publicly available.},
archiveprefix = {arxiv},
arxivid = {1506.01497},
author = {impiombato, d. and giarrusso, s. and mineo, t. and catalano, o. and gargano, c. and {la rosa}, g. and russo, f. and sottile, g. and billotta, s. and bonanno, g. and garozzo, s. and grillo, a. and marano, d. and romeo, g.},
doi = {10.1016/j.nima.2015.05.028},
eprint = {1506.01497},
file = {:users/roy/documents/knowledge/bibliographie/2015/impiombato et al.{\_}characterization and performance of the asic (citiroc) front-end of the astri camera.pdf:pdf},
isbn = {0162-8828 vo - pp},
issn = {01689002},
journal = {nuclear instruments and methods in physics research section a: accelerators, spectrometers, detectors and associated equipment},
month = {sep},
pages = {185--192},
pmid = {27295650},
title = {{characterization and performance of the asic (citiroc) front-end of the astri camera}},
volume = {794},
year = {2015}
}
@article{orr1996,
abstract = {in this paper we provide a short overview of the radial basis functions (rbf), their properties, the motivations behind their use and some of their applications. rbf's have been employed for functional approximation in time-series modeling and in pattern classification. they have been shown to implement the bayesian rule and to model any continuous input- output mapping. rbf's are embedded in a two-layer neural network topology. we present the physical and statistical significance of the elements composing the network. we introduce a few rbf training algorithms and we show how rbf networks can be used in real applications.},
author = {orr, mjl},
file = {:users/roy/documents/knowledge/bibliographie/1996/orr{\_}introduction to radial basis function networks.pdf:pdf},
journal = {university of edinburg},
pages = {1--7},
title = {{introduction to radial basis function networks}},
year = {1996}
}

@book{baudrier2004,
abstract = {current efforts to develop methods and computer algorithms to effectively represent multivariate data commonly encountered in remote sensing applications are described. while this may involve scatter diagrams, multivariate representations of nonparametric probability density estimates are emphasized. the density function provides a useful graphical tool for looking at data and a useful theoretical tool for classification. this approach is called a thunderstorm data analysis.},
author = {baudrier, martial and cochet, thomas and majorel, florence},
booktitle = {mortality},
doi = {10.1002/9780470316849},
file = {:users/roy/documents/knowledge/bibliographie/2004/baudrier, cochet, majorel{\_}multivariate density estimation applied.pdf:pdf},
isbn = {9780470316849},
issn = {00203157},
number = {december},
pages = {1--11},
pmid = {421},
title = {{multivariate density estimation applied}},
year = {2004}
}
@article{potter2010,
abstract = {the graphical depiction of uncertainty information is emerging as a problem of great importance. scientific data sets are not considered complete without indications of error, accuracy, or levels of confidence. the visual por- trayal of this information is a challenging task. this work takes inspiration from graphical data analysis to create visual representations that show not only the data value, but also important characteristics of the data including uncertainty. the canonical box plot is reexamined and a new hybrid summary plot is presented that incorporates a collection of descriptive statistics to highlight salient features of the data. additionally, we present an exten- sion of the summary plot to two dimensional distributions. finally, a use-case of these new plots is presented, demonstrating their ability to present high-level overviews as well as detailed insight into the salient features of the underlying data distribution.},
author = {potter, k and kniss, j and riesenfeld, r and johnson, c r},
doi = {10.1111/j.1467-8659.2009.01677.x},
file = {:users/roy/documents/knowledge/bibliographie/2010/potter et al.{\_}visualizing summary statistics and uncertainty.pdf:pdf},
isbn = {no},
issn = {01677055},
journal = {computer graphics forum},
keywords = {i.3.6 [computer graphics]: methodology and techniq},
number = {3},
pages = {823--832},
title = {{visualizing summary statistics and uncertainty}},
volume = {29},
year = {2010}
}
@inproceedings{ankenman2008,
abstract = {we extend the basic theory of kriging, as applied to the design and analysis of deterministic computer experiments, to the stochastic simulation setting. our goal is to provide flexible, interpolation-based metamodels of simulation output performance measures as functions of the controllable design or decision variables. to accomplish this we characterize both the intrinsic uncertainty inherent in a stochastic simulation and the extrinsic uncertainty about the unknown response surface. we use tractable examples to demonstrate why it is critical to characterize both types of uncertainty, derive general results for experiment design and analysis, and present a numerical example that illustrates the stochastic kriging method.},
author = {ankenman, bruce and nelson, barry l. and staum, jeremy},
booktitle = {2008 winter simulation conference},
doi = {10.1109/wsc.2008.4736089},
file = {:users/roy/documents/knowledge/bibliographie/2008/ankenman, nelson, staum{\_}stochastic kriging for simulation metamodeling.pdf:pdf},
isbn = {978-1-4244-2707-9},
issn = {08917736},
month = {dec},
number = {2},
pages = {362--370},
publisher = {ieee},
title = {{stochastic kriging for simulation metamodeling}},
volume = {58},
year = {2008}
}
@article{xiao2015,
abstract = {despite their well-known limitations, reynolds-averaged navier-stokes (rans) models are still the workhorse tools for turbulent flow simulations in today's engineering application. for many practical flows, the turbulence models are by far the largest source of uncertainty. in this work we develop an open-box, physics-informed bayesian framework for quantifying model-form uncertainties in rans simulations. uncertainties are introduced directly to the reynolds stresses and are represented with compact parameterization accounting for empirical prior knowledge and physical constraints (e.g., realizability, smoothness, and symmetry). an iterative ensemble kalman method is used to assimilate the prior knowledge and observation data in a bayesian framework, and to propagate them to posterior distributions of velocities and other quantities of interest (qois). we use two representative cases, the flow over periodic hills and the flow in a square duct, to evaluate the performance of the proposed framework. simulation results suggest that, even with very sparse observations, the posterior mean velocities and other qois have significantly better agreement with the benchmark data compared to the baseline results. at most locations the posterior distribution adequately captures the true model error within the developed model form uncertainty bounds. the framework is a major improvement over existing black-box, physics-neutral methods for model-form uncertainty quantification, and has potential implications in many fields in which the model uncertainty comes from unresolved physical processes. a notable example is climate modeling, where high-consequence decisions are made based on predictions (e.g., projected temperature rise) with major uncertainties originating from closure models that are used to account for unresolved or unknown physics including radiation, cloud, and boundary layer processes.},
archiveprefix = {arxiv},
arxivid = {1508.06315},
author = {xiao, h. and wu, j. -l. and wang, j. -x. and sun, r. and roy, c. j.},
eprint = {1508.06315},
file = {:users/roy/documents/knowledge/bibliographie/2015/xiao et al.{\_}quantifying and reducing model-form uncertainties in reynolds-averaged navier-stokes simulations an open-box, physics-based,.pdf:pdf},
month = {aug},
pages = {1--41},
title = {{quantifying and reducing model-form uncertainties in reynolds-averaged navier-stokes simulations: an open-box, physics-based, bayesian approach}},
year = {2015}
}
@book{brillinger1992,
address = {new york, ny},
author = {brillinger, d and gani, j and hartigan, j and krickeberg, k},
doi = {10.1007/978-1-4612-0919-5},
editor = {kotz, samuel and johnson, norman l.},
file = {:users/roy/documents/knowledge/bibliographie/1992/brillinger et al.{\_}breakthroughs in statistics 1.pdf:pdf},
isbn = {978-0-387-94037-3},
publisher = {springer new york},
series = {springer series in statistics},
title = {{breakthroughs in statistics 1}},
year = {1992}
}
@article{americansocietyofmechanicalengineers.2007,
abstract = {although no standard method for evaluating numerical uncertainty is currently accepted by the cfd community, there are numerous methods and techniques available to the user to accomplish this task. the following is a list of guidelines, enumerating the criteria to be considered for ar-chival publication of computational results in the journal of fluids engineering. 1. authors must be precise in describing the numerical method used; this includes an as-sessment of the formal order of accuracy of the truncation error introduced by individual terms in the governing equations, such as diffusive terms, source terms, and most impor-tantly, the convective terms. it is not enough to state, for example, that the method is based on a " conservative finite-volume formulation, " giving then a reference to a general cfd textbook. 2. the numerical method used must be at least formally second-order accurate in space (based on a taylor series expansion) for nodes in the interior of the computational grid. the computational expense of second, third, and higher order methods are more expen-sive (per grid point) than first order schemes, but the computational efficiency of these higher order methods (accuracy per overall cost) is much greater. and, it has been dem-onstrated many times that, for first order methods, the effect of numerical diffusion on the solution accuracy is devastating. 3. methods using a blending or switching strategy between first and second order methods (in particular, the well-known " hybrid, " " power-law, " and related exponential schemes) will be viewed as first-order methods, unless it can be demonstrated that their inherent numerical diffusion does not swamp or replace important modeled physical diffusion terms. a similar policy applies to methods invoking significant amounts of explicitly added artificial viscosity or diffusivity. 4. solutions over a range of significantly different grid resolutions should be presented to demonstrate grid-independent or grid-convergent results. this criterion specifically ad-dresses the use of improved grid resolution to systematically evaluate truncation error and accuracy. the use of error estimates based on methods such as richardson extrapolation or those techniques now used in adaptive grid methods, may also be used to demonstrate solution accuracy. 5. stopping criteria for iterative calculations need to be precisely explained. estimates must be given for the corresponding convergence error. 6. in time-dependent solutions, temporal accuracy must be demonstrated so that the spuri-ous effects of phase error are shown to be limited. in particular, it should be demon-strated that unphysical oscillations due to numerical dispersion are significantly smaller in amplitude than captured short-wavelength (in time) features of the flow. 7. clear statements defining the methods used to implement boundary and initial conditions must be presented. typically, the overall accuracy of a simulation is strongly affected by the implementation and order of the boundary conditions. when appropriate, particular attention should be paid to the treatment of inflow and outflow boundary conditions. 8. in the presentation of an existing algorithm or code, all pertinent references or other pub-lications must be cited in the paper, thus aiding the reader in evaluating the code and its method without the need to redefine details of the methods in the current paper. how-ever, basic features of the code must be outlined according to item 1, above. 9. comparison to appropriate analytical or well-established numerical benchmark solutions may be used to demonstrate accuracy for another class of problems. however, in general this does not demonstrate accuracy for another class of problems, especially if any ad-justable parameters are involved, as in turbulence modeling. 10. comparison with reliable experimental results is appropriate, provided experimental un-certainty is established. however, " reasonable agreement " with experimental data alone will not be enough to justify a given single-grid calculation, especially if adjustable pa-rameters are involved. these ten items lay down a set of criteria by which the editors and reviewers of this journal will judge the archival quality of publications dealing with computational studies for the journal of fluids engineering. we recognize that the effort to perform a thorough study of numerical accuracy may be great and that many practical engineering calculations will continue to be per-formed by first order methods, on a single fixed grid. however, such analyses would not be ap-propriate for presentation in this archival journal. with the gains in performance of low-end workstations, it is now reasonable to require papers on solutions by cfd to meet these funda-mental criteria for archiving of a publication. with the details of these ten criteria now presented, a shortened statement will appear as fol-lows: the journal of fluids engineering will not consider any paper reporting the numerical solu-tion of a fluids engineering problem that fails to address the task of systematic truncation error testing and accuracy estimation. authors should address the following criteria for assessing numerical uncertainty. 1. the basic features of the method including formal truncation error of individual terms in the governing numerical equations must be described. 2. methods must be at least second order accurate in space. 3. inherent or explicit artificial viscosity (or diffusivity) must be assessed and minimized. 4. grid independence or convergence must be established. 5. when appropriate, iterative convergence must be addressed. 6. in transient calculations, phase error must be assessed and minimized. 7. the accuracy and implementation of boundary and initial conditions must be fully ex-plained. 8. an existing code must be fully cited in easily available references. 9. benchmark solutions may be used for validation for a specific class of problems. 10. reliable experimental results may be used to validate a solution.},
author = {celik, ismail b. and ghia, urmila and roache, patrick j. and freitas, christopher j.},
doi = {10.1115/1.2960953},
file = {:users/roy/documents/knowledge/bibliographie/2008/celik et al.{\_}procedure for estimation and reporting of uncertainty due to discretization in cfd applications.pdf:pdf},
isbn = {0098-2202},
issn = {00982202},
journal = {journal of fluids engineering},
keywords = {fluid mechanics periodicals.,fluids periodicals.},
number = {7},
pages = {078001},
title = {{procedure for estimation and reporting of uncertainty due to discretization in cfd applications}},
volume = {130},
year = {2008}
}
@article{sankaran2011,
abstract = {simulations of blood flow in both healthy and diseased vascular models can be used to compute a range of hemodynamic parameters including velocities, time varying wall shear stress, pressure drops, and energy losses. the confidence in the data output from cardiovascular simulations depends directly on our level of certainty in simulation input parameters. in this work, we develop a general set of tools to evaluate the sensitivity of output parameters to input uncertainties in cardiovascular simulations. uncertainties can arise from boundary conditions, geometrical parameters, or clinical data. these uncertainties result in a range of possible outputs which are quantified using probability density functions (pdfs). the objective is to systemically model the input uncertainties and quantify the confidence in the output of hemodynamic simulations. input uncertainties are quantified and mapped to the stochastic space using the stochastic collocation technique. we develop an adaptive collocation algorithm for gauss-lobatto-chebyshev grid points that significantly reduces computational cost. this analysis is performed on two idealized problems--an abdominal aortic aneurysm and a carotid artery bifurcation, and one patient specific problem--a fontan procedure for congenital heart defects. in each case, relevant hemodynamic features are extracted and their uncertainty is quantified. uncertainty quantification of the hemodynamic simulations is done using (a) stochastic space representations, (b) pdfs, and (c) the confidence intervals for a specified level of confidence in each problem.},
author = {sankaran, sethuraman and marsden, alison l.},
doi = {10.1115/1.4003259},
file = {:users/roy/documents/knowledge/bibliographie/2011/sankaran, marsden{\_}a stochastic collocation method for uncertainty quantification and propagation in cardiovascular simulations.pdf:pdf},
isbn = {0148-0731},
issn = {01480731},
journal = {journal of biomechanical engineering},
keywords = {abdominal,abdominal aortic aneurysms,abdominal: physiopathology,adaptive algorithms,aortic aneurysm,blood flow velocity,blood flow velocity: physiology,blood flow,blood vessels,cardiovascular,carotid arteries,carotid arteries: physiology,carotid artery bifurcation,chebyshev,clinical data,computational costs,computer simulation,computer simulation,confidence interval,congenital heart defects,data output,energy dissipation,energy loss,fontan procedure,gauss lobatto chebyshev grid,geometrical parameters,grid points,hemodynamic parameters,hemodynamics,hemodynamics: physiology,humans,hydrodynamics,input parameter,input uncertainty,mechanical,models,output parameters,patient specific,probability,probability density function,sensitivity and specificity,shear strength,shear strength: physiology,shear flow,stochastic processes,stochastic collocation,stochastic collocation method,stochastic systems,stress,time varying,uncertainty,uncertainty quantifications,vascular model,wall shear stress,abdominal aorta aneurysm,algorithm,arterial wall thickness,article,biological model,blood flow velocity,cardiovascular disease,cardiovascular system,carotid artery,carotid artery bifurcation,clinical study,computer simulation,congenital heart malformation,coronary artery blood flow,cost,disease simulation,evaluation,geometry,heart function,heart hemodynamics,heart output,hemodynamics,human,mathematical model,mechanical stress,pathophysiology,physiology,physiopathology,probability,probability density function,sensitivity analysis,sensitivity and specificity,shear strength,shear stress,simulation,statistics,stochastic collocation method,stochastic model,uncertainty},
number = {3},
pages = {031001},
pmid = {21303177},
title = {{a stochastic collocation method for uncertainty quantification and propagation in cardiovascular simulations}},
volume = {133},
year = {2011}
}
@article{jin2017,
abstract = {fig. 1. text-based editing provides a natural interface for modifying audio narrations. our approach allows the editor to replace an existing word (or insert a new word) by typing, and the system automatically synthesizes the new word by stitching together snippets of audio from elsewhere in the narration. here we replace the word sixteen by seventeen in a text editor, and the new audio is automatically stitched together from parts of the words else, seventy, want and seen. editing audio narration using conventional software typically involves many painstaking low-level manipulations. some state of the art systems allow the editor to work in a text transcript of the narration, and perform select, cut, copy and paste operations directly in the transcript; these operations are then automatically applied to the waveform in a straightforward manner. however, an obvious gap in the text-based interface is the ability to type new words not appearing in the transcript, for example inserting a new word for emphasis or replacing a misspoken word. while high-quality voice synthesizers exist today, the challenge is to synthesize the new word in a voice that matches the rest of the narration. this paper presents a system that can synthesize a new word or short phrase such that it blends seamlessly in the context of the existing narration. our approach is to use a text to speech synthesizer to say the word in a generic voice, and then use voice conversion to convert it into a voice that matches the narration. offering a range of degrees of control to the editor, our interface supports fully automatic synthesis, selection among a candidate set of alternative pronunciations, fine control over edit placements and pitch profiles, and even guidance by the editors own voice. the paper presents studies showing that the output of our method is preferred over baseline methods and often indistinguishable from the original voice.},
author = {jin, zeyu and mysore, gautham j and diverdi, stephen and diverdi, stephen and lu, jingwan},
doi = {10.1145/3072959.3073702},
file = {:users/roy/documents/knowledge/bibliographie/2017/jin et al.{\_}voco text-based insertion and replacement in audio narration.pdf:pdf},
journal = {acm trans. graph. article},
keywords = {additional key words and phrases,ccs concepts,audio,human computer interaction acm reference format},
number = {96},
title = {{voco: text-based insertion and replacement in audio narration}},
volume = {36},
year = {2017}
}
@article{lodha1996,
abstract = {integrated presentation of data with uncertainty is a worthy goal in scientific visualization. it allows researchers to make informed decisions based on imperfect data. it also allows users to visually compare and contrast different algorithms for performing the same task or different models for representing the same physical phe- nomenon. this work presents listen – a data sonification system – that has been incorporated into two visualization systems: a sys- tem for visualizing geometric uncertainty of surface interpolants and a system for visualizing uncertainty in fluid flow. listen is written in c++ for the sgi platform. it works with the sgi internal audio chip or amidi device or both. listen is an object-oriented systemthat ismodular, flexible, adaptable, portable, interactive and extensible. we demonstrate that sonification is very effective as an additional tool in visualizing geometric and fluid flowuncertainty},
author = {lodha, suresh k. and wilson, c.m. and sheehan, r.e.},
doi = {10.1109/visual.1996.568105},
file = {:users/roy/documents/knowledge/bibliographie/1996/lodha, wilson, sheehan{\_}listen sounding uncertainty visualization.pdf:pdf},
isbn = {0-89791-864-9},
journal = {proceedings of seventh annual ieee visualization '96},
keywords = {and phrases,flow,geometry,interactive,interpola-,midi,modular,portable,sonification,tion,uncertainty,visualiza-},
pages = {189--195},
title = {{listen: sounding uncertainty visualization}},
year = {1996}
}
@article{marrel2012,
abstract = {the global sensitivity analysis method, used to quantify the influence of uncertain input variables on the response variability of a numerical model, is applicable to deterministic computer code (for which the same set of input variables gives always the same output value). this paper proposes a global sensitivity analysis methodology for stochastic computer code (having a variability induced by some uncontrollable variables). the framework of the joint modeling of the mean and dispersion of heteroscedastic data is used. to deal with the complexity of computer experiment outputs, non parametric joint models (based on generalized additive models and gaussian processes) are discussed. the relevance of these new models is analyzed in terms of the obtained variance-based sensitivity indices with two case studies. results show that the joint modeling approach leads accurate sensitivity index estimations even when clear heteroscedasticity is present.},
author = {marrel, amandine and iooss, bertrand and {da veiga}, s{\'{e}}bastien and ribatet, mathieu},
doi = {10.1007/s11222-011-9274-8},
file = {:users/roy/documents/knowledge/bibliographie/2012/marrel et al.{\_}global sensitivity analysis of stochastic computer models with joint metamodels.pdf:pdf},
issn = {0960-3174},
journal = {statistics and computing},
keywords = {additive model,computer experiment,gaussian process,generalized,joint modeling,sobol indices,uncertainty},
month = {may},
number = {3},
pages = {833--847},
title = {{global sensitivity analysis of stochastic computer models with joint metamodels}},
volume = {22},
year = {2012}
}
@book{schilders2008e,
abstract = {the idea for this book originated during the workshop “model order reduction, coupled problems and optimization” held at the lorentz center in leiden from sep- tember 19–23, 2005. during one of the discussion sessions, it became clear that a book describing the state of the art in model order reduction, starting from the very basics and containing an overview of all relevant techniques, would be of great use for students, young researchers starting in the field, and experienced researchers. the observation that most of the theory on model order reduction is scattered over many good papers, making it difficult to find a good starting point, was supported by most of the participants. moreover, most of the speakers at the workshop were willing to contribute to the book that is now in front of you. the goal of this book, as defined during the discussion sessions at the workshop, is three-fold: first, it should describe the basics of model order reduction. second, both general and more specialized model order reduction techniques for linear and nonlinear systems should be covered, including the use of several related numerical techniques. third, the use of model order reduction techniques in practical applica- tions and current research aspects should be discussed. we have organized the book according to these goals. in part i, the rationale behind model order reduction is explained, and an overview of the most common methods is described. furthermore, in the second chapter, an introduction is given to background material from numerical linear algebra needed to assess the theory and methods presented later in the book. this is very important and useful informa- tion, as advances in numerical linear algebra often lead to new results in the area of model order reduction. thus, the first two chapters serve as an introduction to readers who are not familiar with the subject. in part ii, model order reduction techniques and related numerical problems are described from different points of view: both frameworks for structure-preserving techniques and more specialized techniques are presented, while numerical methods for (closely) related problems and approaches for nonlinear systems are considered as well. this part serves as the theoretical back- bone of the book, containing an overview of techniques used and areas covered. in part iii the focus is on research aspects and applications of model order reduction. a variety of experiments with real-life examples shows that different problems require different techniques, while application of the techniques leads to new research topics that are described as well. despite the fact that the workshop was organized already in 2005, this book con- tains many recent advances in model order reduction. moreover, it presents several open problems for which techniques are still in development, related to both linear systems, which become larger and more complex mainly due to industrial require- ments, and nonlinear systems, which demand a completely new theory. the latter illustrates the final and most important goal of this book, namely to serve as a source of inspiration for its readers, who will discover that model order reduction is a very exciting and lively field. at this point we would like to thank all authors of the chapters in this book. without the contributions of these experts, it would not be possible to cover the wide and rapidly developing field of model order reduction in one book.},
address = {berlin, heidelberg},
archiveprefix = {arxiv},
arxivid = {1011.1669},
author = {vinet, luc and zhedanov, alexei},
booktitle = {methods},
doi = {10.1007/978-3-540-78841-6},
editor = {schilders, wilhelmus h. a. and van der vorst, henk a. and rommes, joost},
eprint = {1011.1669},
file = {:users/roy/documents/knowledge/bibliographie/2008/vinet, zhedanov{\_}model order reduction theory, research aspects and applications.pdf:pdf},
isbn = {978-3-540-78840-9},
issn = {00392871},
month = {nov},
number = {december 2007},
pages = {471},
pmid = {25246403},
publisher = {springer berlin heidelberg},
series = {mathematics in industry},
title = {{model order reduction: theory, research aspects and applications}},
volume = {13},
year = {2008}
}
@article{pang1997,
abstract = {visualized data often have dubious origins and quality. different forms of uncertainty and errors are also introduced as the data are derived, transformed, interpolated, and finally rendered. this paper surveys uncertainty visualization techniques that present data so that users are made aware of the locations and degree of uncertainties in their data. the techniques include adding glyphs, adding geometry, modifying geometry, modifying attributes, animation, sonification, and psychovisual approaches. we present our results in uncertainty visualization for environmental visualization, surface interpolation, global illumination with radiosity, flow visualization, and figure animation. we also present a classification of the possibilities in uncertainty visualization and locate our contributions within this classification.},
author = {pang, alex t. and wittenbrink, craig m. and lodha, suresh k.},
doi = {10.1007/s003710050111},
file = {:users/roy/documents/knowledge/bibliographie/1997/pang, wittenbrink, lodha{\_}approaches to uncertainty visualization.pdf:pdf},
isbn = {0178-2789},
issn = {01782789},
journal = {the visual computer},
keywords = {classi cation,comparative visualization,data quality,di erences,verity},
number = {8},
pages = {370--390},
title = {{approaches to uncertainty visualization}},
volume = {13},
year = {1997}
}
@misc{saint-geours2013,
address = {houches},
author = {saint-geours, nathalie},
file = {:users/roy/documents/knowledge/bibliographie/2013/saint-geours{\_}analyse de sensibilit{\'{e}} spatialis{\'{e}}e.pdf:pdf},
title = {{analyse de sensibilit{\'{e}} spatialis{\'{e}}e}},
year = {2013}
}
@article{liu2015,
abstract = {uncertainty quantification in aerodynamic simulations calls for efficient numerical methods {\{}$\backslash$color{\{}noblue{\}} to reduce computational cost{\}}, especially for the uncertainties caused by random geometry variations which involve a large number of variables. this paper compares five methods, including quasi-monte carlo quadrature, polynomial chaos with coefficients determined by sparse quadrature and gradient-enhanced version of kriging, radial basis functions and point collocation polynomial chaos, in their efficiency in estimating statistics of aerodynamic performance upon random perturbation to the airfoil geometry which is parameterized by 9 independent gaussian variables. the results show that gradient-enhanced surrogate methods achieve better accuracy than direct integration methods with the same computational cost.},
archiveprefix = {arxiv},
arxivid = {1505.05731},
author = {liu, dishi and litvinenko, alexander and schillings, claudia and schulz, volker},
eprint = {1505.05731},
file = {:users/roy/documents/knowledge/bibliographie/2015/liu et al.{\_}quantification of airfoil geometry-induced aerodynamic uncertainties - comparison of approaches.pdf:pdf},
month = {may},
pages = {1--23},
title = {{quantification of airfoil geometry-induced aerodynamic uncertainties - comparison of approaches}},
year = {2015}
}
@article{gamboa2014,
abstract = {let x := (x1, . . . ,xp) be random objects (the inputs), defined on some probability space (ω,f,p) and valued in some measurable space e = e1 ×. . .×ep. further, let y := y = f(x1, . . . ,xp) be the output. here, f is a measurable function from e to some hilbert space h (h could be either of finite or infinite dimension). in this work, we give a natural generalization of the sobol indices (that are classically defined when y ∈ r ), when the output belongs to h. these indices have very nice properties. first, they are invariant. under isometry and scaling. further they can be, as in dimension 1, easily estimated by using the so- called pick and freeze method. we investigate the asymptotic behaviour of such estimation scheme.},
author = {gamboa, fabrice and janon, alexandre and klein, thierry and lagnoux, agn{\`{e}}s},
doi = {10.1214/14-ejs895},
file = {:users/roy/documents/knowledge/bibliographie/2014/gamboa et al.{\_}sensitivity analysis for multidimensional and functional outputs.pdf:pdf},
issn = {1935-7524},
journal = {electronic journal of statistics},
keywords = {concentration inequalities,quadratic functionals,semi-parametric efficient estimation,sensitivity analysis,sobol indices,temporal output,vector output},
number = {1},
pages = {575--603},
title = {{sensitivity analysis for multidimensional and functional outputs}},
volume = {8},
year = {2014}
}
@book{mendelson2003b,
author = {mendelson, s.},
file = {:users/roy/documents/knowledge/bibliographie/2003/mendelson{\_}advanced lectures on machine learning.pdf:pdf},
isbn = {3-540-23122-6},
title = {{advanced lectures on machine learning}},
year = {2003}
}
@article{vandam2016,
abstract = {two uncertainty quantification (uq) techniques, latin-hypercube sampling (lhs) and polynomial chaos expansion (pce), have been used in an initial uq study to calculate the effect of boundary condition uncertainty on large-eddy spray simulations. liquid and vapor penetration as well as multidimensional liquid and vapor data were used as response variables. the morris one-at-a-time (moat) screening method was used to identify the most important boundary conditions. the lhs and pce methods both predict the same level of variability in the response variables, which was much larger than the corresponding experimental uncertainty. nested grids were used in conjunction with the pce method to examine the effects of subsets of boundary condition variables. numerical modeling parameters had a much larger effect on the resulting spray predictions; the uncertainty in spray penetration or multidimensional spray contours from physically derived boundary conditions was close to the uncertainty of the measurements.},
author = {{van dam}, noah and rutland, chris},
doi = {10.1115/1.4032196},
file = {:users/roy/documents/knowledge/bibliographie/2016/van dam, rutland{\_}uncertainty quantification of large-eddy spray simulations.pdf:pdf},
issn = {2377-2158},
journal = {journal of verification, validation and uncertainty quantification},
month = {jan},
number = {2},
pages = {021006},
title = {{uncertainty quantification of large-eddy spray simulations}},
volume = {1},
year = {2016}
}

@article{seshadri2014,
abstract = {optimization under uncertainty methodologies -- such as robust design optimization and reliability-based design optimization -- have been applied in a variety of engineering disciplines. these methods typically use multi-objective optimization algorithms, which are computationally expensive and for certain applications may even be prohibitive. another limitation of these strategies is that they typically utilize the first two statistical moments -- the mean and variance -- and ignore higher-order moments as objective functions that may be critical in influencing design decisions. to address these two issues -- the large computational overhead of multi-objective optimization and the use of higher-order moments -- we propose a new approach for optimization under uncertainty: aggressive design. aggressive design is a novel approach, which enables the designer to find the optimal design given a specification of the system's behavior under uncertain inputs by solving a single-objective optimization problem. in this paper we present a computational method that finds the design probability density function (pdf) of the response that best matches the desired pdf of the response. our approach makes use of kernel density estimates to yield a differentiable objective function. we test this method on a linear model problem and the computational design of an airfoil.},
archiveprefix = {arxiv},
arxivid = {1409.7089},
author = {seshadri, pranay and constantine, paul},
eprint = {1409.7089},
file = {:users/roy/documents/knowledge/bibliographie/2014/seshadri, constantine{\_}aggressive design a density-matching approach for optimization under uncertainty.pdf:pdf},
journal = {arxiv preprint arxiv: {\ldots}},
keywords = {a density-matching,approach for optimization under,ressive design,uncertainty},
number = {2011},
pages = {30},
title = {{aggressive design: a density-matching approach for optimization under uncertainty}},
year = {2014}
}
@article{bearman2013,
abstract = {there is a limit to the amount of spatial data that can be shown visually in an effective manner, particularly when the data sets are extensive or complex. using sound to represent some of these data (sonification) is a way of avoiding visual overload. this thesis creates a conceptual model showing how sonification can be used to represent spatial data and evaluates a number of elements within the conceptual model. these are examined in three different case studies to assess the effectiveness of the sonifications. current methods of using sonification to represent spatial data have been restricted by the technology available and have had very limited user testing. while existing research shows that sonification can be done, it does not show whether it is an effective and useful method of representing spatial data to the end user. a number of prototypes show how spatial data can be sonified, but only a small handful of these have performed any user testing beyond the authors' immediate colleagues (where n {\textgreater} 4). this thesis creates and evaluates sonification prototypes, which represent uncertainty using three different case studies of spatial data. each case study is evaluated by a significant user group (between 45 and 71 individuals) who completed a task based evaluation with the sonification tool, as well as reporting qualitatively their views on the effectiveness and usefulness of the sonification method. for all three case studies, using sound to reinforce information shown visually results in more effective performance from the majority of the participants than traditional visual methods. participants who were familiar with the dataset were much more effective at using the sonification than those who were not and an interactive sonification which requires significant involvement from the user was much more effective than a static sonification, which did not provide significant user engagement. using sounds with a clear and easily understood scale (such as piano notes) was important to achieve an effective sonification. these findings are used to improve the conceptual model developed earlier in this thesis and highlight areas for future research.},
author = {bearman, nicholas edward},
file = {:users/roy/documents/knowledge/bibliographie/2013/bearman{\_}using sound to represent uncertainty in spatial data.pdf:pdf},
number = {january},
title = {{using sound to represent uncertainty in spatial data}},
year = {2013}
}
@inproceedings{pichler2016,
address = {Seoul},
author = {Pichler, Richard and Kopriva, James and Laskowski, Gregory and Michelassi, Vittorio and Sandberg, Richard},
booktitle = {Turbomachinery Technical Conference and Exposition gt2016},
file = {:users/roy/documents/knowledge/bibliographie/2016/pichler et al.{\_}highly resolved les of a linear hpt vane cascade using structured and unstructured codes.pdf:pdf},
pages = {1--10},
title = {{Highly resolved {LES} of a linear hpt vane cascade using structured and unstructured codes}},
year = {2016}
}
@conference{pr-cfd-17-8,
address = {paris, france},
author = {roy, pamphile t.},
booktitle = {mascot-num 2017},
editor = {mascot-num, g d r},
file = {:users/roy/documents/knowledge/bibliographie/2017/roy{\_}improving surrogate model-based uncertainty quantification application to les.pdf:pdf},
organization = {institut henri poincar{\'{e}} et agroparisttech},
title = {{improving surrogate model-based uncertainty quantification application to les}},
year = {2017}
}
@article{kleijnen2000,
abstract = {this paper proposes a methodology that replaces the usual ad hoc approach to metamodeling. this methodology considers validation of a metamodel with respect to both the underlying simulation model and the problem entity. it distinguishes between fitting and validating a metamodel, and covers four types of goal: (i) understanding, (ii) prediction, (iii) optimization, and (iv) verification and validation. the methodology consists of a metamodeling process with 10 steps. this process includes classic design of experiments (doe) and measuring fit through standard measures such as r-square and cross-validation statistics. the paper extends this doe to stagewise doe, and discusses several validation criteria, measures, and estimators. the methodology covers metamodels in general (including neural networks); it also gives a specific procedure for developing linear regression (including polynomial) metamodels for random simulation.},
author = {kleijnen, jack p.c. and sargent, robert g.},
doi = {10.1016/s0377-2217(98)00392-0},
file = {:users/roy/documents/knowledge/bibliographie/2000/kleijnen, sargent{\_}a methodology for fitting and validating metamodels in simulation.pdf:pdf},
isbn = {3113466202},
issn = {03772217},
journal = {european journal of operational research},
keywords = {approximation,modeling,regression,response surface,simulation},
number = {1},
pages = {14--29},
title = {{a methodology for fitting and validating metamodels in simulation}},
volume = {120},
year = {2000}
}

@article{iooss2010a,
abstract = {complex computer codes, for instance simulating physical phenomena, are often too time expensive to be directly used to perform uncertainty, sensitivity, optimization and robustness analyses. a widely accepted method to circumvent this problem consists in replacing cpu time expensive computer models by cpu inexpensive mathematical functions, called metamodels. in this paper, we focus on the gaussian process metamodel and two essential steps of its definition phase. first, the initial design of the computer code input variables (which allows to fit the metamodel) has to provide adequate space filling properties. we adopt a numerical approach to compare the performance of different types of space filling designs, in the class of the optimal latin hypercube samples, in terms of the predictivity of the subsequent fitted metamodel. we conclude that such samples with minimal wrap-around discrepancy are particularly well-suited for the gaussian process metamodel fitting. second, the metamodel validation process consists in evaluating the metamodel predictivity with respect to the initial computer code. we propose and test an algorithm, which optimizes the distance between the validation points and the metamodel learning points in order to estimate the true metamodel predictivity with a minimum number of validation points. comparisons with classical validation algorithms and application to a nuclear safety computer code show the relevance of this new sequential validation design.},
author = {iooss, bertrand and boussouf, lo{\"{i}}c and feuillard, vincent and marrel, amandine},
file = {:users/roy/documents/knowledge/bibliographie/2010/iooss et al.{\_}numerical studies of the metamodel fitting and validation processes.pdf:pdf},
journal = {international journal on advances in systems and measurements},
keywords = {gaussian process,latin hypercube sampling,metamodel,computer experiment,discrepancy,op- timal design},
number = {1},
pages = {11--21},
title = {{numerical studies of the metamodel fitting and validation processes}},
volume = {3},
year = {2010}
}
@incollection{sunar2009,
address = {boston, ma},
author = {sunar, berk},
booktitle = {cryptographic engineering},
doi = {10.1007/978-0-387-71817-0_4},
file = {:users/roy/documents/knowledge/bibliographie/2009/sunar{\_}true random number generators for cryptography.pdf:pdf},
isbn = {978-0-387-71817-0},
keywords = {aes,csprng,entropy,fortuna,fpga,sha},
number = {july},
pages = {55--73},
publisher = {springer us},
title = {{true random number generators for cryptography}},
year = {2009}
}
@article{tipping1999,
abstract = {principal component analysis (pca) is one of the most popular techniques for processing, compressing, and visualizing data, although its effectiveness is limited by its global linearity. while nonlinear variants of pca have been proposed, an alternative paradigm is to capture data complexity by a combination of local linear pca projections. however, conventional pca does not correspond to a probability density, and so there is no unique way to combine pca models. therefore, previous attempts to formulate mixture models for pca have been ad hoc to some extent. in this article, pca is formulated within a maximum likelihood framework, based on a specific form of gaussian latent variable model. this leads to a well-defined mixture model for probabilistic principal component analyzers, whose parameters can be determined using an expectation-maximization algorithm. we discuss the advantages of this model in the context of clustering, density modeling, and local dimensionality reduction, and we demonstrate its application to image compression and handwritten digit recognition.},
author = {tipping, m e and bishop, c m},
doi = {10.1162/089976699300016728},
file = {:users/roy/documents/knowledge/bibliographie/1999/tipping, bishop{\_}mixtures of probabilistic principal component analyzers.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {neural computation},
number = {2},
pages = {443--482},
pmid = {9950739},
title = {{mixtures of probabilistic principal component analyzers.}},
volume = {11},
year = {1999}
}
@article{iooss2013c,
author = {iooss, bertrand},
file = {:users/roy/documents/knowledge/bibliographie/2013/iooss{\_}mod{\'{e}}lisation et propagation d'incertitudes.pdf:pdf},
pages = {1--90},
title = {{mod{\'{e}}lisation et propagation d'incertitudes}},
year = {2013}
}
@article{borgonovo2016,
abstract = {the solution of several operations research problems requires the creation of a quantitative model. sensitivity analysis is a crucial step in the model building and result communication process. through sensitivity analysis we gain essential insights on model behavior, on its structure and on its response to changes in the model inputs. several interrogations are possible and several sensitivity analysis methods have been developed, giving rise to a vast and growing literature. we present an overview of available methods, structuring them into local and global methods. for local methods, we discuss tornado diagrams, one way sensitivity functions, differentiation-based methods and scenario decomposition through finite change sensitivity indices, providing a unified view of the associated sensitivity measures. we then analyze global sensitivity methods, first discussing screening methods such as sequential bifurcation and the morris method. we then address variance-based, moment-independent and value of information-based sensitivity methods. we discuss their formalization in a common rationale and present recent results that permit the estimation of global sensitivity measures by post-processing the sample generated by a traditional monte carlo simulation. we then investigate in detail the methodological issues concerning the crucial step of correctly interpreting the results of a sensitivity analysis. a classical example is worked out to illustrate some of the approaches.},
author = {Borgonovo, Emanuele and Plischke, Elmar},
doi = {10.1016/j.ejor.2015.06.032 invited review},
file = {:users/roy/documents/knowledge/bibliographie/2016/borgonovo, plischke{\_}sensitivity analysis a review of recent advances.pdf:pdf},
isbn = {0377-2217},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {computer experiments,sensitivity analysis,simulation},
number = {3},
pages = {869--887},
publisher = {elsevier ltd.},
title = {{Sensitivity Analysis: a Review of Recent Advances}},
volume = {248},
year = {2016}
}
@article{conti2010,
abstract = {computer models are widely used in scientific research to study and predict the behaviour of complex systems. the run times of computer-intensive simulators are often such that it is impractical to make the thousands of model runs that are conventionally required for sensitivity analysis, uncertainty analysis or calibration. in response to this problem, highly efficient techniques have recently been developed based on a statistical meta-model (the emulator) that is built to approximate the computer model. the approach, however, is less straightforward for dynamic simulators, designed to represent time-evolving systems. generalisations of the established methodology to allow for dynamic emulation are here proposed and contrasted. advantages and difficulties are discussed and illustrated with an application to the sheffield dynamic global vegetation model, developed within the uk centre for terrestrial carbon dynamics. ?? 2009 elsevier b.v. all rights reserved.},
author = {conti, stefano and o'hagan, anthony},
doi = {10.1016/j.jspi.2009.08.006},
file = {:users/roy/documents/knowledge/bibliographie/2010/conti, o'hagan{\_}bayesian emulation of complex multi-output and dynamic computer models.pdf:pdf},
isbn = {0378-3758},
issn = {03783758},
journal = {journal of statistical planning and inference},
keywords = {bayesian inference,computer experiments,dynamic models,hierarchical models},
month = {mar},
number = {3},
pages = {640--651},
title = {{bayesian emulation of complex multi-output and dynamic computer models}},
volume = {140},
year = {2010}
}
@article{iooss2017a,
abstract = {a functional risk curve gives the probability of an undesirable event in function of the value of a critical parameter of a considered physical system.in several applicative situations, this curve is built using phenomenological numerical models which simulate complex physical phenomena.facing to cpu-time expensive numerical models, we propose to use the gaussian process regression model to build functional risk curve.an algorithm is given to provide confidence bounds due to this approximation.two methods of global sensitivity analysis of the model random input parameters on the functional risk curve is also studied.as important information is given by the pli sensitivity indices which allow to understand the effect of misjudgment on the input parameters' probability density functions.},
archiveprefix = {arxiv},
arxivid = {1704.00624},
author = {iooss, bertrand and gratiet, lo{\"{i}}c le},
eprint = {1704.00624},
file = {:users/roy/documents/knowledge/bibliographie/2017/iooss, gratiet{\_}uncertainty and sensitivity analysis of functional risk curves based on gaussian processes(2).pdf:pdf},
keywords = {computer experiments,gaussian process,indices,metamodel,non destructive testing,probability of,sobol,structural reliability},
month = {apr},
pages = {1--23},
title = {{uncertainty and sensitivity analysis of functional risk curves based on gaussian processes}},
year = {2017}
}
@inproceedings{owen,
author = {owen, art b},
booktitle = {monte carlo ray tracing: siggraph 2003 course 44},
file = {:users/roy/documents/knowledge/bibliographie/2003/owen{\_}quasi-monte carlo sampling.pdf:pdf},
pages = {69--88},
title = {{quasi-monte carlo sampling}},
year = {2003}
}
@article{iaccarino2008,
abstract = {in the last three decades, computer simulation tools have achieved wide spread use in the design and analysis of engineering devices. this has shortened the overall product design cycle and it has also provided bet- ter understanding of the operating behavior of the systems of interest. as a consequence numerical simulations have lead to a reduction of physical prototyping and to lower costs.},
author = {iaccarino, gianluca},
file = {:users/roy/documents/knowledge/bibliographie/2008/iaccarino{\_}quantification of uncertainty in flow simulations using probabilistic methods.pdf:pdf},
journal = {stanford.edu},
pages = {1--29},
title = {{quantification of uncertainty in flow simulations using probabilistic methods}},
year = {2008}
}
@article{roy2009,
abstract = {validation is a crucial aspect of quantitative structure-activity relationship (qsar) modeling. the present paper shows that traditionally used validation parameters (leave-one-out q(2) for internal validation and predictive r(2) for external validation) may be supplemented with two novel parameters r(m)(2) and r(p)(2) for a stricter test of validation. the parameter r(m)(2)((overall)) penalizes a model for large differences between observed and predicted values of the compounds of the whole set (considering both training and test sets) while the parameter r(p)(2) penalizes model r(2) for large differences between determination coefficient of nonrandom model and square of mean correlation coefficient of random models in case of a randomization test. two other variants of r(m)(2) parameter, r(m)(2)((loo)) and r(m)(2)((test)), penalize a model more strictly than q(2) and r(2)(pred) respectively. three different data sets of moderate to large size have been used to develop multiple models in order to indicate the suitability of the novel parameters in qsar studies. the results show that in many cases the developed models could satisfy the requirements of conventional parameters (q(2) and r(2)(pred)) but fail to achieve the required values for the novel parameters r(m)(2) and r(p)(2). moreover, these parameters also help in identifying the best models from among a set of comparable models. thus, a test for these two parameters is suggested to be a more stringent requirement than the traditional validation parameters to decide acceptability of a predictive qsar model, especially when a regulatory decision is involved.},
author = {{pratim roy}, partha and paul, somnath and mitra, indrani and roy, kunal},
doi = {10.3390/molecules14051660},
file = {:users/roy/documents/knowledge/bibliographie/2009/pratim roy et al.{\_}on two novel parameters for validation of predictive qsar models.pdf:pdf},
isbn = {1420-3049},
issn = {1420-3049},
journal = {molecules (basel, switzerland)},
keywords = {external validation,internal validation,qsar,randomization,validation},
month = {apr},
number = {5},
pages = {1660--701},
pmid = {19471190},
title = {{on two novel parameters for validation of predictive qsar models.}},
volume = {14},
year = {2009}
}
@article{hoel2016,
abstract = {this work embeds a multilevel monte carlo (mlmc) sampling strategy into the monte carlo step of the ensemble kalman filter (enkf), thereby yielding a multilevel ensemble kalman filter (mlenkf) which has provably superior asymptotic cost to a given accuracy level. the theoretical results are illustrated numerically.},
archiveprefix = {arxiv},
arxivid = {1608.08558},
author = {hoel, h{\aa}kon and law, kody j. h. and tempone, raul},
doi = {10.1137/15m100955x},
eprint = {1608.08558},
file = {:users/roy/documents/knowledge/bibliographie/2016/hoel, law, tempone{\_}multilevel ensemble kalman filtering.pdf:pdf},
issn = {0036-1429},
journal = {siam journal on numerical analysis},
keywords = {1,65c30,65y20,a system through sequential,ams subject classification,and,ensemble kalman filter,filtering,filtering refers to the,incorporation of online data,introduction,kalman filter,monte carlo,multilevel,or parameters p of,sequential estimation of the,state v,y},
month = {jan},
number = {3},
pages = {1813--1839},
title = {{multilevel ensemble kalman filtering}},
volume = {54},
year = {2016}
}
@techreport{fallis2013,
archiveprefix = {arxiv},
arxivid = {arxiv:1011.1669v3},
author = {laveyrie, beno{\^{i}}t},
doi = {10.1017/cbo9781107415324.004},
eprint = {arxiv:1011.1669v3},
file = {:users/roy/documents/knowledge/bibliographie/2004/laveyrie{\_}optimisation en calculs a{\'{e}}rodynamiques avec une m{\'{e}}thode de krigeage.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
pmid = {25246403},
title = {{optimisation en calculs a{\'{e}}rodynamiques avec une m{\'{e}}thode de krigeage}},
year = {2004}
}
@article{peherstorfer2016a,
author = {peherstorfer, benjamin and willcox, karen and gunzburger, max},
doi = {10.1137/15m1046472},
file = {:users/roy/documents/knowledge/bibliographie/2016/peherstorfer, willcox, gunzburger{\_}optimal model management for multifidelity monte carlo estimation.pdf:pdf},
issn = {1064-8275},
journal = {siam journal on scientific computing},
keywords = {monte carlo simulation,model reduction,multifidelity,surrogate modeling},
month = {jan},
number = {5},
pages = {a3163--a3194},
title = {{optimal model management for multifidelity monte carlo estimation}},
volume = {38},
year = {2016}
}
@inproceedings{lin2016,
abstract = {the flow in a generic, high-pressure turbine vane was simu- lated using an in-houseddes code. two different operating con- ditions were simulated with one leading to a shock wave while the other does not. one case was used to validate the capabil- ity of the ddes method to capture shock waves and other flow structures using an inlet reynolds number of 271,000 and an exit mach number of 0.840. the test conditions for the other case were an inlet reynolds number of 265,000 and an exit mach number of 0.927, which is representative of a transonic, high pressure turbine vane which was used to further investigate the flow field. the ddes simulations from the first case are com- pared with published experimental data, rans simulations and les simulations. then, ddes results for two cases with adi- abatic and isothermal boundary conditions are compared. the numerical simulations with the isothermal boundary condition are further used to study the flow phenomena with wake vor- tices, shock waves, pressure waves, wake-shock interactions, and wake-pressure wave interactions. the effects of the pressure waves on the vane heat transfer are also analyzed.},
address = {seoul},
author = {lin, dun and su, xinrong and yuan, xin},
booktitle = {turbomachinery technical conference and exposition gt2016},
file = {:users/roy/documents/knowledge/bibliographie/2016/lin, su, yuan{\_}delayed detached-eddy simulations of a high pressure turbine vane.pdf:pdf},
pages = {1--12},
title = {{delayed detached-eddy simulations of a high pressure turbine vane}},
year = {2016}
}
@phdthesis{liu2003,
author = {liu, weiyu},
file = {:users/roy/documents/knowledge/bibliographie/2003/liu{\_}development of gradient-enhanced kriging approximations for multidisciplinary design optimization.pdf:pdf},
school = {notre dame},
title = {{development of gradient-enhanced kriging approximations for multidisciplinary design optimization}},
year = {2003}
}
@inproceedings{padron2014,
address = {reston, virginia},
author = {Padron, Andres S. and Alonso, Juan J. and Palacios, Francisco and Barone, Matthew F. and Eldred, Michael S.},
booktitle = {15th aiaa/issmo multidisciplinary analysis and optimization conference},
doi = {10.2514/6.2014-3013},
file = {:users/roy/documents/knowledge/bibliographie/2014/padron et al.{\_}multi-fidelity uncertainty quantification application to a vertical axis wind turbine under an extreme gust.pdf:pdf},
isbn = {978-1-62410-283-7},
month = {jun},
number = {june},
pages = {1--18},
publisher = {{AIAA}},
title = {{Multi-fidelity uncertainty quantification: application to a vertical axis wind turbine under an extreme gust}},
year = {2014}
}
@incollection{jiang2016,
abstract = {in physics-based engineering modeling, two primary sources of model uncer- tainty that account for the differences between computer models and physical experiments are parameter uncertainty and model discrepancy. one of the main challenges in model updating results from the difficulty in distinguishing between the effects of calibration parameters versus model discrepancy. in this chapter, this identifiability problem is illustrated with several examples that explain the mechanisms behind it and that attempt to shed light onwhen a system may or may not be identifiable. for situations in which identifiability cannot be achieved using only a single response, an approach is developed to improve identifiability by usingmultiple responses that share a mutual dependence on the calibration parameters. furthermore, prior to conducting physical experiments but after conducting computer simulations, in order to address the issue of how to select the most appropriate set of responses to measure experimentally to best enhance identifiability, a preposterior analysis approach is presented to predict the degree of identifiability that will result from using different sets of responses to measure experimentally. to handle the computational challenges of the preposterior analysis, we also present a surrogate preposterior analysis based on the fisher information of the calibration parameters.},
author = {jiang, zhen and arendt, paul d and apley, daniel w and chen, wei},
booktitle = {handbook of uncertainty quantification},
doi = {10.1007/978-3-319-11259-6},
editor = {ghanem, roger and higdon, david and owhadi, houman},
file = {:users/roy/documents/knowledge/bibliographie/2015/jiang et al.{\_}multi-response approach to improving identifiability in model calibration(2).pdf:pdf},
isbn = {978-3-319-11259-6},
keywords = {(non)identifiability,bias correction,calibration,calibration parameters,discrepancy function,experimental uncertainty,fixed-theta preposterior analysis,gaussian process,hyperparameters,identifiability,model discrepancy,model uncer- tainty quantification,modular bayesian approach,multi-response gaussian process,multi-response modular bayesian approach,non-informative prior,non-spatial covariance,observed fisher information,parameter uncertainty,preposterior analysis,preposterior covariance,simply supported beam,spatial correlation,surrogate preposterior analysis},
publisher = {springer international publishing},
title = {{multi-response approach to improving identifiability in model calibration}},
year = {2015}
}
@article{hullman2015,
abstract = {many visual depictions of probability distributions, such as error bars, are difficult for users to accurately interpret.we present and study an alternative representation, hypothetical outcome plots (hops), that animates a finite set of individual draws. in contrast to the sta- tistical background required to interpret many static representations of distributions, hops require relatively little background knowledge to interpret. instead, hops enables viewers to infer properties of the distribution using mental processes like counting and integration. we conducted an experiment comparing hops to error bars and violin plots. with hops, people made much more accurate judgments about plots of two and three quantities. accu- racy was similar with all three representations for most questions about distributions of a single quantity.},
author = {Hullman, Jessica and Resnick, Paul and Adar, Eytan},
doi = {10.1371/journal.pone.0142444},
file = {:users/roy/documents/knowledge/bibliographie/2015/hullman, resnick, adar{\_}hypothetical outcome plots outperform error bars and violin plots for inferences about reliability of variable(2).pdf:pdf},
journal = {Plos One},
pages = {1--25},
title = {{Hypothetical outcome plots outperform error bars and violin plots for inferences about reliability of variable ordering}},
year = {2015}
}
@incollection{woods2016,
abstract = {the aim of this paper is to review methods of designing screening experiments, ranging from designs originally developed for physical experiments to those especially tailored to experiments on numerical models. the strengths and weaknesses of the various designs for screening variables in numerical models are discussed. first, classes of factorial designs for experiments to estimate main effects and interactions through a linear statistical model are described, specifically regular and nonregular fractional factorial designs, supersaturated designs, and systematic fractional replicate designs. generic issues of aliasing, bias, and cancellation of factorial effects are discussed. second, group screening experiments are considered including factorial group screening and sequential bifurcation. third, random sampling plans are addressed including latin hyper- cube sampling and sampling plans to estimate elementary effects. fourth, a variety of modeling methods commonly employed with screening designs are briefly described. finally, a novel study demonstrates six screening methods on two frequently-used exemplars, and their performances are compared.},
author = {woods, david c and lewis, susan m},
booktitle = {handbook of uncertainty quantification},
doi = {10.1007/978-3-319-11259-6_33-1},
file = {:users/roy/documents/knowledge/bibliographie/2015/woods, lewis{\_}design of experiments for screening.pdf:pdf},
isbn = {978-3-319-12384-4},
keywords = {computer experiments,gaussian process models,fractional factorial designs,group screening,space-filling designs,supersaturated designs,variable selection},
pages = {1--43},
publisher = {springer international publishing},
title = {{design of experiments for screening}},
year = {2015}
}
@incollection{becker2016,
abstract = {multidimensional measures (often termed composite indicators) are popular tools in the public discourse for assessing the performance of countries on human development, perceived corruption, innovation, competitiveness, or other complex phenomena. these measures combine a set of variables using an aggregation formula, which is often a weighted arithmetic average. the values of the weights are usually meant to reflect the variables importance in the index. this paper uses measures drawn from global sensitivity analysis, specifically the pearson correlation ratio, to discuss to what extent the importance of each variable coincides with the intentions of the developers. two nonparametric regression approaches are used to provide alternative estimates of the correlation ratios, which are compared with linear measures. the relative advantages of different estimation procedures are discussed. three case studies are investigated: the resource governance index, the good country index, and the financial secrecy index.},
author = {becker, william and paruolo, paolo and saisana, michaela and saltelli, andrea},
booktitle = {handbook of uncertainty quantification},
doi = {10.1007/978-3-319-11259-6_40-1},
file = {:users/roy/documents/knowledge/bibliographie/2015/becker et al.{\_}weights and importance in composite indicators mind the gap.pdf:pdf},
isbn = {978-3-319-12384-4},
keywords = {composite indicators,nonlinear regression,sensitivity analysis},
pages = {1--30},
publisher = {springer international publishing},
title = {{weights and importance in composite indicators: mind the gap}},
year = {2015}
}
@article{wu2015,
abstract = {in this work we consider a class of uncertainty quantification problems where the system performance or reliability is characterized by a scalar parameter y. the performance parameter y is random due to the presence of various sources of un-certainty in the system, and our goal is to estimate the probability density function (pdf) of y. we propose to use the multicanonical monte carlo (mmc) method, a special type of adaptive importance sampling algorithm, to compute the pdf of interest. moreover, we develop an adaptive algorithm to construct local gaus-sian process surrogates to further accelerate the mmc iterations. with numerical examples we demonstrate that the proposed method can achieve several orders of magnitudes of speedup over the standard monte carlo method.},
archiveprefix = {arxiv},
arxivid = {arxiv:1508.06700v1},
author = {wu, keyi and li, jinglai},
doi = {10.1016/j.jcp.2016.06.020},
eprint = {arxiv:1508.06700v1},
file = {:users/roy/documents/knowledge/bibliographie/2016/wu, li{\_}a surrogate accelerated multicanonical monte carlo method for uncertainty quantification.pdf:pdf},
issn = {00219991},
journal = {journal of computational physics},
keywords = {gaussian processes,multicanonical monte carlo,uncertainty quantification},
month = {sep},
number = {april 2016},
pages = {1098--1109},
title = {{a surrogate accelerated multicanonical monte carlo method for uncertainty quantification}},
volume = {321},
year = {2016}
}
@inproceedings{correll2017a,
abstract = {thematic maps are commonly used for visualizing the density of events in spatial data. however, these maps can mislead by giving visual prominence to known base rates (such as population densities) or to artifacts of sample size and normalization (such as outliers arising from smaller, and thus more variable, samples). in this work, we adapt bayesian surprise to generate maps that counter these biases. bayesian surprise, which has shown promise for modeling human visual attention, weights information with respect to how it updates beliefs over a space of models. we introduce surprise maps, a visualization technique that weights event data relative to a set of spatio-temporal models. unexpected events (those that induce large changes in belief over the model space) are visualized more prominently than those that follow expected patterns. using both synthetic and real-world datasets, we demonstrate how surprise maps overcome some limitations of traditional event maps.},
author = {correll, michael and heer, jeffrey},
booktitle = {ieee trans. visualization {\&} comp. graphics (proc. infovis)},
file = {:users/roy/documents/knowledge/bibliographie/2017/correll, heer{\_}surprise! bayesian weighting for de-biasing thematic maps.pdf:pdf},
keywords = {bayesian surprise,event visualization,spatio-temporal data,thematic maps},
mendeley-tags = {bayesian surprise,event visualization,spatio-temporal data,thematic maps},
title = {{surprise! bayesian weighting for de-biasing thematic maps}},
year = {2017}
}
@book{hermann2011,
abstract = {the chapter provides a brief introduction to sonification and auditory display, and suggests a vision on where sonification could be in 50 years. finally the chapter explains the organization of the material in this volume and provides suggestions how to use the book and the media files.},
archiveprefix = {arxiv},
arxivid = {arxiv:1011.1669v3},
author = {hermann, thomas and hunt, andy and neuhoff, john g},
booktitle = {the sonification handbook},
doi = {10.1017/cbo9781107415324.004},
eprint = {arxiv:1011.1669v3},
file = {:users/roy/documents/knowledge/bibliographie/2011/hermann, hunt, neuhoff{\_}the sonification handbook.pdf:pdf},
isbn = {9783832528195},
issn = {1098-6596},
pages = {301--324},
pmid = {25246403},
title = {{the sonification handbook}},
year = {2011}
}
@article{pronzato2017,
abstract = {a few properties of minimax and maximin optimal designs in a compact subset ofrd are presented, and connections with other space-filling constructions are indicated. several methods are given for the evaluation of the minimax-distance (or dispersion) criterion for a given n-point design. various optimisation methods are proposed and their limitations, in particular in terms of dimension d, are indicated. a large majority of the results presented are not new, but their collection in a single document containing a respectable bibliography will hopefully be useful to the reader.},
author = {Pronzato, Luc},
file = {:users/roy/documents/knowledge/bibliographie/2017/pronzato{\_}minimax and maximin space-filling designs some properties and methods for construction.pdf:pdf},
journal = {Journal de la Soci{\'{e}}t{\'{e}} Fran{\c{c}}aise de Statistique},
keywords = {computer experiments,maximin-optimal design,minimax-optimal design,space-filling design,sphere covering,sphere packing},
number = {1},
pages = {7--36},
title = {{Minimax and maximin space-filling designs : some properties and methods for construction}},
volume = {158},
year = {2017}
}
@incollection{marrel2015,
abstract = {this section presents several sensitivity analysis methods to deal with spatial and/or temporal models. focusing on the variance-based approach, solutions are proposed to perform global sensitivity analysis with functional inputs and outputs. some of these solutions are illustrated on two industrial case studies: an environmental model for flood risk assessment and an atmospheric dispersion model for radionuclide release. these test cases are fully described at the beginning of the paper. then a section is dedicated to spatiotemporal inputs and proposes several sensitivity analysis methods. the use of metamodels is also addressed. pros and cons of the various methods are then discussed. in a subsequent section, solutions to deal with spatiotemporal outputs are proposed: aggregated, site, and block indices are described. the use of functional metamodels for sensitivity analysis purpose is also discussed.},
address = {cham},
author = {marrel, amandine and saint-geours, nathalie and {de lozzo}, matthias},
booktitle = {handbook of uncertainty quantification},
doi = {10.1007/978-3-319-11259-6_39-1},
editor = {ghanem, roger and higdon, david and owhadi, houman},
file = {:users/roy/documents/knowledge/bibliographie/2015/marrel, saint-geours, de lozzo{\_}sensitivity analysis of spatial andor temporal phenomena.pdf:pdf},
isbn = {978-3-319-11259-6},
keywords = {spatiotemporal inputs • spatiotemporal outputs • m},
pages = {1--31},
publisher = {springer international publishing},
title = {{sensitivity analysis of spatial and/or temporal phenomena}},
year = {2015}
}
@incollection{rao2006,
abstract = {cross validation allows models to be tested using the full training set by means of repeated resampling; thus, maximizing the total number of points used for testing and potentially, helping to protect against overfitting. improvements in computational power, recent reductions in the (computational) cost of classification algorithms, and the development of closed-form solutions (for performing cross validation in certain classes of learning algorithms) makes it possible to test thousand or millions of variants of learning models on the data. thus, it is now possible to calculate cross validation performance on a much larger number of tuned models than would have been possible otherwise. however, we empirically show how under such large number of models the risk for overfitting increases and the performance estimated by cross validation is no longer an effective estimate of generalization; hence, this paper provides an empirical reminder of the dangers of cross validation. we use a closed-form solution that makes this evaluation possible for the cross validation problem of interest. in addition, through extensive experiments we expose and discuss the effects of the overuse/misuse of cross validation in various aspects, including model selection, feature selection, and data dimensionality. this is illustrated on synthetic, benchmark, and real-world data sets.},
address = {philadelphia, pa},
author = {rao, r bharat and fung, glenn and rosales, romer},
booktitle = {proceedings of the 2008 siam international conference on data mining},
doi = {10.1137/1.9781611972788.54},
file = {:users/roy/documents/knowledge/bibliographie/2008/rao, fung, rosales{\_}on the dangers of cross-validation. an experimental evaluation.pdf:pdf},
isbn = {9781605603179},
month = {apr},
pages = {588--596},
publisher = {society for industrial and applied mathematics},
title = {{on the dangers of cross-validation. an experimental evaluation}},
year = {2008}
}
@article{halton1960,
author = {halton, j. h.},
doi = {10.1007/bf01386213},
file = {:users/roy/documents/knowledge/bibliographie/1960/halton{\_}on the efficiency of certain quasi-random sequences of points in evaluating multi-dimensional integrals.pdf:pdf},
isbn = {0029-599x},
issn = {0029-599x},
journal = {numerische mathematik},
month = {dec},
number = {1},
pages = {84--90},
title = {{on the efficiency of certain quasi-random sequences of points in evaluating multi-dimensional integrals}},
volume = {2},
year = {1960}
}
@article{safta2016a,
author = {safta, cosmin and blaylock, myra and templeton, jeremy and domino, stefan and sargsyan, khachik and najm, habib},
doi = {10.1002/fld.4272},
file = {:users/roy/documents/knowledge/bibliographie/2016/safta et al.{\_}uncertainty quantification in les of channel flow.pdf:pdf},
issn = {02712091},
journal = {international journal for numerical methods in fluids},
keywords = {10.1002/fld.4272 and large eddy simulation,accepted for publication and,bayesian framework,but has not,calibration,chaos,csafta,gov,large eddy simulation,model error,polynomial,polynomial chaos,rosenblatt transformation,sandia,this article has been,to,undergone full peer review},
title = {{uncertainty quantification in les of channel flow}},
year = {2016}
}
@inproceedings{kay2016,
author = {Kay, Matthew and Kola, Tara and Hullman, Jessica R. and Munson, Sean A.},
booktitle = {proceedings of the 2016 chi conference on human factors in computing systems},
doi = {10.1145/2858036.2858558},
file = {:users/roy/documents/knowledge/bibliographie/2016/kay et al.{\_}when ( ish ) is my bus user-centered visualizations of uncertainty in everyday , mobile predictive systems.pdf:pdf},
pages = {5092--5103},
title = {{When ( ish ) is my bus ? user-centered visualizations of uncertainty in everyday , mobile predictive systems}},
year = {2016}
}
@inproceedings{jee2016,
abstract = {it is important to be able to predict the state of the boundary layer on a high-pressure turbine vane with incoming turbulence. the laminar-to-turbulent transition significantly changes the heat transfer on the blade, which impacts the turbine performance and durability assessment. in this study, an experimental cascade with a high-pressure turbine vane was simulated with large-eddy simulation (les). to incorporate the impact of turbulence from the freestream on the boundary layer, appropriate turbulence data sets were generated in separate direct-numerical simulation and fed into the les inlet. the state of the boundary layer is reasonably well predicted for the test condition of the turbulence intensity of tu=4{\%}. streamwise vortical structures and spanwise two-dimensional waves are observed in the transitional boundary layer. discrepancy for the case with the higher turbulence intensity tu=6{\%} is discussed with the focus on the need for more detailed information about the freestream disturbances.},
address = {seoul},
author = {jee, solkeun and joo, jongwook and medic, gorazd},
booktitle = {turbomachinery technical conference and exposition gt2016},
file = {:users/roy/documents/knowledge/bibliographie/2016/jee, joo, medic{\_}large-eddy simulation of a high-pressure turbine vane with inlet turbulence.pdf:pdf},
pages = {1--12},
title = {{large-eddy simulation of a high-pressure turbine vane with inlet turbulence}},
year = {2016}
}

@article{picheny2014a,
abstract = {kriging-based optimization relying on noisy evaluations of complex systems has recently motivated contributions from various research communities. five strategies have been implemented in the diceoptim package. the corresponding functions constitute a user-friendly tool for solving expensive noisy optimization problems in a sequential framework, while offering some flexibility for advanced users. besides, the implementation is done in a unified environment, making this package a useful device for studying the relative performances of existing approaches depending on the experimental setup. an overview of the package structure and interface is provided, as well as a description of the strategies and some insight about the implementation challenges and the proposed solutions. the strategies are compared to some existing optimization packages on analytical test functions and show promising performances. ?? 2013 elsevier inc. all rights reserved.},
author = {picheny, victor and ginsbourger, david},
doi = {10.1016/j.csda.2013.03.018},
file = {:users/roy/documents/knowledge/bibliographie/2014/picheny, ginsbourger{\_}noisy kriging-based optimization methods a unified implementation within the diceoptim package.pdf:pdf},
issn = {01679473},
journal = {computational statistics and data analysis},
keywords = {active learning,computer experiments,gaussian processes},
pages = {1035--1053},
publisher = {elsevier b.v.},
title = {{noisy kriging-based optimization methods: a unified implementation within the diceoptim package}},
volume = {71},
year = {2014}
}
@article{lemaitre2015,
author = {lema{\^{i}}tre, p. and sergienko, e. and arnaud, a. and bousquet, n. and gamboa, f. and iooss, b.},
doi = {10.1080/00949655.2013.873039},
file = {:users/roy/documents/knowledge/bibliographie/2015/lema{\^{i}}tre et al.{\_}density modification-based reliability sensitivity analysis.pdf:pdf},
issn = {0094-9655},
journal = {journal of statistical computation and simulation},
keywords = {computer experiment,kullback,leibler,sensitivity analysis,structural reliability,uncertainty},
month = {apr},
number = {6},
pages = {1200--1223},
title = {{density modification-based reliability sensitivity analysis}},
volume = {85},
year = {2015}
}
@article{gigerenzer1995,
abstract = {is the mind, by design, predisposed against performing bayesian inference? previous research on base rate neglect suggests that the mind lacks the appropriate cognitive algorithms. however, any claim against the existence of an algorithm, bayesian or otherwise, is impossible to evaluate unless one specifies the information format in which it is designed to operate. the authors show that bayes- ian algorithms are computationally simpler in frequency formats than in the probability formats used in previous research. frequency formats correspond to the sequential way information is ac- quired in natural sampling, from animal foraging to neural networks. by analyzing several thousand solutions to bayesian problems, the authors found that when information was presented in frequency formats, statistically naive participants derived up to 50{\%} of all inferences by bayesian algorithms. non-bayesian algorithms included simple versions of fisherian and neyman-pearsonian inference.},
author = {Gigerenzer, Gerd and Hoffrage, Ulrich},
doi = {10.1037/0033-295x.102.4.684},
file = {:users/roy/documents/knowledge/bibliographie/1995/gigerenzer, hoffrage{\_}how to improve bayesian reasoning without instruction frequency formats.pdf:pdf},
journal = {Psychological Review},
number = {4},
pages = {684--704},
title = {{How to improve bayesian reasoning without instruction : frequency formats}},
volume = {102},
year = {1995}
}
@phdthesis{ginsbourger2009,
abstract = {cette th{\`{e}}se s'inscrit dans la th{\'{e}}matique de planification d'exp{\'{e}}riences num{\'{e}}riques. elle porte plus pr{\'{e}}cis{\'{e}}ment sur l'optimisation de simulateurs num{\'{e}}riques co{\^{u}}teux {\`{a}} {\'{e}}valuer, par des strat{\'{e}}gies d? {\'{e}}chantillonnage bas{\'{e}}es sur des repr{\'{e}}sentations simplifi{\'{e}}es du simulateur, les metamod{\`{e}}les. une fois choisi un metamod{\`{e}}le parmi les familles existantes (polyn{\^{o}}mes, splines, mod{\`{e}}les additifs, krigeage, r{\'{e}}seaux de neurones), on estime les param{\`{e}}tres du metamod{\`{e}}le. on dispose alors d'une repr{\'{e}}sentation simplifi{\'{e}}e du simulateur, que l'on pourra faire {\'{e}}voluer en fonction des informations apport{\'{e}}es par de nouvelles {\'{e}}valuations. etant donn{\'{e}} qu'il est difficile de savoir a priori quel sera le type de metamod{\`{e}}le capable de guider au mieux un algorithme d'optimisation, une des motivations de ce travail est d'examiner comment une construction ad hoc de la structure du metamod{\`{e}}le, voire la prise en compte de plusieurs metamod{\`{e}}les, peuvent am{\'{e}}liorer les m{\'{e}}thodes d'approximation et les strat{\'{e}}gies d'optimisation globale actuellement employ{\'{e}}es. cela soul{\`{e}}ve {\`{a}} la fois des questions math{\'{e}}matiques et statistiques de s{\'{e}}lection de mod{\`{e}}le (quelles familles de m{\'{e}}tamod{\`{e}}les consid{\'{e}}rer ? comment estimer les termes de covariance et/ou de tendance d'un m{\'{e}}tamod{\`{e}}le de krigeage, et selon quels crit{\`{e}}res les {\'{e}}valuer ? comment prendre en compte certaines formes d'instationnarit{\'{e}} dans la covariance de krigeage que sont les sym{\'{e}}tries et la pr{\'{e}}sence de bruits d'observation h{\'{e}}t{\'{e}}rog{\`{e}}nes ?), de combinaison de mod{\`{e}}les (une fois un ensemble de metamod{\`{e}}les choisis, comment agr{\`{e}}ge-t- on les pseudo-informations qu'ils nous apportent ?), et de d{\'{e}}finition de crit{\`{e}}res d{\'{e}}cisionnels pour guider les {\'{e}}valuations au sein d'algorithmes d'optimisation (comment parall{\'{e}}liser ego ou des proc{\'{e}}dures similaires d'exploration sur base de krigeage ?).},
author = {ginsbourger, david},
file = {:users/roy/documents/knowledge/bibliographie/2009/ginsbourger{\_}multiples m{\'{e}}tamod{\`{e}}les pour l'approximation et l'optimisation de fonctions num{\'{e}}riques multivariables.pdf:pdf},
keywords = {calcul distribu{\'{e}} synchrone,gaussian processes,global optimization,kernel methods,krigeage,kriging,maximum likelihood,maximum de vraisemblance,mixtures of experts,m{\'{e}}langes d'experts,noyaux de covariance,optimisation globale,synchronous distributed computingprocessus al{\'{e}}atoi},
title = {{multiples m{\'{e}}tamod{\`{e}}les pour l'approximation et l'optimisation de fonctions num{\'{e}}riques multivariables}},
year = {2009}
}
@incollection{zeiler2014,
abstract = {large convolutional network models have recently demonstrated impressive classification performance on the imagenet benchmark krizhevsky et al. [18]. however there is no clear understanding of why they perform so well, or how they might be improved. in this paper we explore both issues. we introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. used in a diagnostic role, these visualizations allow us to find model architectures that outperform krizhevsky et al on the imagenet classification benchmark. we also perform an ablation study to discover the performance contribution from different model layers. we show our imagenet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on caltech-101 and caltech-256 datasets.},
archiveprefix = {arxiv},
arxivid = {1311.2901},
author = {zeiler, matthew d. and fergus, rob},
booktitle = {computer vision–eccv 2014},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
file = {:users/roy/documents/knowledge/bibliographie/2014/zeiler, fergus{\_}visualizing and understanding convolutional networks.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
pages = {818--833},
pmid = {26353135},
title = {{visualizing and understanding convolutional networks}},
volume = {8689},
year = {2014}
}
@techreport{boumhaout2016,
author = {boumhaout, khalid},
file = {:users/roy/documents/knowledge/bibliographie/2016/boumhaout{\_}rapport de stage m{\'{e}}thodes robustes et estimation it{\'{e}}rative en analyse de sensibilit{\'{e}}.pdf:pdf},
institution = {edf},
title = {{rapport de stage m{\'{e}}thodes robustes et estimation it{\'{e}}rative en analyse de sensibilit{\'{e}}}},
year = {2016}
}
@article{iooss2013d,
author = {iooss, bertrand},
file = {:users/roy/documents/knowledge/bibliographie/2013/iooss{\_}planification et analyse d'exp{\'{e}}riences num{\'{e}}riques.pdf:pdf},
title = {{planification et analyse d'exp{\'{e}}riences num{\'{e}}riques}},
year = {2013}
}
@article{dupoirieux2016,
author = {dupoirieux, f and diagnostics, associated optical and cochet, a and bodoc, v and brossard, c and dessornes, o and guin, c and orain, m and diagnostics, associated optical and grisch, f and boukhalfa, a and cabot, g and renou, b and vandel, a and experiments, associated validation and pichon, t le and laverdant, a and scherrer, d and dessornes, o and ferrier, m and moule, y and fiorina, b and franzelli, b and darabiha, n and massot, m and dayma, g and moureau, v and vervisch, l and berlemont, a and riber, e and cuenot, b and vicquelin, r and riber, e and moureau, v and lartigue, g and figuer, a and mery, y and lamouroux, j and richard, s and gicquel, l and schmitt, t and candel, s and experiments, associated validation and dupoirieux, f and bertier, n and guin, c and eberle, c and ortega, i k and delhaye, d and ferry, d and focsa, c and irimiea, c and carpentier, y and chazallon, b and laffon, c and penanhoat, o and harivel, n and vancassel, x and simon, f and orain, m and bodoc, v and huet, m and vuillot, f and bertier, n and mazur, m and kings, n and tao, w and richecoeur, f and ducruix, s and lapeyre, c and poinsot, t and turbine, power gas and vedeshkin, g k and sverdlov, e d and dubovitsky, a n and bellenoue, m and boust, b and zitoun, r and gaillard, t and davidenko, d and leyko, m and naour, b le and fabignon, y and anthoine, j and davidenko, d and devillers, r and dupays, j and gueyffier, d and hijlkema, j and lupoglazoff, n and lamet, j m and guy, a and erades, c and messineo, j and hijlkema, j and casalis, g and anthoine, j and vingert, l and ordonneau, g and fdida, n and touze, c le and matuszewski, l and murrone, a and appriou, alain and board, editorial and appriou, alain and bidaud, philippe and busso, esteban and jacquin, laurent and touboul, pierre},
doi = {10.12762/2016.al11},
file = {:users/roy/documents/knowledge/bibliographie/2016/dupoirieux et al.{\_}challenges in combustion for aerospace propulsion.pdf:pdf},
journal = {aerospace lab},
number = {11},
title = {{challenges in combustion for aerospace propulsion}},
year = {2016}
}
@article{joseph2008,
abstract = {kriging is a useful method for developing metamodels for product design optimization. the most popular kriging method, known as ordinary kriging, uses a constant mean in the model. in this article, a modified kriging method is proposed, which has an unknown mean model. therefore, it is called blind kriging. the unknown mean model is identified from experimental data using a bayesian variable selection technique. many examples are presented, which show remarkable improvement in prediction using blind kriging over ordinary kriging. moreover, a blind kriging predictor is easier to interpret and seems to be more robust against mis-specification in the correlation parameters.},
author = {joseph, v roshan and hung, ying and sudjianto, agus},
doi = {10.1115/1.2829873},
file = {:users/roy/documents/knowledge/bibliographie/2008/joseph, hung, sudjianto{\_}blind kriging a new method for developing metamodels.pdf:pdf},
issn = {10500472},
journal = {journal of mechanical design},
keywords = {asme journal of mechanical,computer experiments,cross validation,design,design optimization,finite element models,kriging,metamodels,to appear in the,variable selection},
number = {3},
pages = {1--8},
title = {{blind kriging: a new method for developing metamodels}},
volume = {130},
year = {2008}
}
@article{zhang2015,
abstract = {the prometheus combustor design system aims to reduce the complexity of evaluating combustor designs by automatically defining preprocessing, simulation, and postprocessing tasks based on the automatic identification of combustor features within the computer-aided design (cad) environment. this system enables best practice to be codified and topological changes to a combustor's design to be more easily considered within an automated design process. the following paper presents the prometheus combustor design system and its application to the multiobjective isothermal optimization of a combustor prediffuser and the multifidelity isothermal optimization of a fuel injector feed arm in combination with a surrogate modeling strategy accelerated via a high-performance graphical processing unit (gpu).},
author = {zhang, xu and toal, david j. j. and keane, andy j. and witham, frederic and gregory, jonathan and ravikanti, murthy and aurifeille, emmanuel and stow, simon and rogers, mark and zedda, marco},
doi = {10.1115/1.4031711},
file = {:users/roy/documents/knowledge/bibliographie/2015/zhang et al.{\_}isothermal combustor prediffuser and fuel injector feed arm design optimization using the prometheus design system.pdf:pdf},
issn = {0742-4795},
journal = {journal of engineering for gas turbines and power},
month = {nov},
number = {6},
pages = {061504},
title = {{isothermal combustor prediffuser and fuel injector feed arm design optimization using the prometheus design system}},
volume = {138},
year = {2015}
}
@article{koziel2013,
abstract = {computational fluid dynamic (cfd) models are ubiquitous in aerodynamic design. variable-fidelity optimization algorithms have proven to be computationally efficient and therefore suitable to reduce high cpu-cost related to the design process solely based on accurate cfd models. a convenient way of constructing the variable-fidelity models is by using the high-fidelity solver, but with a varying degree of discretization and reduced number of flow solver iterations. so far, selection of the appropriate parameters has only been guided by the designer experience. in this paper, an automated lowfidelity model selection technique is presented. by defining the problem as a constrained nonlinear optimization problem, suitable grid and flow solver parameters are obtained. our approach is compared to conventional methods of generating a family of variable-fidelity models. comparison of the standard and the proposed approaches in the context of aerodynamic design of a transonic airfoil indicates that the automated model generation can yield significant computational savings. ?? 2013 the authors. published by elsevier b.v.},
author = {koziel, slawomir and leifsson, leifur},
doi = {10.1016/j.procs.2013.05.254},
file = {:users/roy/documents/knowledge/bibliographie/2013/koziel, leifsson{\_}multi-level cfd-based airfoil shape optimization with automated low-fidelity model selection.pdf:pdf},
issn = {18770509},
journal = {procedia computer science},
keywords = {aerodynamic design,low-fidelity model selection,multi-level algorithm,numerical optimization},
pages = {889--898},
publisher = {elsevier b.v.},
title = {{multi-level cfd-based airfoil shape optimization with automated low-fidelity model selection}},
volume = {18},
year = {2013}
}
@article{peherstorfer2015,
abstract = {estimating statistics of model outputs with the monte carlo method often requires a large number of model evaluations. this leads to long runtimes if the model is expensive to evaluate. importance sampling is one approach that can lead to a reduction in the number of model evaluations. importance sampling uses a biasing distribution to sample the model more efficiently, but generating such a biasing distribution can be difficult and usually also requires model evaluations. a different strategy to speed up monte carlo sampling is to replace the computationally expensive high-fidelity model with a computationally cheap surrogate model; however, because the surrogate model outputs are only approximations of the high-fidelity model outputs, the estimate obtained using a surrogate model is in general biased with respect to the estimate obtained using the high-fidelity model. we introduce a multifidelity importance sampling (mfis) method, which combines evaluations of both the high-fidelity and a surrogate model. it uses a surrogate model to facilitate the construction of the biasing distribution, but relies on a small number of evaluations of the high-fidelity model to derive an unbiased estimate of the statistics of interest. we prove that the mfis estimate is unbiased even in the absence of accuracy guarantees on the surrogate model itself. the mfis method can be used with any type of surrogate model, such as projection-based reduced-order models and data-fit models. furthermore, the mfis method is applicable to black-box models, i.e., where only inputs and the corresponding outputs of the high-fidelity and the surrogate model are available but not the details of the models themselves. we demonstrate on nonlinear and time-dependent problems that our mfis method achieves speedups of up to several orders of magnitude compared to monte carlo with importance sampling that uses the high-fidelity model only.},
author = {Peherstorfer, Benjamin and Cui, Tiangang and Marzouk, Youssef and Willcox, Karen},
doi = {10.1016/j.cma.2015.12.002},
file = {:users/roy/documents/knowledge/bibliographie/2016/peherstorfer et al.{\_}multifidelity importance sampling.pdf:pdf},
issn = {00457825},
journal = {Computer methods in applied mechanics and engineering},
keywords = {importance sampling,monte carlo method,multifidelity monte carlo,multifidelity uq,multifidelity design,multifidelity methods,multifidelity uncertainty quantification,surrogate modeling},
month = {mar},
pages = {490--509},
title = {{Multifidelity importance sampling}},
volume = {300},
year = {2016}
}
@article{ahmed2009,
author = {ahmed, m y m and qin, n},
file = {:users/roy/documents/knowledge/bibliographie/2009/ahmed, qin{\_}surrogate-based aerodynamic design optimization use of surrogates in aerodynamic design optimization.pdf:pdf},
journal = {13th international conference on aerospace sciences {\&} aviation technology, asat-13, cairo, egipt},
keywords = {metamodeling,metamodels,surrogate-assisted optimization},
pages = {asat--13--ae--14},
title = {{surrogate-based aerodynamic design optimization: use of surrogates in aerodynamic design optimization}},
year = {2009}
}
@article{delozzo2015,
author = {{de lozzo}, matthias},
file = {:users/roy/documents/knowledge/bibliographie/2015/de lozzo{\_}substitution de mod{\`{e}}le et approche multifid{\'{e}}lit{\'{e}} en exp{\'{e}}rimentation num{\'{e}}rique.pdf:pdf},
journal = {journal de la soci{\'{e}}t{\'{e}} fran{\c{c}}aise de statistique},
keywords = {synth{\`{e}}se},
number = {4},
pages = {21--55},
title = {{substitution de mod{\`{e}}le et approche multifid{\'{e}}lit{\'{e}} en exp{\'{e}}rimentation num{\'{e}}rique}},
volume = {156},
year = {2015}
}
@article{srivastava2014a,
abstract = {deep neural nets with a large number of parameters are very powerful machine learning systems. however, overfitting is a serious problem in such networks. large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. dropout is a technique for addressing this problem. the key idea is to randomly drop units (along with their connections) from the neural network during training. this prevents units from co-adapting too much. during training, dropout samples from an exponential number of different " thinned " networks. at test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. this significantly reduces overfitting and gives major improvements over other regularization methods. we show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archiveprefix = {arxiv},
arxivid = {1102.4807},
author = {srivastava, nitish and hinton, geoffrey and krizhevsky, alex and sutskever, ilya and salakhutdinov, ruslan},
doi = {10.1214/12-aos1000},
eprint = {1102.4807},
file = {:users/roy/documents/knowledge/bibliographie/2012/srivastava et al.{\_}dropout a simple way to prevent neural networks from overfittin.pdf:pdf},
isbn = {1532-4435},
issn = {0090-5364},
journal = {journal of machine learning research},
keywords = {deep learning,model combination,neural networks,regularization},
month = {apr},
number = {2},
pages = {1171--1197},
title = {{dropout: a simple way to prevent neural networks from overfittin}},
volume = {40},
year = {2012}
}
@article{thomas2017,
author = {thomas, browne and fort, j.-c. and iooss, bertrand and {le gratiet}, loic},
file = {:users/roy/documents/knowledge/bibliographie/2017/thomas et al.{\_}estimate of quantile-oriented sensitivity indices.pdf:pdf},
journal = {hal repository},
pages = {1--24},
title = {{estimate of quantile-oriented sensitivity indices}},
year = {2017}
}
@phdthesis{caniou2012,
abstract = {cette th{\`{e}}se est une contribution {\`{a}} la mod{\'{e}}lisation imbriqu{\'{e}}e de syst{\`{e}}mes complexes. elle propose une m{\'{e}}thodologie globale pour quantifier les incertitudes et leurs origines dans une cha{\^{i}}ne de calcul form{\'{e}}e par plusieurs mod{\`{e}}les pouvant {\^{e}}tre reli{\'{e}}s les uns aux autres de fa{\c{c}}on complexe. ce travail est organis{\'{e}} selon trois axes. d'abord, la structure ded{\'{e}}pendance des param{\`{e}}tres du mod{\`{e}}le, induite par la mod{\'{e}}lisation imbriqu{\'{e}}e, est mod{\'{e}}lis{\'{e}}e de fa{\c{c}}on rigoureuse gr{\^{a}}ce {\`{a}} la th{\'{e}}orie des copules. puis, deux m{\'{e}}thodes d'analyse de sensibilit{\'{e}} adapt{\'{e}}es aux mod{\`{e}}les {\`{a}} param{\`{e}}tres d'entr{\'{e}}e corr{\'{e}}l{\'{e}}s sont pr{\'{e}}sent{\'{e}}es : l'une est bas{\'{e}}e sur l'analyse de la distribution de la r{\'{e}}ponse du mod{\`{e}}le, l'autre sur la d{\'{e}}composition de la covariance. enfin, un cadre de travail inspir{\'{e}} de la th{\'{e}}orie des graphes est propos{\'{e}} pour la description de l'imbrication des mod{\`{e}}les. la m{\'{e}}thodologie propos{\'{e}}e est appliqu{\'{e}}e {\`{a}} des exemples industriels d'envergure : un mod{\`{e}}le multi{\'{e}}chelles de calcul des propri{\'{e}}t{\'{e}}s m{\'{e}}caniques du b{\'{e}}ton par une m{\'{e}}thode d'homog{\'{e}}n{\'{e}}isation et un mod{\`{e}}le multiphysique de calcul de dommage sur la culasse d'un moteur diesel. les r{\'{e}}sultats obtenus fournissent des indications importantes pour une am{\'{e}}lioration significative de la performance d'une structure.},
author = {caniou, y},
file = {:users/roy/documents/knowledge/bibliographie/2012/caniou{\_}analyse de sensibilit{\'{e}} globale pour les mod{\`{e}}les imbriqu{\'{e}}s et multi{\'{e}}chelles.pdf:pdf},
keywords = {global sensitivity analysis,copula theory,correlation,graph theory,multiscale modelling.,nested modelling},
title = {{analyse de sensibilit{\'{e}} globale pour les mod{\`{e}}les imbriqu{\'{e}}s et multi{\'{e}}chelles}},
year = {2012}
}

@phdthesis{correll2015,
author = {correll, michael},
file = {:users/roy/documents/knowledge/bibliographie/2015/correll{\_}improving visual statistics.pdf:pdf},
title = {{improving visual statistics}},
year = {2015}
}
@article{owen2016,
abstract = {this paper makes the case for using shapley value to quantify the importance of random input variables to a function. alternatives based on the anova decomposition can run into conceptual and computational problems when the input variables are dependent. our main goal here is to show that shapley value removes the conceptual problems. we do this with some simple examples where shapley value leads to intuitively reasonable nearly closed form values.},
archiveprefix = {arxiv},
arxivid = {1610.02080},
author = {owen, art b. and prieur, cl{\'{e}}mentine},
eprint = {1610.02080},
file = {:users/roy/documents/knowledge/bibliographie/2016/owen, prieur{\_}on shapley value for measuring importance of dependent inputs.pdf:pdf},
month = {oct},
title = {{on shapley value for measuring importance of dependent inputs}},
year = {2016}
}
@article{loyolar2016,
abstract = {very large high dimensional data are common nowadays and they impose new challenges to data-driven and data-intensive algorithms. computational intelligence techniques have the potential to provide powerful tools for addressing these challenges, but the current literature focuses mainly on handling scalability issues related to data volume in terms of sample size for classification tasks. this work presents a systematic and comprehensive approach for optimally handling regression tasks with very large high dimensional data. the proposed approach is based on smart sampling techniques for minimizing the number of samples to be generated by using an iterative approach that creates new sam- ple sets until the input and output space of the function to be approximated are optimally covered. incre- mental function learning takes place in each sampling iteration, the new samples are used to fine tune the regression results of the function learning algorithm. the accuracy and confidence levels of the resulting approximation function are assessed using the probably approximately correct computation framework. the smart sampling and incremental function learning techniques can be easily used in practical appli- cations and scale well in the case of extremely large data. the feasibility and good results of the proposed techniques are demonstrated using benchmark functions as well as functions from real-world problems.},
author = {{loyola r}, diego g. and pedergnana, mattia and garc{\'{i}}a, sebasti{\'{a}}n gimeno},
doi = {10.1016/j.neunet.2015.09.001},
file = {:users/roy/documents/knowledge/bibliographie/2016/loyola r, pedergnana, garc{\'{i}}a{\_}smart sampling and incremental function learning for very large high dimensional data.pdf:pdf},
journal = {neural networks},
keywords = {design of experiments,high dimensional function approximation,sampling discrepancy},
pages = {75--87},
publisher = {elsevier ltd},
title = {{smart sampling and incremental function learning for very large high dimensional data}},
volume = {78},
year = {2016}
}
@article{patrick2015,
abstract = {sensitivity analysis methods are important tools for research and design with compu- tational models like cfd. traditional sensitivity analysis methods are unable to compute useful gradient information for long time averaged quantities in chaotic dynamical sys- tems, such as high fidelity simulations of turbulent fluid flows. the least squares shad- owing (lss) method has been used to compute useful gradient information for a number of chaotic systems, including a simulation of homogeneous isotropic turbulence. however, some lss gradient calculations for the kuramoto-sivshinsky (k-s) equation and the lorenz 96 system have a systematic error due to breaks in the assumption of ergodicity. since these systems have similar characteristics to turbulent fluid flows, this ergodicity breaking error must be minimized. this paper proposes a new approach using lss, multiple shooting shadowing (mss), which uses the multiple shooting implementation of lss to reduce the size of the ergodicity breaking error by not running the multiple shooting algorithm to full convergence. this way, gradients are computed from an ensemble of solutions, rather than the shadow direction alone, making the method more robust to the ergodicity breaking error. in this paper, mss is demonstrated for the k-s equation and it is found that mss cannot fix the systematic error of lss when the system has a wide range of chaotic time scales. i.},
author = {blonigan, patrick j. and wang, qiqi},
doi = {10.2514/6.2015-1534\r10.2514/6.2015-1534},
file = {:users/roy/documents/knowledge/bibliographie/2015/blonigan, wang{\_}multiple shooting shadowing for sensitivity analysis of chaotic systems and turbulent fluid flows.pdf:pdf},
isbn = {978-1-62410-343-8},
journal = {53rd aiaa aerospace sciences meeting},
number = {january},
pages = {1--16},
title = {{multiple shooting shadowing for sensitivity analysis of chaotic systems and turbulent fluid flows}},
year = {2015}
}
@article{liu2014,
abstract = {information visualization (infovis), the study of transforming data, information, and knowledge into interactive visual representations, is very important to users because it provides mental models of information. the boom in big data analytics has triggered broad use of infovis in a variety of domains, ranging from finance to sports to politics. in this paper, we present a comprehensive survey and key insights into this fast-rising area. the research on infovis is organized into a taxonomy that contains four main categories, namely empirical methodologies, user interactions, visualization frameworks, and applications, which are each described in terms of their major goals, fundamental principles, recent trends, and state-of-the-art approaches. at the conclusion of this survey, we identify existing technical challenges and propose directions for future research.},
author = {liu, shixia and cui, weiwei and wu, yingcai and liu, mengchen},
doi = {10.1007/s00371-013-0892-3},
file = {:users/roy/documents/knowledge/bibliographie/2014/liu et al.{\_}a survey on information visualization recent advances and challenges.pdf:pdf},
isbn = {0178-2789},
issn = {01782789},
journal = {visual computer},
keywords = {information visualization,interactive techniques,large datasets},
number = {12},
pages = {1373--1393},
title = {{a survey on information visualization: recent advances and challenges}},
volume = {30},
year = {2014}
}
@article{lucca-negro2001,
abstract = {this paper reviews the studies undertaken on vortex breakdown over the past 45 years. the paper is structured such that the area is considered in three sections - experimental, numerical and finally theoretical, and provides a 'guide' to the literature and where necessary directs the reader to more indepth reviews in the specific areas. ?? 2001 elsevier science ltd.},
author = {lucca-negro, o. and o'doherty, t.},
doi = {10.1016/s0360-1285(00)00022-8},
file = {:users/roy/documents/knowledge/bibliographie/2001/lucca-negro, o'doherty{\_}vortex breakdown a review.pdf:pdf},
isbn = {0360-1285},
issn = {03601285},
journal = {progress in energy and combustion science},
keywords = {bubble breakdown,experimental analysis,numerical analysis,spiral breakdown,theoretical analysis,vortex breakdown},
number = {4},
pages = {431--481},
title = {{vortex breakdown: a review}},
volume = {27},
year = {2001}
}
@article{hullman2018,
author = {hullman, jessica and kay, matthew and kim, yea-seul and shrestha, samana},
file = {:users/roy/documents/knowledge/bibliographie/2018/hullman et al.{\_}imagining replications graphical prediction {\&} discrete visualizations improve recall {\&} estimation of effect uncertainty.pdf:pdf},
journal = {ieee transactions on visualization and computer graphics},
title = {{imagining replications: graphical prediction {\&} discrete visualizations improve recall {\&} estimation of effect uncertainty}},
year = {2018}
}
@article{magri2016,
abstract = {we present an adjoint-based method for the calculation of eigenvalue perturbations in nonlinear, degenerate and non self-adjoint eigenproblems. this method is applied to a thermo-acoustic annular combustor network, the stability of which is governed by a nonlinear eigenproblem. we calculate the first- and second-order sensitivities of the growth rate and frequency to geometric, flow and flame parameters. three different configurations are analysed. the benchmark sensitivities are obtained by finite difference, which involves solving the nonlinear eigenproblem at least as many times as the number of parameters. by solving only one adjoint eigenproblem, we obtain the sensitivities to any thermo-acoustic parameter, which match the finite-difference solutions at much lower computational cost.},
archiveprefix = {arxiv},
arxivid = {1602.08438},
author = {magri, luca and bauerheim, michael and juniper, matthew p.},
doi = {10.1016/j.jcp.2016.07.032},
eprint = {1602.08438},
file = {:users/roy/documents/knowledge/bibliographie/2016/magri, bauerheim, juniper{\_}stability analysis of thermo-acoustic nonlinear eigenproblems in annular combustors. part i. sensitivity.pdf:pdf},
issn = {00219991},
journal = {journal of computational physics},
keywords = {adjoint methods,annular combustors,thermo-acoustic stability,uncertainty quantification},
month = {nov},
pages = {395--410},
title = {{stability analysis of thermo-acoustic nonlinear eigenproblems in annular combustors. part i. sensitivity}},
volume = {325},
year = {2016}
}
@article{jatale2015a,
author = {jatale, anchal and smith, philip j. and thornock, jeremy n. and smith, sean t. and hradisky, michal},
doi = {10.1115/1.4031141},
file = {:users/roy/documents/knowledge/bibliographie/2015/jatale et al.{\_}a validation of flare combustion efficiency predictions from large eddy simulations.pdf:pdf},
issn = {2377-2158},
journal = {journal of verification, validation and uncertainty quantification},
keywords = {combustion efficiency,flares,uncertainty quantification,validation},
month = {dec},
number = {2},
pages = {021001},
title = {{a validation of flare combustion efficiency predictions from large eddy simulations}},
volume = {1},
year = {2015}
}


@article{kocis1997,
abstract = {the halton, sobol, and faure sequences and the braaten-weller construction of the general- ized halton sequence are studied in order to assess their applicability for the quasi monte carlo integration with large number of variates. a modification of the halton sequence (the halton sequence leaped) and a new construction of the generalized halton sequence are suggested for unrestricted number of dimensions and are shown to improve considerably on the original halton sequence. problems associated with estimation of the error in quasi monte carlo integration and with the selection of test functions are identified. then an estimate of the maximum error of the quasi monte carlo integration of nine test functions is computed for up to 400 dimensions and is used to evaluate the known generators mentioned above and the two new generators. an empirical formula for the error of the quasi monte carlo integration is suggested.},
author = {kocis, ladislav and whiten, william j.},
file = {:users/roy/documents/knowledge/bibliographie/1997/kocis, whiten{\_}computational investigations of low- discrepancy sequences.pdf:pdf},
journal = {acm transactions on mathematical software},
keywords = {discrepancy,faure sequence,halton sequence,monte carlo and quasi monte carlo integration,sobol sequence,error of numerical integration,generalized halton sequence,low-discrepancy sequences},
number = {2},
pages = {266--294},
title = {{computational investigations of low- discrepancy sequences}},
volume = {23},
year = {1997}
}
@book{fishman1996,
address = {new york, ny},
author = {fishman, george s.},
doi = {10.1007/978-1-4757-2553-7},
edition = {springer s},
file = {:users/roy/documents/knowledge/bibliographie/1996/fishman{\_}monte carlo.pdf:pdf},
isbn = {978-1-4419-2847-4},
publisher = {springer new york},
title = {{monte carlo}},
year = {1996}
}
@article{lozzo2016,
abstract = {in a case of radioactive release in the environment, modeling the radionuclide atmospheric dispersion is particularly useful for emergency response procedures and risk assessment. for this, the cea has devel- oped a numerical simulator, called ceres-mithra, to predict spatial maps of radionuclide concentrations at different instants. this computer code depends on many uncertain scalar and temporal parameters, de- scribing the radionuclide, release or weather characteristics. the purpose is to detect the input parameters the uncertainties of which highly affect the predicted concentrations and to quantify their influences. to this end, we present various measures for the sensitivity analysis of a spatial model. some of them lead to as many analyses as spatial locations (site sensitivity indices) while others consider a single one, with respect to the whole spatial domain (block sensitivity indices). for both categories, variance-based and dependence measures are considered, based on recent literature. all of these sensitivity measures are applied to the c-m computer code and compared to each other, showing the complementarity of block and site sensitivity analyses. finally, a sensitivity analysis summarizing the input uncertainty contribution over the entirety of the spatio-temporal domain is proposed.},
author = {{de lozzo}, matthias and marrel, amandine},
file = {:users/roy/documents/knowledge/bibliographie/2016/de lozzo, marrel{\_}sensitivity analysis with dependence and variance-based measures for spatio-temporal numerical simulators.pdf:pdf},
keywords = {dependence measures,gaussian process model,global sensitivity analysis,site {\&} block sensitivity measures,sobol' indices,spatio-temporal models},
title = {{sensitivity analysis with dependence and variance-based measures for spatio-temporal numerical simulators}},
year = {2016}
}
@article{liem2014,
author = {liem, rhea p and martins, joaquim r r a},
doi = {10.2514/6.2014-2301},
file = {:users/roy/documents/knowledge/bibliographie/2014/liem, martins{\_}surrogate models and mixtures of experts in aerodynamic performance prediction for mission analysis.pdf:pdf},
isbn = {978-1-62410-283-7},
journal = {15th aiaa/issmo multidisciplinary analysis and optimization conference},
number = {june},
pages = {1--37},
title = {{surrogate models and mixtures of experts in aerodynamic performance prediction for mission analysis}},
year = {2014}
}
@inproceedings{moreland2016,
abstract = {we know the rainbow color map is terrible, and it is emphatically reviled by the visualization community, yet its use continues to persist. why do we continue to use a this perceptual encoding with so many known flaws? instead of focusing on why we should not use rainbow colors, this position statement explores the rational for why we do pick these colors despite their flaws. often the decision is influenced by a lack of knowledge, but even experts that know better sometimes choose poorly. a larger issue is the expedience that we have inadvertently made the rainbow color map become. knowing why the rainbow color map is used will help us move away from it. education is good, but clearly not sufficient. we gain traction by making sensible color alternatives more convenient. it is not feasible to force a color map on users. our goal is to supplant the rainbow color map as a common standard, and we will find that even those wedded to it will migrate away.},
author = {Moreland, Kenneth},
booktitle = {Proceedings of Human Vision and Electronic Imaging ({HVEI})},
doi = {10.2352/issn.2470-1173.2016.16.hvei-133},
file = {:users/roy/documents/knowledge/bibliographie/2016/moreland{\_}why we use bad color maps and what you can do about it.pdf:pdf},
title = {{Why we use bad color maps and what you can do about it}},
year = {2016}
}
@book{rasmussen2006,
author = {rasmussen, ce and williams, c},
file = {:users/roy/documents/knowledge/bibliographie/2006/rasmussen, williams{\_}gaussian processes for machine learning.pdf:pdf},
isbn = {026218253x},
publisher = {mit press},
title = {{gaussian processes for machine learning}},
year = {2006}
}
@article{lamboni2010,
abstract = {many dynamic models are used for risk assessment and decision support in ecology and crop science. such models generate time-dependent model predictions, with time either discretised or continuous. their global sensitivity analysis is usually applied separately on each time output, but campbell et al. (2006 [1]) advocated global sensitivity analyses on the expansion of the dynamics in a well-chosen functional basis. this paper focuses on the particular case when principal components analysis is combined with analysis of variance. in addition to the indices associated with the principal components, generalised sensitivity indices are proposed to synthesize the influence of each parameter on the whole time series output. index definitions are given when the uncertainty on the input factors is either discrete or continuous and when the dynamic model is either discrete or functional. a general estimation algorithm is proposed, based on classical methods of global sensitivity analysis. the method is applied to a dynamic wheat crop model with 13 uncertain parameters. three methods of global sensitivity analysis are compared: the sobol'saltelli method, the extended fast method, and the fractional factorial design of resolution 6. {\textcopyright} 2010 elsevier ltd. all rights reserved.},
author = {lamboni, matieyendou and monod, herv{\'{e}} and makowski, david},
doi = {10.1016/j.ress.2010.12.002},
file = {:users/roy/documents/knowledge/bibliographie/2011/lamboni, monod, makowski{\_}multivariate sensitivity analysis to measure global contribution of input factors in dynamic models.pdf:pdf},
isbn = {0951-8320},
issn = {09518320},
journal = {reliability engineering and system safety},
keywords = {dynamic model,factorial design,latin hypercube sampling,principal components analysis,rkhs,sensitivity analysis,sobol' decomposition},
number = {4},
pages = {450--459},
title = {{multivariate sensitivity analysis to measure global contribution of input factors in dynamic models}},
volume = {96},
year = {2011}
}
@incollection{nouy2016a,
abstract = {parameter-dependent models arise in many contexts such as uncertainty quan- tification, sensitivity analysis, inverse problems, or optimization. parametric or uncertainty analyses usually require the evaluation of an output of a model for many instances of the input parameters, which may be intractable for complex numerical models. a possible remedy consists in replacing the model by an approximate model with reduced complexity (a so-called reduced order model) allowing a fast evaluation of output variables of interest. this chapter provides an overview of low-rank methods for the approximation of functions that are identified either with order-two tensors (for vector-valued functions) or higher- order tensors (for multivariate functions). different approaches are presented for the computation of low-rank approximations, either based on samples of the function or on the equations that are satisfied by the function, the latter approaches including projection-based model order reduction methods. for multivariate functions, different notions of ranks and the corresponding low-rank approximation formats are introduced.},
author = {nouy, anthony},
booktitle = {handbook of uncertainty quantification},
doi = {10.1007/978-3-319-11259-6_21-1},
file = {:users/roy/documents/knowledge/bibliographie/2015/nouy{\_}low-rank tensor methods for model order reduction.pdf:pdf},
isbn = {978-3-319-12384-4},
keywords = {high-dimensional problems,low-rank approximation,model order reduction,parameter-dependent equations,stochastic equations,uncertainty quantification},
pages = {1--26},
publisher = {springer international publishing},
title = {{low-rank tensor methods for model order reduction}},
year = {2015}
}
@article{last2015,
author = {last, mark and gorelik, anna usyskin},
doi = {10.1007/978-3-319-14998-1},
file = {:users/roy/documents/knowledge/bibliographie/2015/last, gorelik{\_}multimedia data mining and analytics.pdf:pdf},
isbn = {978-3-319-14997-4},
number = {october},
title = {{multimedia data mining and analytics}},
year = {2015}
}
@article{tissot2014a,
abstract = {in variance-based sensitivity analysis, the method of sobol' [sensitivity analysis for nonlinear mathematical models. math model comput exp. 1993;1:407?414] allows one to compute sobol' indices (si) using the monte carlo integration. one of the main drawbacks of this approach is that estimating si requires a number of simulations which is dependent on the dimension of the model of interest. for example, estimating all the first- or second-order si of a d-dimensional function basically requires or independent input vectors, respectively. some interesting combinatorial results have been introduced to weaken this defect, in particular by saltelli [making best use of model evaluations to compute sensitivity indices. comput phys commun. 2002;145:280?297] and more recently by owen [variance components and generalized sobol' indices. siam/asa j uncertain quantification. 2013;1:19?41], but the quantities they estimate still depend linearly on the dimension d. in this paper, we introduce a new approach to estimate all the first- and second-order si by using only two input vectors. we establish theoretical properties of such a method for the estimation of first-order si and discuss the generalization to higher order indices. in particular, we prove on numerical examples that this procedure is tractable and competitive for the estimation of all first- and second-order si. as an illustration, we propose to apply this new approach to a marine ecosystem model of the ligurian sea (northwestern mediterranean) in order to study the relative importance of its several parameters. the calibration process of this kind of chemical simulators is well known to be quite intricate, and a rigorous and robust ? that is, valid without strong regularity assumptions ? sensitivity analysis, as the method of sobol' provides, could be of great help. this article has supplementary material online.$\backslash$nin variance-based sensitivity analysis, the method of sobol' [sensitivity analysis for nonlinear mathematical models. math model comput exp. 1993;1:407?414] allows one to compute sobol' indices (si) using the monte carlo integration. one of the main drawbacks of this approach is that estimating si requires a number of simulations which is dependent on the dimension of the model of interest. for example, estimating all the first- or second-order si of a d-dimensional function basically requires or independent input vectors, respectively. some interesting combinatorial results have been introduced to weaken this defect, in particular by saltelli [making best use of model evaluations to compute sensitivity indices. comput phys commun. 2002;145:280?297] and more recently by owen [variance components and generalized sobol' indices. siam/asa j uncertain quantification. 2013;1:19?41], but the quantities they estimate still depend linearly on the dimension d. in this paper, we introduce a new approach to estimate all the first- and second-order si by using only two input vectors. we establish theoretical properties of such a method for the estimation of first-order si and discuss the generalization to higher order indices. in particular, we prove on numerical examples that this procedure is tractable and competitive for the estimation of all first- and second-order si. as an illustration, we propose to apply this new approach to a marine ecosystem model of the ligurian sea (northwestern mediterranean) in order to study the relative importance of its several parameters. the calibration process of this kind of chemical simulators is well known to be quite intricate, and a rigorous and robust ? that is, valid without strong regularity assumptions ? sensitivity analysis, as the method of sobol' provides, could be of great help. this article has supplementary material online.},
author = {tissot, j.-y. and prieur, cl{\'{e}}mentine},
doi = {10.1080/00949655.2014.971799},
file = {:users/roy/documents/knowledge/bibliographie/2015/tissot, prieur{\_}a randomized orthogonal array-based procedure for the estimation of first- and second-order sobol' indices.pdf:pdf},
issn = {0094-9655},
journal = {journal of statistical computation and simulation},
month = {may},
number = {7},
pages = {1358--1381},
title = {{a randomized orthogonal array-based procedure for the estimation of first- and second-order sobol' indices}},
volume = {85},
year = {2015}
}
@article{kitano2016,
abstract = {les of gas and spray combustion in back-step flows are performed to investigate the mechanism underlying combustion instability and the effect of initial droplet diameter on combustion instability. methane and kerosene are used as fuel for gas and spray combustion, respectively, and two-step global reaction models are used for the reactions. a dynamic thickened flame model is employed as the turbulent combustion model. the motions of evaporating droplets are tracked using the lagrangian manner. the results show that in gas combustion, the pressure, heat release rate and streamwise velocity oscillate with the same frequency but different phases, and that the inlet velocity oscillation periodically generates a large vortex near the dump plane that drives combustion instability. in spray combustion, an oscillation of droplet evaporation rate is also observed. whereas the difference in the initial droplet diameter slightly affects the mode of the pressure oscillation, it strongly affects the intensity of the pressure oscillation and causes a maximum value for a specific initial droplet diameter. this arises from the difference in the initial droplet diameter causing a difference in the droplet evaporation rate, which alters the relationship between the oscillations of the heat release rate and pressure near the dump plane.},
author = {kitano, tomoaki and kaneko, keisuke and kurose, ryoichi and komori, satoru},
doi = {10.1016/j.combustflame.2016.05.005},
file = {:users/roy/documents/knowledge/bibliographie/2016/kitano et al.{\_}large-eddy simulations of gas- and liquid-fueled combustion instabilities in back-step flows.pdf:pdf},
issn = {00102180},
journal = {combustion and flame},
keywords = {combustion instability,droplet evaporation,les,spray combustion},
month = {aug},
pages = {63--78},
publisher = {elsevier inc.},
title = {{large-eddy simulations of gas- and liquid-fueled combustion instabilities in back-step flows}},
volume = {170},
year = {2016}
}

@techreport{gu,
author = {gu, ming and eisenstat, stanley c.},
file = {:users/roy/documents/knowledge/bibliographie/unknown/gu, eisenstat{\_}a stable and fast algorithm for updating the singular value decomposition.pdf:pdf},
institution = {yale},
title = {{a stable and fast algorithm for updating the singular value decomposition}}
}
@article{kraus2016,
abstract = {in the present work, combustion instabilities of a modular combustor are investigated. the combustor operates with partially premixed, swirl-stabilized flames and can be operated in single- and different multiple-burner setups. the design parameters of the combustor prevent large-scale flame–flame interactions in the multiple-burner arrangements. the objective is to investigate how the interaction of the swirl jets affects the thermoacoustic stability of the combustor. results of measurements of pressure oscillations and high-speed oh*-chemiluminescence imaging for the single-burner setup and two multiple-burner setups are discussed. additionally, results of investigations with different flame characteristics are presented. these are achieved by varying the ratio of the mass flow rates through the swirlers of the double-concentric swirl nozzle. several unstable modes with high pressure amplitudes are observed in the single-burner setup as well as in the multiple-burner setups. numerical studies of the acoustic behavior of the combustor setups were performed that indicate that the different geometries show similar acoustic behaviors. the results lead to the conclusion that the interaction of the swirl jets in the multiple-burner setups affects the thermoacoustic response spectrum of the flame even in the absence of large-scale flame–flame interactions. based on the findings in earlier studies, it is concluded that the differences in the flame response characteristics are induced by the reduction of the swirl intensity in the multiple-burner arrangements, which is caused by the exchange of momentum between the adjacent swirl jets.},
author = {kraus, christian and harth, stefan and bockhorn, henning},
doi = {10.1177/1756827715627064},
file = {:users/roy/documents/knowledge/bibliographie/2016/kraus, harth, bockhorn{\_}experimental investigation of combustion instabilities in lean swirl-stabilized partially-premixed flames in sing.pdf:pdf},
issn = {1756-8277},
journal = {international journal of spray and combustion dynamics},
keywords = {18 august 2014,8 april 2015,accepted,combustion instabilities,date received,multiple burners,swirl flame,thermoacoustics},
month = {mar},
number = {1},
pages = {4--26},
title = {{experimental investigation of combustion instabilities in lean swirl-stabilized partially-premixed flames in single- and multiple-burner setup}},
volume = {8},
year = {2016}
}
@article{gorgoraptis2015,
author = {gorgoraptis, eleftherios},
file = {:users/roy/documents/knowledge/bibliographie/2015/gorgoraptis{\_}hit les.pdf:pdf},
journal = {avbp quality program form},
number = {7.0},
title = {{hit les}},
year = {2015}
}
@misc{iooss2014,
address = {houches},
author = {iooss, bertrand and marrel, amandine},
file = {:users/roy/documents/knowledge/bibliographie/2014/iooss, marrel{\_}enjeux et m{\'{e}}thodes de l'exploration des mod{\`{e}}les.pdf:pdf},
title = {{enjeux et m{\'{e}}thodes de l'exploration des mod{\`{e}}les}},
year = {2014}
}
@article{matheron1963,
abstract = {knowledge of ore grades and ore reserves as well as error estima- tion of these values, is fundamental for mi ning engineers and mining geologists. unt il now no appropriate scien t i fic approach to those esti - mation problems has existed : geostatistics, the principles of which are summarized in thi s paper, constitutes a new science leadin g to such an approach. the a uthor cri t icizes classical statistical methods still in use, and shows some of the main resul ts given by geostatistics. any ore deposit evaluation as well as proper decision of star t ing mining operations shoul d be preceded by a geostatistical in vestigation which may avoid economic failures.},
author = {matheron, georges},
file = {:users/roy/documents/knowledge/bibliographie/1963/matheron{\_}principles of geostatistics.pdf:pdf},
journal = {economic geology},
pages = {1246--1266},
title = {{principles of geostatistics}},
volume = {58},
year = {1963}
}

@article{palies2010,
abstract = {the dynamics of premixed confined swirling flames is investigated by examining their response to incident velocity perturbations. a generalized transfer function designated as the flame describing function (fdf) is determined by sweeping a frequency range extending from 0 to 400hz and by changing the root mean square fluctuation level between 0{\%} and 72{\%} of the bulk velocity. the unsteady heat release rate is deduced from the emission intensity of oh* radicals. this global information is complemented by phase conditioned abel transformed emission images. this processing yields the distribution of light emission. by assuming that the light intensity is proportional to the heat release rate, it is possible to deduce the distribution of unsteady heat release rate in wm-3 and see how it evolves with time during the modulation cycle and for different forcing frequencies. these data can be useful for the determination of regimes of instability but also give clues on the mechanisms which control the swirling flame dynamics. it is found from experiments and demonstrated analytically that a swirler submitted to axial acoustic waves originating from the upstream manifold generates a vorticity wave on its downstream side. the flame is then submitted to a transmitted axial acoustic perturbation which propagates at the speed of sound and to an azimuthal velocity perturbation which is convected at the flow velocity. the net result is that the dynamical response and unsteady heat release rate are determined by the combined effects of these axial and induced azimuthal velocity perturbations. the former disturbance induces a shedding of vortices from the injector lip which roll-up the flame extremity while the latter effectively perturbs the swirl number which results in an angular oscillation of the flame root. this motion is equivalent to that which would be induced by perturbations of the burning velocity. the phase between incident perturbations is controlled by the convective time delay between the swirler and the injector. the constructive or destructive interference between the different perturbations is shown to yield the low and high gains observed for certain frequencies. ?? 2010 the combustion institute.},
author = {palies, p. and durox, d. and schuller, t. and candel, s.},
doi = {10.1016/j.combustflame.2010.02.011},
file = {:users/roy/documents/knowledge/bibliographie/2010/palies et al.{\_}the combined dynamics of swirler and turbulent premixed swirling flames.pdf:pdf},
isbn = {0010-2180},
issn = {00102180},
journal = {combustion and flame},
keywords = {acoustic coupling,combustion dynamics,confinement,swirling flame},
month = {sep},
number = {9},
pages = {1698--1717},
publisher = {the combustion institute.},
title = {{the combined dynamics of swirler and turbulent premixed swirling flames}},
volume = {157},
year = {2010}
}
@inproceedings{durantin2016,
abstract = {the numerical optimization of a photoacoustic gas sensor is a challenging problem in terms of computational time. the signal detected by the gas sensor is a non-linear function depending on several geometrical parameters. the optimization requires a very large number of function calls, thus an important simulation time. in the case of photoacoustics, two dif- ferent numerical models with different fidelity levels are available to simulate the behavior of the component. in order to reduce the computational burden of optimizing the gas sensor, a new multifidelity metamodeling framework, based on radial basis function, is proposed. the present method offers an alternative to co-kriging (a widely used multifidelity metamodel). this multifidelity model is then used in an optimization sequence to enrich a training database, via a strategy inspired by gutmann [1]. the process is applied to the optimization of geometrical parameters in the gas sensor problem.},
address = {crete island},
annote = {co-rbf},
author = {durantin, c{\'{e}}dric and rouxel, justin and d{\'{e}}sid{\'{e}}ri, jean antoine and gli{\`{e}}re, alain},
booktitle = {vii european congress on computational methods in applied sciences and engineering},
file = {:users/roy/documents/knowledge/bibliographie/2016/durantin et al.{\_}optimization of a photoacoustic gas sensor using.pdf:pdf},
keywords = {abstract.,metamodel,multifidelity,optimization,radial basis function,a non-linear function,a photoacoustic gas sensor,abstract,depending on several geometrical,in terms of computational,is a challenging problem,metamodel,multifidelity,optimization,parameters,radial basis function,the gas sensor is,the numerical optimization of,the optimization requires a,the signal detected by,time,very large number},
mendeley-tags = {abstract.,metamodel,multifidelity,optimization,radial basis function},
title = {{optimization of a photoacoustic gas sensor using}},
year = {2016}
}
@techreport{bohling2005a,
author = {bohling, geoff},
booktitle = {kansas geological survey},
file = {:users/roy/documents/knowledge/bibliographie/2005/bohling{\_}kriging.pdf:pdf},
number = {october},
pages = {1--20},
title = {{kriging}},
year = {2005}
}
@incollection{prieur2016,
abstract = {this section aims at presenting an overview of variance-based approaches for global sensitivity analysis. starting from functional anova, sobol' indices are first defined and then estimation algorithms are provided. the performance of these algorithms is theorically and practically discussed. the review includes recent results on the topic.},
author = {prieur, cl{\'{e}}mentine and tarantola, stefano},
booktitle = {handbook of uncertainty quantification},
doi = {10.1007/978-3-319-11259-6_35-1},
file = {:users/roy/documents/knowledge/bibliographie/2015/prieur, tarantola{\_}variance-based sensitivity analysis theory and estimation algorithms.pdf:pdf},
isbn = {978-3-319-12384-4},
keywords = {effective algorithm for sensitivity indices,fanova,fourier amplitude sensitivity test,global sensitivity analysis,latin hypercube sampling,monte carlo sampling,orthogonal arrays (oa),polynomial chaos expansion,quasi-monte carlo sampling,random balance design,replication,sampling design,sensitivity indices with given data,sobol' sensitivity indices,spectral meth- ods,effective dimension},
pages = {1--23},
publisher = {springer international publishing},
title = {{variance-based sensitivity analysis: theory and estimation algorithms}},
year = {2015}
}
@incollection{moreland2009,
abstract = {one of the most fundamental features of scientific visualiza- tion is the process of mapping scalar values to colors. this process allows us to view scalar fields by coloring surfaces and volumes. unfortunately, the majority of scientific visualization tools still use a color map that is famous for its ineffectiveness: the rainbow color map. this color map, which na¨ıvely sweeps through the most saturated colors, is well known for its ability to obscure data, introduce artifacts, and confuse users. although many alternate color maps have been proposed, none have achieved widespread adoption by the visualization community for scien- tific visualization. this paper explores the use of diverging color maps (sometimes also called ratio, bipolar, or double-ended color maps) for use in scientific visualization, provides a diverging color map that gen- erally performs well in scientific visualization applications, and presents an algorithm that allows users to easily generate their own customized color maps. 1},
author = {moreland, kenneth},
booktitle = {lecture notes in computer science (including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics)},
doi = {10.1007/978-3-642-10520-3_9},
file = {:users/roy/documents/knowledge/bibliographie/2009/moreland{\_}diverging color maps for scientific visualization.pdf:pdf},
isbn = {364210519x},
issn = {03029743},
number = {part 2},
pages = {92--103},
pmid = {1000164476},
title = {{diverging color maps for scientific visualization}},
volume = {5876 lncs},
year = {2009}
}
@inproceedings{correll2014,
author = {correll, michael and gleicher, michael},
booktitle = {ieee trans. visualization {\&} comp. graphics},
doi = {10.1109/tvcg.2014.2346298},
file = {:users/roy/documents/knowledge/bibliographie/2014/correll, gleicher{\_}error bars considered harmful exploring alternate encodings for mean and error.pdf:pdf},
number = {12},
pages = {2142--2151},
title = {{error bars considered harmful : exploring alternate encodings for mean and error}},
volume = {20},
year = {2014}
}
@article{cherston2016,
abstract = {this paper presents a platform that enables composers to generate unique auditory representations of real-time particle collision data from the atlas experiment at cern. an associated web page then enables the public to listen to real-time experimental data through the aesthetic lens of selected artists. the current tool is built in collaboration with the atlas outreach team and is designed to increase public engagement in high energy physics by exposing the data through a novel interaction mode. more broadly, it is part of a larger vision to better harness audio as a medium to interact with big data from ever more prevalent real-time sensors.},
author = {cherston, juliana},
doi = {10.1145/2851581.2892295},
file = {:users/roy/documents/knowledge/bibliographie/2016/cherston{\_}sonification platform for interaction with real-time particle collision data from the atlas detector.pdf:pdf},
isbn = {9781450340823},
journal = {proceedings of the 2016 chi conference extended abstracts on human factors in computing systems},
keywords = {auditory display,physics,real-time sensor data,sonification},
pages = {1647--1653},
title = {{sonification platform for interaction with real-time particle collision data from the atlas detector}},
year = {2016}
}
@article{lewandowski2016,
abstract = {this article reviews published data on the mechanical properties of additively manufactured metallic materials. the additive manufacturing techniques utilized to generate samples covered in this review include powder bed fusion (e.g., ebm, slm, dmls) and directed energy deposition (e.g., lens, ebf3). although only a limited number of metallic alloy systems are currently available for additive manufacturing (e.g., ti-6al-4v, tial, stainless steel, inconel 625/718, and al-si-10mg), the bulk of the published mechanical properties information has been generated on ti-6al-4v. however, summary tables for published mechanical properties and/or key figures are included for each of the alloys listed above, grouped by the additive technique used to generate the data. published values for mechanical properties obtained from hardness, tension/compression, fracture toughness, fatigue crack growth, and high cycle fatigue are included for as-built, heat-treated, and/or hip conditions, when available. the effects of test...},
author = {Lewandowski, John J. and Seifi, Mohsen},
doi = {10.1146/annurev-matsci-070115-032024},
file = {:users/roy/documents/knowledge/bibliographie/2016/lewandowski, seifi{\_}metal additive manufacturing a review of mechanical properties.pdf:pdf},
isbn = {978-0-8243-1746-1},
issn = {1531-7331},
journal = {Annual Review of Materials Research},
keywords = {fatigue,fracture,mechanical properties,metal additive manufacturing},
number = {1},
pages = {151--186},
title = {{Metal additive manufacturing: a review of mechanical properties}},
volume = {46},
year = {2016}
}
@article{krizhevsky2012a,
abstract = {we trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the imagenet lsrvrc-2010 contest into the 1000 different classes. on the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. the neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. to make training faster, we used non-saturating neurons and a very efficient gpu implementation of the convolutional operation. to reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. we also entered a variant of the model in the ilsvrc-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archiveprefix = {arxiv},
arxivid = {1102.0183},
author = {krizhevsky, alex and sutskever, ilya and hinton, geoffrey e},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:users/roy/documents/knowledge/bibliographie/2012/krizhevsky, sutskever, hinton{\_}imagenet classification with deep convolutional neural networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {advances in neural information processing systems},
pages = {1--9},
pmid = {7491034},
title = {{imagenet classification with deep convolutional neural networks}},
year = {2012}
}
@article{kennedy2001,
abstract = {we consider prediction and uncertainty analysis for systems which are approximated using complex mathematical models. such models, implemented as computer codes, are often generic in the sense that by a suitable choice of some of the model's input parameters the code can be used to predict the behaviour of the system in a variety of specific applications. however, in any specific application the values of necessary parameters may be unknown. in this case, physical observations of the system in the specific context are used to learn about the unknown parameters. the process of fitting the model to the observed data by adjusting the parameters is known as calibration. calibration is typically effected by ad hoc fitting, and after calibration the model is used, with the fitted input values, to predict the future behaviour of the system. we present a bayesian calibration technique which improves on this traditional approach in two respects. first, the predictions allow for all sources of uncertainty, including the remaining uncertainty over the fitted parameters. second, they attempt to correct for any inadequacy of the model which is revealed by a discrepancy between the observed data and the model predictions from even the best-fitting parameter values. the method is illustrated by using data from a nuclear radiation release at tomsk, and from a more complex simulated nuclear accident exercise.},
author = {kennedy, marc c and o'hagan, anthony},
doi = {10.1111/1467-9868.00294},
file = {:users/roy/documents/knowledge/bibliographie/2001/kennedy, o'hagan{\_}bayesian calibration of computer models.pdf:pdf},
isbn = {1369-7412},
issn = {1369-7412},
journal = {journal of the royal statistical society: series b (statistical methodology)},
keywords = {calibration,computer experiments,deterministic models,gaussian process,interpolation,model inadequacy,sensitivity analysis,uncertainty analysis},
month = {aug},
number = {3},
pages = {425--464},
pmid = {3290900},
title = {{bayesian calibration of computer models}},
volume = {63},
year = {2001}
}


@article{hullman2016,
abstract = {evaluating a visualization that depicts uncertainty is fraught with challenges due to the complex psychology of uncer-tainty. however, relatively little attention is paid to se-lecting and motivating a chosen interpretation or elicitation method for subjective probabilities in the uncertainty visual-ization literature. i survey existing evaluation work in uncer-tainty visualization, and examine how research in judgment and decision-making that focuses on subjective uncertainty elicitation sheds light on common approaches in visualiza-tion. i propose suggestions for practice aimed at reducing errors and noise related to how ground truth is defined for subjective probability estimates, the choice of an elicitation method, and the strategies used by subjects making judg-ments with an uncertainty visualization.},
author = {hullman, jessica},
doi = {10.1145/2993901.2993919},
file = {:users/roy/documents/knowledge/bibliographie/2016/hullman{\_}why evaluating uncertainty visualization is error prone.pdf:pdf},
isbn = {978-1-4503-4818-8},
journal = {proceedings of the beyond time and errors on novel evaluation methods for visualization},
keywords = {elicitation,subjective probability distribution,uncertainty visualization},
pages = {143--151},
title = {{why evaluating uncertainty visualization is error prone}},
year = {2016}
}
@article{archer1997a,
author = {archer, g. e. b. and saltelli, a. and sobol', i.m},
doi = {10.1080/00949659708811825},
file = {:users/roy/documents/knowledge/bibliographie/1997/archer, saltelli, sobol'{\_}sensitivity measures,anova-like techniques and the use of bootstrap.pdf:pdf},
issn = {0094-9655},
journal = {journal of statistical computation and simulation},
keywords = {analysis of variance decomposition,sensitivity indices,bootstrap},
month = {may},
number = {2},
pages = {99--120},
title = {{sensitivity measures,anova-like techniques and the use of bootstrap}},
volume = {58},
year = {1997}
}
@misc{pronzato,
author = {pronzato, luc},
file = {:users/roy/documents/knowledge/bibliographie/2014/pronzato{\_}(adaptive) experimental design.pdf:pdf},
title = {{(adaptive) experimental design}},
year = {2014}
}
@article{swiler2014,
author = {swiler, laura p. and hough, patricia d. and qian, peter and xu, xu and storlie, curtis and lee, herbert},
doi = {10.1007/978-3-319-04280-0_21},
file = {:users/roy/documents/knowledge/bibliographie/2014/swiler et al.{\_}surrogate models for mixed discrete-continuous variables.pdf:pdf},
isbn = {9783319042794},
issn = {1860949x},
journal = {studies in computational intelligence},
number = {august},
pages = {181--202},
title = {{surrogate models for mixed discrete-continuous variables}},
volume = {539},
year = {2014}
}
@article{gran2009,
abstract = {although sample size calculations have become an important element in the design of research projects, such methods for studies involving current status data are scarce. here, we propose a method for calculating power and sample size for studies using current status data. this method is based on a weibull survival model for a two-group comparison. the weibull model allows the investigator to specify a group difference in terms of a hazards ratio or a failure time ratio. we consider exponential, weibull and uniformly distributed censoring distributions. we base our power calculations on a parametric approach with the wald test because it is easy for medical investigators to conceptualize and specify the required input variables. as expected, studies with current status data have substantially less power than studies with the usual right-censored failure time data. our simulation results demonstrate the merits of these proposed power calculations.},
archiveprefix = {arxiv},
arxivid = {nihms150003},
author = {gran, j.m. and wasmuth, l. and amundsen, e.j. and lindqvist, b.h. and aalen, o.o.},
doi = {10.1002/sim},
eprint = {nihms150003},
file = {:users/roy/documents/knowledge/bibliographie/2009/gran et al.{\_}growth rates in epidemic models application to a model for hivaids progression.pdf:pdf},
isbn = {2007090091480},
issn = {02776715},
journal = {statistics in medicine},
keywords = {ces-d,conditional maximum likelihood,fixed effects,generalized linear mixed model,hausman test,linear mixed model,random effects,robust,variance},
number = {july 2006},
pages = {221--239},
pmid = {19455509},
title = {{growth rates in epidemic models: application to a model for hiv/aids progression}},
volume = {28},
year = {2009}
}
@phdthesis{szafirnicoledanielle2015,
author = {{szafir, nicole, danielle}, albers},
file = {:users/roy/documents/knowledge/bibliographie/2015/szafir, nicole, danielle{\_}utilizing color for perceptually-driven data visualization.pdf:pdf},
title = {{utilizing color for perceptually-driven data visualization}},
year = {2015}
}
@article{gratiet,
abstract = {global sensitivity analysis is now established as a powerful approach for determining the key random input parameters that drive the uncertainty of model output predictions. yet the classical computation of the so-called sobol' indices is based on monte carlo simulation, which is not affordable when computationally expensive models are used, as it is the case in most applications in engineering and applied sciences. in this respect metamodels such as polynomial chaos expansions (pce) and gaussian processes (gp) have received tremendous attention in the last few years, as they allow one to replace the original, taxing model by a surrogate which is built from an experimental design of limited size. then the surrogate can be used to compute the sensitivity indices in negligible time. in this chapter an introduction to each technique is given, with an emphasis on their strengths and limitations in the context of global sensitivity analysis. in particular, sobol' (resp. total sobol') indices can be computed analytically from the pce coefficients. in contrast, confidence intervals on sensitivity indices can be derived straightforwardly from the properties of gps. the performance of the two techniques is finally compared on three well-known analytical benchmarks (ishigami, g-sobol and morris functions) as well as on a realistic engineering application (deflection of a truss structure).},
archiveprefix = {arxiv},
arxivid = {1606.04273},
author = {gratiet, l le and marelli, s and sudret, b},
doi = {10.1007/978-3-319-11259-6_38-1},
eprint = {1606.04273},
file = {:users/roy/documents/knowledge/bibliographie/2016/gratiet, marelli, sudret{\_}metamodel-based sensitivity analysis polynomial chaos expansions and gaussian processes.pdf:pdf},
month = {jun},
title = {{metamodel-based sensitivity analysis: polynomial chaos expansions and gaussian processes}},
year = {2016}
}
@incollection{wikle2016,
abstract = {analyses of complex processes should account for the uncertainty in the data, the processes that generated the data, and the models that are used to represent the processes and data. accounting for these uncertainties can be daunting in traditional statistical analyses. in recent years, hierarchical statistical models have provided a coherent probabilistic framework that can accommodate these multiple sources of quantifiable uncertainty. this overview describes a science-based hierarchical statistical modeling approach and the associated bayesian inference. in addition, given that many complex processes involve the dynamical evolution of spatial processes, an overview of hierarchical dynamical spatio-temporal models is also presented. the hierarchical and spatio-temporal modeling frameworks are illustrated with a problem concernedwith assimilating ocean vector wind observations from satellite and weather center analyses.},
author = {wikle, christopher k},
booktitle = {handbook of uncertainty quantification},
doi = {10.1007/978-3-319-11259-6_4-1},
file = {:users/roy/documents/knowledge/bibliographie/2015/wikle{\_}hierarchical models for uncertainty quantification an overview.pdf:pdf},
isbn = {978-3-319-12384-4},
keywords = {bhm,basis functions,bayesian,integro-difference equations,latent process,mcmc,multivariate,ocean,quadratic nonlinearity,reduced-rank representation,spatio-temporal,wind},
pages = {1--26},
publisher = {springer international publishing},
title = {{hierarchical models for uncertainty quantification: an overview}},
year = {2015}
}
@article{azzimonti2016,
abstract = {we consider a gaussian process model trained on few evaluations of an expensive-to-evaluate deterministic function and we study the problem of estimating a fixed excursion set of this function. we review the concept of conservative estimates, recently introduced in this framework, and, in particular, we focus on estimates based on vorob'ev quantiles. we present a method that sequentially selects new evaluations of the function in order to reduce the uncertainty on such estimates. the sequential strategies are first benchmarked on artificial test cases generated from gaussian process realizations in two and five dimensions, and then applied to two reliability engineering test cases.},
archiveprefix = {arxiv},
arxivid = {1611.07256},
author = {azzimonti, dario and ginsbourger, david and chevalier, cl{\'{e}}ment and bect, julien and richet, yann},
eprint = {1611.07256},
file = {:users/roy/documents/knowledge/bibliographie/2016/azzimonti et al.{\_}adaptive design of experiments for conservative estimation of excursion sets.pdf:pdf},
keywords = {batch sequential strategies,conservative estimates,design,excursion sets,gaussian process models,of experiments},
month = {nov},
pages = {1--22},
title = {{adaptive design of experiments for conservative estimation of excursion sets}},
year = {2016}
}
@article{marrel2009,
abstract = {global sensitivity analysis of complex numerical models can be performed by calculating variance-based importance measures of the input variables, such as the sobol indices. however, these techniques, requiring a large number of model evaluations, are often unacceptable for time expensive computer codes. a well-known and widely used decision consists in replacing the computer code by a metamodel, predicting the model responses with a negligible computation time and rending straightforward the estimation of sobol indices. in this paper, we discuss about the gaussian process model which gives analytical expressions of sobol indices. two approaches are studied to compute the sobol indices: the first based on the predictor of the gaussian process model and the second based on the global stochastic process model. comparisons between the two estimates, made on analytical examples, show the superiority of the second approach in terms of convergence and robustness. moreover, the second approach allows to integrate the modeling error of the gaussian process model by directly giving some confidence intervals on the sobol indices. these techniques are finally applied to a real case of hydrogeological modeling. {\textcopyright} 2009 elsevier ltd. all rights reserved.},
archiveprefix = {arxiv},
arxivid = {0802.1008},
author = {marrel, amandine and iooss, bertrand and laurent, b{\'{e}}atrice and roustant, olivier},
doi = {10.1016/j.ress.2008.07.008},
eprint = {0802.1008},
file = {:users/roy/documents/knowledge/bibliographie/2009/marrel et al.{\_}calculations of sobol indices for the gaussian process metamodel.pdf:pdf},
issn = {09518320},
journal = {reliability engineering and system safety},
keywords = {computer code,covariance,gaussian process,metamodel,sensitivity analysis,uncertainty},
number = {3},
pages = {742--751},
title = {{calculations of sobol indices for the gaussian process metamodel}},
volume = {94},
year = {2009}
}
@book{pronzato2013,
author = {pronzato, luc and p{\'{a}}zman, andrej},
doi = {10.1007/978-1-4614-6363-4},
file = {:users/roy/documents/knowledge/bibliographie/2013/pronzato, p{\'{a}}zman{\_}design of experiments in nonlinear models.pdf:pdf},
title = {{design of experiments in nonlinear models}},
year = {2013}
}
@article{nobach2007,
author = {nobach, h and cordier, l and delville, j and lewalle, j},
file = {:users/roy/documents/knowledge/bibliographie/2007/nobach et al.{\_}review of some fundamentals of data processing.pdf:pdf},
isbn = {978-3-540-25141-5},
journal = {springer handbook},
pages = {1337--1398},
title = {{review of some fundamentals of data processing}},
year = {2007}
}
@article{damianou2013,
abstract = {in this paper we introduce deep gaussian process (gp) models. deep gps are a deep belief network based on gaussian process mappings. the data is modeled as the output of a multivariate gp. the inputs to that gaussian process are then governed by another gp. a single layer model is equivalent to a standard gp or the gp latent variable model (gp-lvm). we perform inference in the model by approximate variational marginalization. this results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. our fully bayesian treatment allows for the application of deep models even when data is scarce. model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.},
archiveprefix = {arxiv},
arxivid = {1211.0358},
author = {damianou, andreas c. and lawrence, neil d.},
eprint = {1211.0358},
file = {:users/roy/documents/knowledge/bibliographie/2012/damianou, lawrence{\_}deep gaussian processes.pdf:pdf},
isbn = {026218253x},
issn = {0029-5981},
journal = {international journal for numerical methods in engineering},
month = {nov},
number = {3},
pages = {455--471},
title = {{deep gaussian processes}},
volume = {63},
year = {2012}
}
@article{patel2015,
abstract = {a grand challenge in machine learning is the development of computational algorithms that match or outperform humans in perceptual inference tasks that are complicated by nuisance variation. for instance, visual object recognition involves the unknown object position, orientation, and scale in object recognition while speech recognition involves the unknown voice pronunciation, pitch, and speed. recently, a new breed of deep learning algorithms have emerged for high-nuisance inference tasks that routinely yield pattern recognition systems with near- or super-human capabilities. but a fundamental question remains: why do they work? intuitions abound, but a coherent framework for understanding, analyzing, and synthesizing deep learning architectures has remained elusive. we answer this question by developing a new probabilistic framework for deep learning based on the deep rendering model: a generative probabilistic model that explicitly captures latent nuisance variation. by relaxing the generative model to a discriminative one, we can recover two of the current leading deep learning systems, deep convolutional neural networks and random decision forests, providing insights into their successes and shortcomings, as well as a principled route to their improvement.},
archiveprefix = {arxiv},
arxivid = {1504.00641},
author = {patel, ankit b and nguyen, tan and baraniuk, richard g},
eprint = {1504.00641},
file = {:users/roy/documents/knowledge/bibliographie/2015/patel, nguyen, baraniuk{\_}a probabilistic theory of deep learning.pdf:pdf},
isbn = {1504.00641},
journal = {arxiv},
month = {apr},
pages = {1--56},
title = {{a probabilistic theory of deep learning}},
year = {2015}
}
@inproceedings{arandiga2005,
abstract = {we explore the use of the singular value decomposition (svd) in image compression. we link the svd and the multiresolution algorithms. in [22] it is derived a multiresolution representation of the svd decomposition, and in [15] the svd algorithm and wavelets are linked, proposing a mixed algorithm which roughly consist on applying firstly a discrete wavelet transform and secondly the svd algorithm to each subband. we propose a new algorithm, which is carried out in two main steps. firstly we decompose the data matrix corresponding to the image following a singular value decomposition. secondly we apply a harten's multiresolution decomposition to the singular vectors which are considered significant. we study the compression capabilities of this new algorithm. we also propose a variant of the implementation, where the multiresolution transformation is carried out by blocks. we apply on each block, depending on a selection process, either the algorithm presented or the 2d multiresolution algorithm based on biorthogonal wavelets.},
address = {corfu, greece},
author = {arandiga, francesc and donat, rosa and trillo, j. carlos},
booktitle = {5th wseas international conference on multimedia, internet {\&} video technologies},
file = {:users/roy/documents/knowledge/bibliographie/2005/arandiga, donat, trillo{\_}svd-wavelet algorithm for image compression svd-wavelet algorithm for image compression.pdf:pdf},
keywords = {singular value decomposition,image processing,multiresolution},
pages = {171--177},
publisher = {world scientific and engineering academy and society (wseas)},
title = {{svd-wavelet algorithm for image compression svd-wavelet algorithm for image compression}},
year = {2005}
}
@article{palar2016,
author = {Palar, Pramudita Satria and Tsuchiya, Takeshi and Parks, Geoffrey Thomas},
doi = {10.1016/j.cma.2016.03.022},
file = {:users/roy/documents/knowledge/bibliographie/2016/palar, tsuchiya, parks{\_}multi-fidelity non-intrusive polynomial chaos based on regression.pdf:pdf},
issn = {00457825},
journal = {Computer methods in applied mechanics and engineering},
month = {jun},
pages = {579--606},
publisher = {elsevier ltd},
title = {{Multi-fidelity non-intrusive polynomial chaos based on regression}},
volume = {305},
year = {2016}
}
@article{owen2017,
abstract = {polynomial chaos and gaussian process emulation are methods for surrogate-based uncertainty quantifcation, and have been developed independently in their respective communities over the last 25 years. despite tackling similar problems in the field, to our knowledge there has yet to be a critical comparison of the two approaches in the literature. we begin by providing a detailed description of polynomial chaos and gaussian process approaches for building a surrogate model of a black-box function. the accuracy of each surrogate method is then tested and compared for two simulators used in industry: a land-surface model (adjules) and a launch vehicle controller (vegacontrol). we analyse surrogates built on experimental designs of various size and type to investigate their performance in a range of modelling scenarios. specifcally, polynomial chaos and gaussian process surrogates are built on sobol sequence and tensor grid designs. their accuracy is measured by their ability to estimate the mean and standard deviation of the simulator output, as well as a root mean square error metric, based on an independent validation design. we find that one method does not unanimously outperform the other, but advantages can be gained in some cases, such that the preferred method depends on the modelling goals of the practitioner.},
archiveprefix = {arxiv},
arxivid = {arxiv:1511.00926v2},
author = {owen, n e and challenor, p. and menon, p. p. and bennani, s.},
doi = {10.1137/15m1046812},
eprint = {arxiv:1511.00926v2},
file = {:users/roy/documents/knowledge/bibliographie/2017/owen et al.{\_}comparison of surrogate-based uncertainty quantification methods for computationally expensive simulators.pdf:pdf},
issn = {2166-2525},
journal = {siam/asa journal on uncertainty quantification},
keywords = {1,10,1137,15m1046812,33c45,60g15,65c20,ams subject classifications,black-box function,computer simulation of physical,doi,emulator,gaussian process,in science,introduction,polynomial chaos,systems is now ubiquitous},
number = {1},
pages = {403--435},
title = {{comparison of surrogate-based uncertainty quantification methods for computationally expensive simulators}},
volume = {5},
year = {2017}
}
@article{kim,
author = {kim, yea-seul and reinecke, katharina and hullman, jessica},
file = {:users/roy/documents/knowledge/bibliographie/unknown/kim, reinecke, hullman{\_}data through others ' eyes the impact of visualizing others ' expectations on visualization interpretation.pdf:pdf},
title = {{data through others ' eyes : the impact of visualizing others ' expectations on visualization interpretation}}
}
@inproceedings{dalal2008,
abstract = {low-discrepancy sequences, also known as ldquoquasi-randomrdquo sequences, are numbers that are better equidistributed in a given volume than pseudo-random numbers. evaluation of high-dimensional integrals is commonly required in scientific fields as well as other areas (such as finance), and is performed by stochastic monte carlo simulations. simulations which use quasi-random numbers can achieve faster convergence and better accuracy than simulations using conventional pseudo-random numbers. such simulations are called quasi-monte carlo. conventional monte carlo simulations are increasingly implemented on reconfigurable devices such as fpgas due to their inherently parallel nature. this has not been possible for quasi-monte carlo simulations because, to our knowledge, no low-discrepancy sequences have been generated in hardware before. we present fpga-optimized scalable designs to generate three different common low-discrepancy sequences: sobol, niederreiter and halton. we implement these three generators on virtex-4 fpgas with varying degrees of fine-grained parallelization, although our ideas can be applied to a far broader class of sequences. we conclude with results from the implementation of an actual quasi-monte carlo simulation for extracting partial inductances from integrated circuits.},
author = {dalal, ishaan l. and stefan, deian and harwayne-gidansky, jared},
booktitle = {2008 international conference on application-specific systems, architectures and processors},
doi = {10.1109/asap.2008.4580163},
file = {:users/roy/documents/knowledge/bibliographie/2008/dalal, stefan, harwayne-gidansky{\_}low discrepancy sequences for monte carlo simulations on reconfigurable platforms.pdf:pdf},
isbn = {978-1-4244-1897-8},
issn = {10636862},
month = {jul},
pages = {108--113},
publisher = {ieee},
title = {{low discrepancy sequences for monte carlo simulations on reconfigurable platforms}},
year = {2008}
}
@article{beans2017,
abstract = {the article focuses on the joint effort of musicians and researchers in the field of data sonification which translates data into sound. topics discussed are meeting of scientists and musicians at the international community for auditory display (icad), new mexico, applications include sonification of ocean waves, sound of walking to help patients of parkinson's disease, and comments from composer margaret schedel.},
author = {beans, carolyn},
doi = {10.1073/pnas.1705325114},
file = {:users/roy/documents/knowledge/bibliographie/2017/beans{\_}science and culture musicians join scientists to explore data through sound.pdf:pdf},
isbn = {1705325114},
issn = {0027-8424},
journal = {proceedings of the national academy of sciences},
number = {18},
pages = {4563--4565},
pmid = {28461386},
title = {{science and culture: musicians join scientists to explore data through sound}},
volume = {114},
year = {2017}
}
@article{mercier2016,
abstract = {this paper focuses on the experimental and numerical investigation of the shape taken by confined turbulent ch4/h2/air premixed flames stabilized over a bluff-body swirling injector. two configurations, which correspond to two levels of h2 enrichment in the ch4/h2 fuel blend, are investigated. experiments show that high h2 concentrations promote m flame shapes, whereas v flame shapes are observed for lower values of h2 enrichment. in both cases, non-reacting and reacting flow large eddy simulation (les) calculations were performed. numerical results are compared with detailed velocimetry measurements under non-reacting and reacting conditions, oh-laser induced fluorescence and oh* chemiluminescence measurements. all temperatures of solid walls of the experimental setup including the combustor dump plane, the injector central rod tip, the combustor sidewalls and the quartz windows were also characterized. assuming a fully adiabatic combustion chamber, les always predicts an m flame shape and does not capture the v to m shape transition observed in the experiments when the hydrogen concentration in the fuel blend is increased. by accounting for non-adiabaticity using measured thermal boundary conditions, simulations predict the correct flame stabilization for both v and m flames and show a good agreement with experiments in terms of flame shape. key features that need to be included in non-adiabatic simulations are finally stressed out.},
author = {mercier, r. and guiberti, t.f. and chatelier, a. and durox, d. and gicquel, o. and darabiha, n. and schuller, t. and fiorina, b.},
doi = {10.1016/j.combustflame.2016.05.006},
file = {:users/roy/documents/knowledge/bibliographie/2016/mercier et al.{\_}experimental and numerical investigation of the influence of thermal boundary conditions on premixed swirling flame stabi.pdf:pdf},
issn = {00102180},
journal = {combustion and flame},
keywords = {h2 enrichment,large eddy simulation,non-adiabatic combustion,swirling flames,turbulent premixed combustion},
month = {sep},
pages = {42--58},
publisher = {elsevier inc.},
title = {{experimental and numerical investigation of the influence of thermal boundary conditions on premixed swirling flame stabilization}},
volume = {171},
year = {2016}
}
@article{nair2010,
abstract = {restricted boltzmann machines were developed using binary stochastic hidden units. these can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. the learning and inference rules for these “stepped sigmoid units” are unchanged. they can be approximated eﬃciently by noisy, rectiﬁed linear units. compared with binary units, these units learn features that are better for object recognition on the norb dataset and face veriﬁcation on the labeled faces in the wild dataset. unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
author = {nair, vinod and hinton, geoffrey e},
file = {:users/roy/documents/knowledge/bibliographie/2010/nair, hinton{\_}rectified linear units improve restricted boltzmann machines.pdf:pdf},
journal = {proceedings of the 27th international conference on machine learning},
number = {3},
pages = {807--814},
title = {{rectified linear units improve restricted boltzmann machines}},
year = {2010}
}
@article{chern2017,
author = {chern, albert and kn{\"{o}}ppel, felix and berlin, t u and pinkall, ulrich and berlin, t u and schr{\"{o}}der, peter},
doi = {http://dx.doi.org/10.1145/3072959.3073591},
file = {:users/roy/documents/knowledge/bibliographie/2017/chern et al.{\_}inside fluids clebsch maps for visualization and processing.pdf:pdf},
number = {4},
title = {{inside fluids : clebsch maps for visualization and processing}},
volume = {36},
year = {2017}
}
@article{jassim2013,
author = {jassim, firas a},
file = {:users/roy/documents/knowledge/bibliographie/2013/jassim{\_}image inpainting by kriging interpolation technique.pdf:pdf},
keywords = {-image inpainting,10,and for eliminating noises,been proposed by,for,gauss convolution kernel has,image masking,inpainting algorithm that uses,kriging,rbf,reconstruction of damaged images,scratch removal,text removal,the radial basis functions},
number = {5},
pages = {91--96},
title = {{image inpainting by kriging interpolation technique}},
volume = {3},
year = {2013}
}
@article{gunes2006,
abstract = {data recovery and reconstruction methods for unsteady flow fields with spatio-temporal missing data are studied based on proper orthogonal decomposition (pod) and on kriging interpolation. it is found that for sufficient temporal resolution, pod-based methods outperform kriging interpolation. however, for insufficient temporal resolution, large spatial gappiness or for flow fields with black zones, kriging interpolation is more effective. the comparison is performed based on randomly generated laminar and turbulent flow fields obtained from simulations of uniform flow past a circular cylinder. ?? 2005 elsevier inc. all rights reserved.},
author = {gunes, hasan and sirisup, sirod and karniadakis, george em},
doi = {10.1016/j.jcp.2005.06.023},
file = {:users/roy/documents/knowledge/bibliographie/2006/gunes, sirisup, karniadakis{\_}gappy data to krig or not to krig.pdf:pdf},
issn = {00219991},
journal = {journal of computational physics},
keywords = {kriging,proper orthogonal decomposition,unsteady flow},
number = {1},
pages = {358--382},
pmid = {9021644},
title = {{gappy data: to krig or not to krig?}},
volume = {212},
year = {2006}
}
@incollection{eldred2016,
abstract = {when faced with a restrictive evaluation budget that is typical of today's high- fidelity simulationmodels, the effective exploitation of lower-fidelity alternatives within the uncertainty quantification (uq) process becomes critically important. herein, we explore the use of multifidelity modeling within uq, for which we rigorously combine information from multiple simulation-based models within a hierarchy of fidelity, in seeking accurate high-fidelity statistics at lower computational cost. motivated by correction functions that enable the provable convergence of a multifidelity optimization approach to an optimal high-fidelity point solution, we extend these ideas to discrepancy modeling within a stochas- tic domain and seek convergence of a multifidelity uncertainty quantification process to globally integrated high-fidelity statistics. for constructing stochastic models of both the low-fidelity model and the model discrepancy, we employ stochastic expansion methods (non-intrusive polynomial chaos and stochastic collocation) computed by integration/interpolation on structured sparse grids or regularized regression on unstructured grids. we seek to employ a coarsely resolved grid for the discrepancy in combination with a more finely resolved grid for the low-fidelity model. the resolutions of these grids may be defined statically or determined through uniform and adaptive refinement processes. adaptive refinement is particularly attractive, as it has the ability to preferentially target stochastic regions where the model discrepancy becomes more complex, i.e.,where the predictive capabilities of the low-fidelitymodel start to break down and greater reliance on the high-fidelity model (via the discrepancy) is necessary. these adaptive refinement processes can either be performed separately for the different grids or within a coordinated multifidelity algorithm. in particular, we present an adaptive greedy multifidelity approach in which we extend the generalized sparse grid concept to consider candidate index set refinements drawn from multiple sparse grids, as governed by induced changes in the statistical quantities of interest and normalized by relative computational cost. through a series of numerical experiments using statically defined sparse grids, adaptive multifidelity sparse grids, and multifidelity compressed sensing, we demonstrate that the multifidelity uq process converges more rapidly than a single-fidelity uq in cases where the variance of the discrepancy is reduced relative to the variance of the high-fidelitymodel (resulting in reductions in initial stochastic error), where the spectrum of the expansion coefficients of the model discrepancy decays more rapidly than that of the high-fidelity model (resulting in accelerated convergence rates), and/or where the discrepancy is more sparse than the high-fidelitymodel (requiring the recovery of fewer significant terms).},
author = {eldred, michael s and ng, leo w t and barone, matthew f and domino, stefan p},
booktitle = {handbook of uncertainty quantification},
doi = {10.1007/978-3-319-11259-6_25-1},
file = {:users/roy/documents/knowledge/bibliographie/2015/eldred et al.{\_}multifidelity uncertainty quantification using spectral stochastic discrepancy models.pdf:pdf},
isbn = {978-3-319-12384-4},
keywords = {compressed sensing,discrepancy model,multifidelity,polynomial chaos,sparse grid,stochastic collocation,uncertainty quantification,wind turbine},
pages = {1--45},
publisher = {springer international publishing},
title = {{multifidelity uncertainty quantification using spectral stochastic discrepancy models}},
year = {2015}
}


@article{nash1970,
abstract = {the principles governing the application of the conceptual model technique to river flow forecasting are discussed. the necessity for a systematic approach to the development and testing of the model is explained and some preliminary ideas suggested.},
author = {nash, j.e. and sutcliffe, j.v.},
doi = {10.1016/0022-1694(70)90255-6},
file = {:users/roy/documents/knowledge/bibliographie/1970/nash, sutcliffe{\_}river flow forecasting through conceptual models part i — a discussion of principles.pdf:pdf},
isbn = {0022-1694},
issn = {00221694},
journal = {journal of hydrology},
month = {apr},
number = {3},
pages = {282--290},
title = {{river flow forecasting through conceptual models part i — a discussion of principles}},
volume = {10},
year = {1970}
}
@article{knio2006,
abstract = {uncertainty quantification in cfd computations is receiving increased interest, due in large part to the increasing complexity of physical models, and the inherent introduction of random model data. this paper focuses on recent application of pc methods for uncertainty representation and propagation in cfd computations. the fundamental concept on which polynomial chaos (pc) representations are based is to regard uncertainty as generating a new set of dimensions, and the solution as being dependent on these dimensions. a spectral decomposition in terms of orthogonal basis functions is used, the evolution of the basis coefficients providing quantitative estimates of the effect of random model data. a general overview of pc applications in cfd is provided, focusing exclusively on applications involving the unreduced navier-stokes equations. included in the present review are an exposition of the mechanics of pc decompositions, an illustration of various means of implementing these representations, and a perspective on the applicability of the corresponding techniques to propagate and quantify uncertainty in navier-stokes computations. {\textcopyright} 2006 the japan society of fluid mechanics and elsevier b.v.},
author = {knio, o. m. and {le ma{\^{i}}tre}, o. p.},
doi = {10.1016/j.fluiddyn.2005.12.003},
file = {:users/roy/documents/knowledge/bibliographie/2006/knio, le ma{\^{i}}tre{\_}uncertainty propagation in cfd using polynomial chaos decomposition.pdf:pdf},
isbn = {1410516725},
issn = {0169-5983},
journal = {fluid dynamics research},
keywords = {cfd,navier-stokes,numerical method,polynomial chaos,uncertainty quantification},
month = {sep},
number = {9},
pages = {616--640},
title = {{uncertainty propagation in cfd using polynomial chaos decomposition}},
volume = {38},
year = {2006}
}
@article{foster2009,
abstract = {the use of gaussian processes can be an effective approach to$\backslash$nprediction in a supervised learning environment.  for large data$\backslash$nsets, the standard gaussian process approach requires solving very$\backslash$nlarge systems of linear equations and approximations are required$\backslash$nfor the calculations to be practical.   we will focus on the$\backslash$nsubset of regressors approximation technique. we will demonstrate$\backslash$nthat there can be numerical instabilities in a well known$\backslash$nimplementation of the technique. we discuss alternate$\backslash$nimplementations that have better numerical stability properties$\backslash$nand can lead to better predictions. our results will be$\backslash$nillustrated by looking at an application involving prediction of$\backslash$ngalaxy redshift from broadband spectrum data.},
author = {foster, leslie and waagen, alex and aijaz, nabeela and hurley, michael and luis, apolonio and rinsky, joel and satyavolu, chandrika and way, michael j. and gazis, paul and srivastava, ashok},
file = {:users/roy/documents/knowledge/bibliographie/2009/foster et al.{\_}stable and efficient gaussian process calculations.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {journal of machine learning research},
keywords = {gaussian processes,low rank approximations,numerical stability,photometric red-,shift,subset of regressors method},
pages = {857----882},
title = {{stable and efficient gaussian process calculations}},
volume = {10},
year = {2009}
}
@inproceedings{blonigan2016,
address = {reston, virginia},
author = {blonigan, patrick j and wang, qiqi and nielsen, eric j and diskin, boris},
booktitle = {54th aiaa aerospace sciences meeting},
doi = {10.2514/6.2016-0296},
file = {:users/roy/documents/knowledge/bibliographie/2016/blonigan et al.{\_}least squares shadowing sensitivity analysis of chaotic flow around a two-dimensional airfoil.pdf:pdf},
isbn = {978-1-62410-393-3},
month = {jan},
number = {january},
pages = {1--28},
publisher = {american institute of aeronautics and astronautics},
title = {{least squares shadowing sensitivity analysis of chaotic flow around a two-dimensional airfoil}},
year = {2016}
}
@book{montgomery2011,
abstract = {chapter 1 is an introduction to the field of statistics and how engineers use statistical methodology as part of the engineering problem-solving process. this chapter also introduces the reader to some engineering applications of statistics, including building empirical models, designing engineering experiments, and monitoring manufacturing processes. these topics are discussed in more depth in subsequent chapters. chapters 2, 3, 4, and 5 cover the basic concepts of probability, discrete and continuous random variables, probability distributions, expected values, joint probability distributions, and independence. we have given a reasonably complete treatment of these topics but have avoided many of the mathematical or more theoretical details. chapter 6 begins the treatment of statistical methods with random sampling; data summary and description techniques, including stem-and-leaf plots, histograms, box plots, and probability plotting; and several types of time series plots. chapter 7 discusses sampling distributions, the central limit theorem, and point estimation of parameters. this chapter also introduces some of the important properties of estimators, the method of maximum likelihood, the method of moments, and bayesian estimation. chapter 8 discusses interval estimation for a single sample. topics included are confidence intervals for means, variances or standard deviations, proportions, prediction intervals, and tolerance intervals. chapter 9 discusses hypothesis tests for a single sample. chapter 10 presents tests and confidence intervals for two samples. this material has been extensively rewritten and reorganized. there is detailed information and examples of methods for determining appropriate sample sizes. we want the student to become familiar with how these techniques are used to solve real-world engineering problems and to get some understanding of the concepts behind them. we give a logical, heuristic development of the procedures rather than a formal, mathematical one. we have also included some material on nonparametric methods in these chapters. chapters 11 and 12 present simple and multiple linear regression including model adequacy checking and regression model diagnostics and an introduction to logistic regression. we use matrix algebra throughout the multiple regression material (chapter 12) because it is the only easy way to understand the concepts presented. scalar arithmetic presentations of multiple regression are awkward at best, and we have found that undergraduate engineers are exposed to enough matrix algebra to understand the presentation of this material. chapters 13 and 14 deal with single- and multifactor experiments, respectively. the notions of randomization, blocking, factorial designs, interactions, graphical data analysis, and fractional factorials are emphasized. chapter 15 introduces statistical quality control, emphasizing the control chart and the fundamentals of statistical process control.},
author = {montgomery, douglas c. and runger, george c.},
file = {:users/roy/documents/knowledge/bibliographie/2011/montgomery, runger{\_}applied statistics and probability for engineers.pdf:pdf},
isbn = {9780470053041},
pages = {792},
pmid = {16464839},
title = {{applied statistics and probability for engineers}},
year = {2011}
}
@article{iooss2013b,
author = {iooss, bertrand},
file = {:users/roy/documents/knowledge/bibliographie/2013/iooss{\_}m{\'{e}}tamod{\`{e}}le.pdf:pdf},
pages = {1--90},
title = {{m{\'{e}}tamod{\`{e}}le}},
year = {2013}
}
@incollection{poinsot2017,
abstract = {this chapter describes the basic phenomena controlling reacting flows and the corresponding computational methodologies. combustion regimes (premixed or diffusion flames, laminar or turbulent cases, stable or unstable behavior) are first defined before introducing the basic terminology required to understand numerical techniques for reacting flows. the governing equations for combustion are more complex than those used in classical aerodynamics, and their specificities are discussed before describing numerical methods for laminar flames. finally, the various methods used for turbulent flames are discussed, focusing on recent techniques such as direct numerical simulations and large eddy simulations.},
author = {poinsot, thierry j and veynante, denis p},
booktitle = {encyclopedia of computational mechanics second edition},
file = {:users/roy/documents/knowledge/bibliographie/2017/poinsot, veynante{\_}combustion.pdf:pdf},
isbn = {9781119003793},
keywords = {chemistry,combustion,instabilities,reacting flows,turbulent flames},
pages = {30},
publisher = {ile},
title = {{combustion}},
volume = {3},
year = {2017}
}
@inproceedings{mycek2017a,
author = {mycek, paul and ricci, sophie and roy, pamphile t. and goutal, nicole},
booktitle = {cemracs},
file = {:users/roy/documents/knowledge/bibliographie/2017/mycek et al.{\_}multilevel monte carlo estimation of covariances in the context of open-channel flow simulations(3).pdf:pdf},
title = {{multilevel monte carlo estimation of covariances in the context of open-channel flow simulations}},
year = {2017}
}


@article{razafindradina2013,
abstract = {this paper gives a new scheme of colour image compression related to singular values matrix approximation. the image has to be converted in luminance / chrominance space before being processed like jpeg standard 4 : 2 : 0. our approach is first based on a chrominance sub-sampling, then an over estimation of its singular values. instead of keeping only the k first singular values for the 3 components r, g and b, we hold k first coefficients for the y component and only k' (k' {\textless}= k) coefficients for 2 components cb and cr. results show that for 512 x 512 pixels that, from k = 40 corresponding in an average distortion of 30 db and a ratio of 15 : 1, the restored image has good quality. the algorithm allows a significant speed gain by sub-sampling too.},
archiveprefix = {arxiv},
arxivid = {1308.0608},
author = {razafindradina, henri bruno and randriamitantsoa, paul auguste and razafindrakoto, nicolas raft},
eprint = {1308.0608},
file = {:users/roy/documents/knowledge/bibliographie/2013/razafindradina, randriamitantsoa, razafindrakoto{\_}compression d'images par svd et sur-approximation des composantes de chrominance.pdf:pdf},
keywords = {2010,a,a r i m,approximation,compression,pages 1 {\`{a}} 8,sub-sampling,svd,volume 1},
month = {aug},
pages = {1--8},
title = {{compression d'images par svd et sur-approximation des composantes de chrominance}},
volume = {1},
year = {2013}
}
@article{potter2012a,
abstract = {quantifying uncertainty is an increasingly important topic across many domains. the uncertainties present in data come with many diverse representations having originated from a wide variety of disciplines. communicating these uncertainties is a task often left to visualization without clear connection between the quantification and visualization. in this paper, we first identify frequently occurring types of uncertainty. second, we connect those uncertainty representations to ones commonly used in visualization. we then look at various approaches to visualizing this uncertainty by partitioning the work based on the dimensionality of the data and the dimensionality of the uncertainty. we also discuss noteworthy exceptions to our taxonomy along with future research directions for the uncertainty visualization community.},
author = {Potter, Kristin and Rosen, Paul and Johnson, Chris R.},
doi = {10.1007/978-3-642-32677-6_15},
file = {:users/roy/documents/knowledge/bibliographie/2012/potter, rosen, johnson{\_}from quantification to visualization a taxonomy of uncertainty visualization approaches.pdf:pdf},
isbn = {9783642326769},
issn = {18684238},
journal = {{IFIP} Advances in Information and Communication Technology},
keywords = {uncertainty visualization},
pages = {226--247},
pmid = {25663949},
title = {{From quantification to visualization: a taxonomy of uncertainty visualization approaches}},
volume = {377 aict},
year = {2012}
}


@article{volkwein2013,
author = {volkwein, stefan},
file = {:users/roy/documents/knowledge/bibliographie/2013/volkwein{\_}proper orthogonal decomposition theory and reduced-order modeling.pdf:pdf},
number = {september},
pages = {1--29},
title = {{proper orthogonal decomposition: theory and reduced-order modeling}},
year = {2013}
}
@article{michaele.muelleragianlucaiaccarinoaheinzpitscha,
abstract = {while the accuracy of chemical kinetic mechanisms continues to improve, these mechanisms are still models with, sometimes considerable, uncertainty. in order to rigorously validate turbulent combustion simulations against exper- imental data, this uncertainty must be separated from deficiencies in the turbulent combustion model itself. in this work, a method is developed for quantifying the uncertainty in turbulent flame simulations due to input uncertainty in the chemical mechanism. here the method is developed for large eddy simulation (les) combined with a steady flamelet model. rather than a brute force probabilistic approach in which hundreds or thousands of les runs are required to compute statistics of outputs of interest, the method takes advantage of the actual algorithm employed with the steady flamelet model. first, the high-dimensional uncertainty in the chemical kinetics is propagated through the flamelet equations, and the resulting lower-dimensional joint distribution of the temperature, species mass frac- tions, and other derived quantities is used as a stochastic equation of state in the les. since only a few “active” quantities are needed to evolve the les governing equations, efficient non-intrusive stochastic collocation is used to propagate the uncertainty in the density, requiring only a few les runs. this process captures the uncertainty in the flow field induced by the uncertainty in the chemical kinetic rates. the remaining uncertainty in “passive” quantities, that is, quantities needed only for post-processing such as the temperature and species mass fractions, is computed with random sampling during the les runs. the uncertainty quantification algorithm is demonstrated with sandia flame d, and it is shown that the uncertainty in the simulation results caused by uncertainties in the kinetic rates is sufficiently large to account for the discrepancies with the experimental measurements. the implication is that the turbulent combustion model cannot be fairly assessed with such a large uncertainty.},
author = {muellera, michael e. and iaccarino, gianluca and pitsch, heinz},
file = {:users/roy/documents/knowledge/bibliographie/2012/muellera, iaccarino, pitsch{\_}chemical kinetic uncertainty quantification for large eddy simulation of turbulent nonpremixed combustion.pdf:pdf},
journal = {combustion institute},
keywords = {large eddy simulation,sandia flame d,turbulent nonpremixed flames,uncertainty quantification},
title = {{chemical kinetic uncertainty quantification for large eddy simulation of turbulent nonpremixed combustion}},
year = {2012}
}
@inproceedings{ni2016,
address = {reston, virginia},
author = {ni, angxiu and blonigan, patrick j. and chater, mario and wang, qiqi},
booktitle = {46th aiaa fluid dynamics conference},
doi = {10.2514/6.2016-4399},
file = {:users/roy/documents/knowledge/bibliographie/2016/ni et al.{\_}sensitivity analysis on chaotic dynamical system by non-intrusive least square shadowing (ni-lss).pdf:pdf},
isbn = {978-1-62410-436-7},
month = {jun},
number = {june},
pages = {1--16},
publisher = {american institute of aeronautics and astronautics},
title = {{sensitivity analysis on chaotic dynamical system by non-intrusive least square shadowing (ni-lss)}},
year = {2016}
}
@article{riveiro2007,
abstract = {this paper highlights the importance of uncertainty visualization in information fusion, reviews general methods of representing uncertainty and presents perceptual and cognitive principles from tufte, chambers and bertin as well as users experiments documented in the literature. examples of uncertainty representations in information fusion are analyzed using these general theories. these principles can be used in future theoretical evaluations of existing or newly developed uncertainty visualization techniques before usability testing with actual users.},
author = {riveiro, maria},
doi = {10.1109/icif.2007.4408049},
file = {:users/roy/documents/knowledge/bibliographie/2007/riveiro{\_}evaluation of uncertainty visualization techniques for information fusion.pdf:pdf},
isbn = {978-0-662-45804-3},
issn = {978-0-662-45804-3},
journal = {10th international conference on information fusion},
keywords = {data visualisation,information fusion,sensor fusion,uncertainty representations,uncertainty visualization techniques,usability testing},
pages = {1--8},
title = {{evaluation of uncertainty visualization techniques for information fusion}},
year = {2007}
}
@techreport{bohling2005,
abstract = {“geostatistics: study of phenomena that vary in space and/or time” (deutsch, 2002) “geostatistics can be regarded as a collection of numerical techniques that deal with the characterization of spatial attributes, employing primarily random models in a manner similar to the way in which time series analysis characterizes temporal data.” (olea, 1999) “geostatistics offers a way of describing the spatial continuity of natural phenomena and provides adaptations of classical regression techniques to take advantage of this continuity.” (isaaks and srivastava, 1989) geostatistics deals with spatially autocorrelated data. autocorrelation: correlation between elements of a series and others from the same series separated from them by a given interval. (oxford american dictionary) some spatially autocorrelated parameters of interest to reservoir engineers: facies, reservoir thickness, porosity, permeability},
author = {bohling, geoff},
doi = {10.1162/0162287054769931},
file = {:users/roy/documents/knowledge/bibliographie/2005/bohling{\_}introduction to geostatistics and variogram analysis.pdf:pdf},
isbn = {0521587476},
issn = {{\textless}null{\textgreater}},
number = {october},
pages = {1--20},
pmid = {20443128},
title = {{introduction to geostatistics and variogram analysis}},
year = {2005}
}
@book{soize2017,
author = {soize, christian},
doi = {10.1007/978-3-319-54339-0},
file = {:users/roy/documents/knowledge/bibliographie/2017/soize{\_}uncertainty quantication an accelerated course with advanced applications in computational engineering.pdf:pdf},
isbn = {9783319543383},
title = {{uncertainty quantication: an accelerated course with advanced applications in computational engineering}},
year = {2017}
}
@incollection{box1951,
abstract = {the work described is the result of a study extending over the past few years by a chemist and a statistician. development has come about mainly in answer to problems of determining optimum conditions in chemical investigations, but we believe that the methods will be of value in other fields where experimentation is sequential and the error fairly small.},
author = {box, g. e. p. and wilson, k. b.},
booktitle = {journal of the royal statistical society},
doi = {10.1007/978-1-4612-4380-9_23},
file = {:users/roy/documents/knowledge/bibliographie/1992/box, wilson{\_}on the experimental attainment of optimum conditions.pdf:pdf},
isbn = {ee000175 00359246 di993125 99p0202f},
issn = {0035-9246},
number = {1},
pages = {270--310},
title = {{on the experimental attainment of optimum conditions}},
volume = {13},
year = {1992}
}
@article{elmocayd2016,
author = {{el mo{\c{c}}ayd}, nabil and ricci, sophie and rochoux, c},
file = {:users/roy/documents/knowledge/bibliographie/2016/el mo{\c{c}}ayd, ricci, rochoux{\_}polynomial surrogate model for open-channel flows in steady state.pdf:pdf},
pages = {1--40},
title = {{polynomial surrogate model for open-channel flows in steady state}},
year = {2016}
}
@article{baudin2013,
abstract = {this document is an introduction to sensitivity analysis with the nisp module for scilab. in the first part, we introduce the global sensitivity analysis problem, and present the stan-dardized regression coefficients of affine models. we also present the link with the linear correlation coefficient. in the second part, we present the conditional probability, expecta-tion and variance, and present the laws of total expectation and total variance. in the third part, we present the anova decomposition and present an example based on the product of two independent random variables. we present the decomposition of the expectation and the variance, and analyze the higher order sensitivity indices and the total sensitivity indices. the fourth part focuses on the ishigami function, a function for which all sensitivity indices are known analytically. in the final part, we present methods to estimate sensitivity indices.},
author = {baudin, m and martinez, j.-m.},
file = {:users/roy/documents/knowledge/bibliographie/2013/baudin, martinez{\_}introduction to sensitivity analysis with nisp.pdf:pdf},
isbn = {9780470059975},
number = {january},
pages = {73},
title = {{introduction to sensitivity analysis with nisp}},
year = {2013}
}
@article{sames2016,
abstract = {additive manufacturing (am), widely known as 3d printing, is a method of manufacturing that forms parts from powder, wire or sheets in a process that proceeds layer by layer. many techniques (using many different names) have been developed to accomplish this via melting or solid-state joining. in this review, these techniques for producing metal parts are explored, with a focus on the science of metal am: processing defects, heat transfer, solidification, solid-state precipitation, mechanical properties and post-processing metallurgy. the various metal am techniques are compared, with analysis of the strengths and limitations of each. only a few alloys have been developed for commercial production, but recent efforts are presented as a path for the ongoing development of new materials for am processes.},
author = {Sames, W. J. and List, F. A. and Pannala, S. and Dehoff, R. R. and Babu, S. S.},
doi = {10.1080/09506608.2015.1116649},
file = {:users/roy/documents/knowledge/bibliographie/2016/sames et al.{\_}the metallurgy and processing science of metal additive manufacturing.pdf:pdf},
isbn = {0950-6608},
issn = {0950-6608},
journal = {International Materials Reviews},
keywords = {3d printing,additive manufacturing review,advanced manufacturing,metallurgy},
number = {5},
pages = {315--360},
title = {{The metallurgy and processing science of metal additive manufacturing}},
volume = {61},
year = {2016}
}
@article{busch2007,
abstract = {heisenberg's uncertainty principle is usually taken to express a limitation of operational possibilities imposed by quantum mechanics. here we demonstrate that the full content of this principle also includes its positive role as a condition ensuring that mutually exclusive experimental options can be reconciled if an appropriate trade-off is accepted. the uncertainty principle is shown to appear in three manifestations, in the form of uncertainty relations: for the widths of the position and momentum distributions in any quantum state; for the inaccuracies of any joint measurement of these quantities; and for the inaccuracy of a measurement of one of the quantities and the ensuing disturbance in the distribution of the other quantity. whilst conceptually distinct, these three kinds of uncertainty relations are shown to be closely related formally. finally, we survey models and experimental implementations of joint measurements of position and momentum and comment briefly on the status of experimental tests of the uncertainty principle.},
archiveprefix = {arxiv},
arxivid = {quant-ph/0609185},
author = {busch, p. and heinonen, t. and lahti, p.},
doi = {10.1016/j.physrep.2007.05.006},
eprint = {0609185},
file = {:users/roy/documents/knowledge/bibliographie/2006/busch, heinonen, lahti{\_}heisenberg's uncertainty principle.pdf:pdf},
isbn = {03701573},
issn = {03701573},
journal = {physics reports},
keywords = {disturbance,inaccuracy,joint measurement,uncertainty principle},
month = {sep},
number = {6},
pages = {155--176},
primaryclass = {quant-ph},
title = {{heisenberg's uncertainty principle}},
volume = {452},
year = {2006}
}
@article{constantine2014a,
abstract = {many multivariate functions in engineering models vary primarily along a few directions in the space of input parameters. when these directions correspond to coordinate directions, one may apply global sensitivity measures to determine the most influential parameters. however, these methods perform poorly when the directions of variability are not aligned with the natural coordinates of the input space. we present a method to first detect the directions of the strongest variability using evaluations of the gradient and subsequently exploit these directions to construct a response surface on a low-dimensional subspace---i.e., the active subspace---of the inputs. we develop a theoretical framework with error bounds, and we link the theoretical quantities to the parameters of a kriging response surface on the active subspace. we apply the method to an elliptic pde model with coefficients parameterized by 100 gaussian random variables and compare it with a local sensitivity analysis method for dimension reduction.},
archiveprefix = {arxiv},
arxivid = {1304.2070},
author = {constantine, paul g. and dow, eric and wang, qiqi},
doi = {10.1137/130916138},
eprint = {1304.2070},
file = {:users/roy/documents/knowledge/bibliographie/2013/constantine, dow, wang{\_}active subspace methods in theory and practice applications to kriging surfaces.pdf:pdf},
issn = {1064-8275},
journal = {siam journal on scientific computing},
keywords = {65c60,gaussian process,active subspace methods,kriging,response surfaces,uncertainty quantification},
month = {apr},
number = {4},
pages = {a1500--a1524},
title = {{active subspace methods in theory and practice: applications to kriging surfaces}},
volume = {36},
year = {2013}
}
@article{saltelli2017,
annote = {imporve calcul of st by using an adaptive strategy.

1. ns
2. guess sti
3. refine with probability prop to sti
4. iterate.},
author = {saltelli, andrea},
file = {:users/roy/documents/knowledge/bibliographie/2017/saltelli{\_}a new sample-based algorithms to compute the total sensitivity index.pdf:pdf},
number = {march},
title = {{a new sample-based algorithms to compute the total sensitivity index}},
year = {2017}
}
@misc{neumann,
author = {neumann, john},
file = {:users/roy/documents/knowledge/bibliographie/unknown/neumann{\_}various techniques used in connection with random digits.pdf:pdf},
title = {{various techniques used in connection with random digits}}
}
@article{zachow2009,
abstract = {rhinologists are often faced with the challenge of assessing nasal breathing from a functional point of view to derive effective therapeutic interventions. while the complex nasal anatomy can be revealed by visual inspection and medical imaging, only vague information is available regarding the nasal airflow itself: rhinomanometry delivers rather unspecific integral information on the pressure gradient as well as on total flow and nasal flow resistance. in this article we demonstrate how the understanding of physiological nasal breathing can be improved by simulating and visually analyzing nasal airflow, based on an anatomically correct model of the upper human respiratory tract. in particular we demonstrate how various information visualization (infovis) techniques, such as a highly scalable implementation of parallel coordinates, time series visualizations, as well as unstructured grid multi-volume rendering, all integrated within a multiple linked views framework, can be utilized to gain a deeper understanding of nasal breathing. evaluation is accomplished by visual exploration of spatio-temporal airflow characteristics that include not only information on flow features but also on accompanying quantities such as temperature and humidity. to our knowledge, this is the first in-depth visual exploration of the physiological function of the nose over several simulated breathing cycles under consideration of a complete model of the nasal airways, realistic boundary conditions, and all physically relevant time-varying quantities.},
author = {zachow, stefan and muigg, philipp and hildebrandt, thomas and doleisch, helmut and hege, hans christian},
doi = {10.1109/tvcg.2009.198},
file = {:users/roy/documents/knowledge/bibliographie/2009/zachow et al.{\_}visual exploration of nasal airflow.pdf:pdf},
isbn = {1077-2626 (print)$\backslash$r1077-2626 (linking)},
issn = {10772626},
journal = {ieee transactions on visualization and computer graphics},
keywords = {flow visualization,exploratory data analysis,interactive visual analysis of scientific data,time-dependent data},
number = {6},
pages = {1407--1414},
pmid = {19834215},
title = {{visual exploration of nasal airflow}},
volume = {15},
year = {2009}
}
@article{villemonteix2009,
abstract = {in many global optimization problems motivated by engineering applications, the number of function evaluations is severely limited by time or cost. to ensure that each evaluation contributes to the localization of good candidates for the role of global minimizer, a sequential choice of evaluation points is usually carried out. in particular, when kriging is used to interpolate past evaluations, the uncertainty associated with the lack of information on the function can be expressed and used to compute a number of criteria accounting for the interest of an additional evaluation at any given point. this paper introduces minimizer entropy as a new kriging-based criterion for the sequential choice of points at which the function should be evaluated. based on $\backslash$emph{\{}stepwise uncertainty reduction{\}}, it accounts for the informational gain on the minimizer expected from a new evaluation. the criterion is approximated using conditional simulations of the gaussian process model behind kriging, and then inserted into an algorithm similar in spirit to the $\backslash$emph{\{}efficient global optimization{\}} (ego) algorithm. an empirical comparison is carried out between our criterion and $\backslash$emph{\{}expected improvement{\}}, one of the reference criteria in the literature. experimental results indicate major evaluation savings over ego. finally, the method, which we call iago (for informational approach to global optimization) is extended to robust optimization problems, where both the factors to be tuned and the function evaluations are corrupted by noise.},
archiveprefix = {arxiv},
arxivid = {cs/0611143},
author = {villemonteix, julien and vazquez, emmanuel and walter, eric},
doi = {10.1007/s10898-008-9354-2},
eprint = {0611143},
file = {:users/roy/documents/knowledge/bibliographie/2009/villemonteix, vazquez, walter{\_}an informational approach to the global optimization of expensive-to-evaluate functions.pdf:pdf},
issn = {0925-5001},
journal = {journal of global optimization},
keywords = {gaussian process,global optimization,kriging,robust optimization,stepwise uncertainty reduction},
month = {aug},
number = {4},
pages = {509--534},
primaryclass = {cs},
title = {{an informational approach to the global optimization of expensive-to-evaluate functions}},
volume = {44},
year = {2009}
}
@article{constantine2014,
abstract = {we present a computational analysis of the reactive flow in a hypersonic scramjet engine with focus on effects of uncertainties in the operating conditions. we employ a novel methodology based on active subspaces to characterize the effects of the input uncertainty on the scramjet performance. the active subspace identifies one-dimensional structure in the map from simulation inputs to quantity of interest that allows us to reparameterize the operating conditions; instead of seven physical parameters, we can use a single derived active variable. this dimension reduction enables otherwise infeasible uncertainty quantification, considering the simulation cost of roughly 9500 cpu-hours per run. for two values of the fuel injection rate, we use a total of 68 simulations to (i) identify the parameters that contribute the most to the variation in the output quantity of interest, (ii) estimate upper and lower bounds on the quantity of interest, (iii) classify sets of operating conditions as safe or unsafe corresponding to a threshold on the output quantity of interest, and (iv) estimate a cumulative distribution function for the quantity of interest.},
archiveprefix = {arxiv},
arxivid = {1408.6269},
author = {constantine, p.g. and emory, michael and larsson, johan and iaccarino, gianluca},
doi = {10.1016/j.jcp.2015.09.001},
eprint = {1408.6269},
file = {:users/roy/documents/knowledge/bibliographie/2015/constantine et al.{\_}exploiting active subspaces to quantify uncertainty in the numerical simulation of the hyshot ii scramjet.pdf:pdf},
issn = {00219991},
journal = {journal of computational physics},
month = {dec},
pages = {1--20},
title = {{exploiting active subspaces to quantify uncertainty in the numerical simulation of the hyshot ii scramjet}},
volume = {302},
year = {2015}
}
@article{kramer1999,
author = {kramer, gregory and walker, bruce and bargar, robin},
file = {:users/roy/documents/knowledge/bibliographie/1999/kramer, walker, bargar{\_}sonification report status of the field and research agenda.pdf:pdf},
isbn = {0967090407},
number = {march},
pages = {38},
title = {{sonification report: status of the field and research agenda}},
year = {1999}
}
@article{s??ndor2004a,
abstract = {we describe the properties of (t,m,s)-nets and halton draws. four types of (t,m,s)-nets, two types of halton draws, and independent draws are compared in an application of maximum simulated likelihood estimation of a mixed logit model. all of the quasi-random procedures are found to perform far better than independent draws. the best performance is attained by one of the (t,m,s)-nets. the properties of the nets imply that two of them should outperform the other two, and our results confirm this expectation. the two more-accurate nets perform better than both types of halton draws, while the two less-accurate nets perform worse than the halton draws. ?? 2003 elsevier ltd. all rights reserved.},
author = {s{\'{a}}ndor, zsolt and train, kenneth},
doi = {10.1016/s0191-2615(03)00014-6},
file = {:users/roy/documents/knowledge/bibliographie/2004/s{\'{a}}ndor, train{\_}quasi-random simulation of discrete choice models.pdf:pdf},
isbn = {01912615},
issn = {01912615},
journal = {transportation research part b: methodological},
month = {may},
number = {4},
pages = {313--327},
pmid = {4989},
title = {{quasi-random simulation of discrete choice models}},
volume = {38},
year = {2004}
}
@misc{margheri,
author = {margheri, l},
file = {:users/roy/documents/knowledge/bibliographie/unknown/margheri{\_}an application of the pod method for uncertainty quantification in a problem of city pollutant dispersion.pdf:pdf},
title = {{an application of the pod method for uncertainty quantification in a problem of city pollutant dispersion}}
}
@article{ehlschlaeger1997,
author = {Ehlschlaeger, Charles R. and Shortridge, Ashton M. and Goodchild, Michael F.},
file = {:users/roy/documents/knowledge/bibliographie/1997/ehlschlaeger, shortridge, goodchild{\_}visualizing spatial data uncertainty animation using animation.pdf:pdf},
journal = {Computers {\&} Geosciences},
keywords = {animation,digital elevation model,optimal route,random fields,spatial data,uncertainty,fields.},
number = {4},
pages = {387--395},
title = {{Visualizing spatial data uncertainty animation using animation}},
volume = {23},
year = {1997}
}
@article{sobol1993,
author = {sobol', i.m},
file = {:users/roy/documents/knowledge/bibliographie/1993/sobol'{\_}sensitivity analysis for nonlinear mathematical models.pdf:pdf},
journal = {mathematical modeling and computational experiment},
keywords = {anova,sobol' sensitivity indices,sobol' variance decomposition},
number = {4},
pages = {407--414},
title = {{sensitivity analysis for nonlinear mathematical models}},
volume = {1},
year = {1993}
}
@article{perrin2017,
abstract = {due to increasing available computational resources and to a series of breakthroughs in the solving of nonlinear equations and in the modeling of complex mechanical systems, simulation nowadays becomes more and more predictive. methods that could quantify the uncertainties associated with the results of the simulation are therefore needed to complete these predictions and widen the possibilities of simulation. one key step of these methods is the exploration of the whole space of the input variables, especially when the computational cost associated with one run of the simulation is high, and when there exists constraints on the inputs, such that the input space cannot be transformed into a hypercube through a bijection. in this context, the present work proposes an adaptive method to generate initial designs of experiments in any bounded convex input space, which are distributed as uniformly as possible on their definition space, while preserving good projection properties for each scalar input. finally, it will be shown how this method can be used to add new elements to an initial design of experiments while preserving very interesting space filling properties.},
author = {perrin, guillaume and cannamela, claire},
file = {:users/roy/documents/knowledge/bibliographie/2017/perrin, cannamela{\_}a repulsion-based method for the definition and the enrichment of optimized space filling designs in constrained input.pdf:pdf},
journal = {journal de la soci{\'{e}}t{\'{e}} fran{\c{c}}aise de statistique},
keywords = {latin hypercube sampling,computer experiment,design of experiments,optimal design,surrogate model},
number = {1},
title = {{a repulsion-based method for the definition and the enrichment of optimized space filling designs in constrained input spaces}},
volume = {158},
year = {2017}
}
@incollection{nodet2016,
abstract = {this contribution presents derivative-basedmethods for local sensitivity analysis, called variational sensitivity analysis (vsa). if one defines an output called the response function, its sensitivity to input variations around a nominal value can be studied using derivative (gradient) information. themain issue ofvsais then to provide an efficient way of computing gradients. this contribution first presents the theoretical grounds of vsa: framework and problem statement and tangent and adjoint methods. then it covers practical means to compute derivatives, from naive to more sophisticated approaches, discussing their various merits. finally, applications of vsa are reviewed, and some examples are presented, covering various applications fields: oceanogra- phy, glaciology, and meteorology.},
author = {nodet, maelle and vidard, arthur},
booktitle = {handbook of uncertainty quantification},
doi = {10.1007/978-3-319-11259-6_32-1},
file = {:users/roy/documents/knowledge/bibliographie/2015/nodet, vidard{\_}variational methods.pdf:pdf},
isbn = {978-3-319-12384-4},
keywords = {adjoint model,automatic differentiation,derivative,geophysical applications,glaciology,gradient,local sensitivity analysis,meteorology,oceanography,stability analysis,tangent model,variational methods,variational sensitivity analysis},
pages = {1--20},
publisher = {springer international publishing},
title = {{variational methods}},
year = {2015}
}
@article{picheny2014,
author = {picheny, victor},
doi = {10.1007/s11222-014-9477-x},
file = {:users/roy/documents/knowledge/bibliographie/2015/picheny{\_}multiobjective optimization using gaussian process emulators via stepwise uncertainty reduction.pdf:pdf},
isbn = {1122201494},
issn = {0960-3174},
journal = {statistics and computing},
keywords = {ego,excursion sets,kriging,pareto front},
month = {nov},
number = {6},
pages = {1265--1280},
title = {{multiobjective optimization using gaussian process emulators via stepwise uncertainty reduction}},
volume = {25},
year = {2015}
}
@article{marrel2011,
abstract = {the global sensitivity analysis of a complex numerical model often calls for the estimation of variance-based importance measures, named sobol' indices. metamodel-based techniques have been developed in order to replace the cpu time-expensive computer code with an inexpensive mathematical function, which predicts the computer code output. the common metamodel-based sensitivity analysis methods are well-suited for computer codes with scalar outputs. however, in the environmental domain, as in many areas of application, the numerical model outputs are often spatial maps, which may also vary with time. in this paper, we introduce an innovative method to obtain a spatial map of sobol' indices with a minimal number of numerical model computations. it is based upon the functional decomposition of the spatial output onto a wavelet basis and the metamodeling of the wavelet coefficients by the gaussian process. an analytical example is presented to clarify the various steps of our methodology. this technique is then applied to a real hydrogeological case: for each model input variable, a spatial map of sobol' indices is thus obtained.},
annote = {color map of sobol indices ={\textgreater} 1d output curve with f(s), indices per s for all orders

do mpi of sobol over s},
archiveprefix = {arxiv},
arxivid = {0911.1189},
author = {marrel, amandine and iooss, bertrand and jullien, michel and laurent, b{\'{e}}atrice and volkova, elena},
doi = {10.1002/env.1071},
eprint = {0911.1189},
file = {:users/roy/documents/knowledge/bibliographie/2011/marrel et al.{\_}global sensitivity analysis for models with spatially dependent outputs.pdf:pdf},
isbn = {1099-095x},
issn = {11804009},
journal = {environmetrics},
keywords = {computer experiment,functional data,gaussian process,metamodel,radionuclide migration},
number = {3},
pages = {383--397},
title = {{global sensitivity analysis for models with spatially dependent outputs}},
volume = {22},
year = {2011}
}

@article{hamby1994,
abstract = {mathematical models are utilized to approximate various highly complex engineering, physical, environmental, social, and economic phenomena. model parameters exerting the most influence on model results are identified through a 'sensitivity analysis'. a comprehensive review is presented of more than a dozen sensitivity analysis methods. this review is intended for those not intimately familiar with statistics or the techniques utilized for sensitivity analysis of computer models. the most fundamental of sensitivity techniques utilizes partial differentiation whereas the simplest approach requires varying parameter values one-at-a-time. correlation analysis is used to determine relationships between independent and dependent variables. regression analysis provides the most comprehensive sensitivity measure and is commonly utilized to build response surfaces that approximate complex models.},
author = {hamby, d. m.},
doi = {10.1007/bf00547132},
file = {:users/roy/documents/knowledge/bibliographie/1994/hamby{\_}a review of techniques for parameter sensitivity analysis of environmental models.pdf:pdf},
isbn = {http://hdl.handle.net/2027.42/42691},
issn = {0167-6369},
journal = {environmental monitoring and assessment},
month = {sep},
number = {2},
pages = {135--154},
pmid = {24214086},
title = {{a review of techniques for parameter sensitivity analysis of environmental models}},
volume = {32},
year = {1994}
}
@article{pia,
abstract = {the issue of how epistemic uncertainties affect the outcome of monte carlo simulation is discussed by means of a concrete use case: the simulation of the longitudinal energy deposition profile of low energy protons. a variety of electromagnetic and hadronic physics models is investigated, and their effects are analyzed. possible systematic effects are highlighted. the results identify requirements for experimental measurements capable of reducing epistemic uncertainties in the simulation.},
author = {pia, maria grazia and batic, matej and begalli, marcia and lechner, anton and quintieri, lina and saracco, paolo},
file = {:users/roy/documents/knowledge/bibliographie/2010/pia et al.{\_}quantifying the unknown.pdf:pdf},
title = {{quantifying the unknown}},
year = {2010}
}
@misc{marco,
author = {marcotte, d.},
file = {:users/roy/documents/knowledge/bibliographie/unknown/marcotte{\_}krigeage.pdf:pdf},
title = {krigeage}
}
@article{nardo2005,
abstract = {bacteria must be able to respond to a changing environment, and one way to respond is to move. the transduction of sensory signals alters the concentration of small phosphorylated response regulators that bind to the rotary flagellar motor and cause switching. this simple pathway has provided a paradigm for sensory systems in general. however, the increasing number of sequenced bacterial genomes shows that although the central sensory mechanism seems to be common to all bacteria, there is added complexity in a wide range of species.},
author = {nardo, m. and saisana, m. and saltelli, a. and tarantola, s.},
doi = {10.1038/nrm1524},
file = {:users/roy/documents/knowledge/bibliographie/2005/nardo et al.{\_}tools for composite indicators building.pdf:pdf},
isbn = {384437549x},
issn = {14710072},
journal = {analysis},
number = {december},
pages = {134},
pmid = {15573139},
title = {{tools for composite indicators building}},
volume = {eur 21682},
year = {2005}
}
@phdthesis{zhu2015,
author = {zhu, manqi},
file = {:users/roy/documents/knowledge/bibliographie/2015/zhu{\_}large eddy simulation of thermal cracking in petroleum industry.pdf:pdf},
keywords = {large eddy simulation,heat transfer,helically ribbed tubes,roughness,thermal cracking process,turbulent reacting flow},
school = {universit{\'{e}} de toulouse},
title = {{large eddy simulation of thermal cracking in petroleum industry}},
year = {2015}
}
@book{lemaitre2010,
abstract = {this book presents applications of spectral methods to problems of uncertainty propagation and quantification in model-based computations, focusing on the computational and algorithmic features of these methods most useful in dealing with models based on partial differential equations, in particular models arising in simulations of fluid flows. spectral stochastic methods are probabilistic in nature, and are consequently rooted in the rich mathematical foundations associated with probability and measure spaces. a brief discussion is provided of only those theoretical aspects needed to set the stage for subsequent applications. these are demonstrated through detailed treatments of elementary problems, as well as in more elaborate examples involving vortex-dominated flows and compressible flows at low mach numbers. some recent developments are also outlined in the book, including iterative techniques (such as stochastic multigrids and newton schemes), intrusive and non-intrusive formalisms, spectral representations using mixed and discontinuous bases, multi-resolution approximations, and adaptive techniques. readers are assumed to be familiar with elementary methods for the numerical solution of time-dependent, partial differential equations; prior experience with spectral approximation is helpful but not essential.},
author = {{le ma{\^{i}}tre}, o. p. and knio, omar m.},
doi = {10.1007/978-90-481-3520-2},
file = {:users/roy/documents/knowledge/bibliographie/2010/le ma{\^{i}}tre, knio{\_}spectral methods for uncertainty quantification.pdf:pdf},
isbn = {978-90-481-3519-6},
pages = {552},
title = {{spectral methods for uncertainty quantification}},
year = {2010}
}

@article{smithmason2017,
author = {{smith mason}, jennifer and retchless, david and klippel, alexander},
doi = {10.1080/15230406.2016.1154804},
file = {:users/roy/documents/knowledge/bibliographie/2017/smith mason, retchless, klippel{\_}domains of uncertainty visualization research a visual summary approach.pdf:pdf},
issn = {1523-0406},
journal = {cartography and geographic information science},
keywords = {classification,uncertainty,visual,visualization},
number = {4},
pages = {296--309},
title = {{domains of uncertainty visualization research: a visual summary approach}},
volume = {44},
year = {2017}
}
@article{laurenceau2008a,
author = {laurenceau, julien and meaux, matthieu},
doi = {10.2514/6.2008-1889},
file = {:users/roy/documents/knowledge/bibliographie/2008/laurenceau, meaux{\_}comparison of gradient and response surface based optimization frameworks using adjoint method.pdf:pdf},
isbn = {978-1-60086-993-8},
journal = {49th aiaa/asme/asce/ahs/asc structures, structural dynamics, and materials conference 16th aiaa/asme/ahs adaptive structures conference 10t},
number = {april},
pages = {1--21},
title = {{comparison of gradient and response surface based optimization frameworks using adjoint method}},
year = {2008}
}
@article{giles2001a,
abstract = {we study monte carlo approximations to high dimensional parameter dependent integrals. we survey the multilevel variance reduction technique introduced by the author in [4] and present extensions and new developments of it. the tools needed for the convergence analysis of vector-valued monte carlo methods are discussed, as well. applications to stochastic solution of integral equations are given for the case where an approximation of the full solution function or a family of functionals of the solution depending on a parameter of a certain dimension is sought.},
author = {giles, m. b.},
doi = {10.1017/s09624929},
file = {:users/roy/documents/knowledge/bibliographie/2001/giles{\_}multilevel monte carlo methods.pdf:pdf},
isbn = {0302-9743},
issn = {0962-4929},
journal = {large-scale scientific computing},
keywords = {complexity},
pages = {58--67},
title = {{multilevel monte carlo methods}},
volume = {2179},
year = {2001}
}
@article{ansaldi2015,
author = {ansaldi, t and ortiz, f granados and airiau, c and lai, c},
file = {:users/roy/documents/knowledge/bibliographie/2015/ansaldi et al.{\_}sensitivity and uncertainty quantification for jet stability analysis.pdf:pdf},
title = {{sensitivity and uncertainty quantification for jet stability analysis}},
year = {2015}
}
@book{goodfellow2016,
author = {goodfellow, ian and bengio, yoshua and courville, aaron},
file = {:users/roy/documents/knowledge/bibliographie/2016/goodfellow, bengio, courville{\_}deep learning.pdf:pdf},
publisher = {mit press},
title = {{deep learning}},
year = {2016}
}
@article{clemson1995,
abstract = {sentitivity analysis is usually difficult because to test exhaustively too many trials are required for the available time, money, or pateince. this article discusses latin hypercube sampling and taguchi methods, both of which are methods for experimental design that allow testing of all combinations of parameter values with a manageable number of trials. the procedure for use of the taguchi methods are presented in some detail.},
author = {clemson, barry and tang, yongming and pyne, james and unal, resit},
doi = {10.1002/sdr.4260110104},
file = {:users/roy/documents/knowledge/bibliographie/1995/clemson et al.{\_}efficient methods for sensitivity analysis.pdf:pdf},
issn = {08837066},
journal = {system dynamics review},
keywords = {class,modeling,nutrient cycling,sensitivity analysis},
number = {1},
pages = {31--49},
title = {{efficient methods for sensitivity analysis}},
volume = {11},
year = {1995}
}
@article{daaelampe2011,
abstract = {abstract in this work, we present a technique based on kernel density estimation for rendering smooth curves. with this approach, we produce uncluttered and expressive pictures, revealing frequency information about one, or, multiple curves, independent of the level of detail in the data, the zoom level, and the screen resolution. with this technique the visual representation scales seamlessly from an exact line drawing, (for low-frequency/low-complexity curves) to a probability density estimate for more intricate situations. this scale-independence facilitates displays based on non-linear time, enabling high-resolution accuracy of recent values, accompanied by long historical series for context. we demonstrate the functionality of this approach in the context of prediction scenarios and in the context of streaming data. [publication abstract]},
author = {{daae lampe}, o. and hauser, h.},
doi = {10.1111/j.1467-8659.2011.01912.x},
file = {:users/roy/documents/knowledge/bibliographie/2011/daae lampe, hauser{\_}curve density estimates.pdf:pdf},
isbn = {01677055},
issn = {01677055},
journal = {computer graphics forum},
keywords = {categories and subject descriptors (according to a,i.3.3 [computer graphics],picture/image generation-line and curve generation},
number = {3},
pages = {633--642},
title = {{curve density estimates}},
volume = {30},
year = {2011}
}
@article{johnson1990,
author = {johnson, m.e. and moore, l.m. and ylvisaker, don},
doi = {10.1016/0378-3758(90)90122-b},
file = {:users/roy/documents/knowledge/bibliographie/1990/johnson, moore, ylvisaker{\_}minimax and maximin distance designs.pdf:pdf},
issn = {03783758},
journal = {journal of statistical planning and inference},
keywords = {bayesian},
month = {oct},
number = {2},
pages = {131--148},
title = {{minimax and maximin distance designs}},
volume = {26},
year = {1990}
}
@article{americaninstituteofaeronauticsandastronauticsaiaa2002,
abstract = {this document presents guidelines for assessing the credibility of modeling and simulation in computational fluid dynamics. the two main principles that are necessary for assessing credibility are verification and validation. verification is the process of determining if a computational simulation accurately represents the conceptual model, but no claim is made of the relationship of the simulation to the real world. validation is the process of determining if a computational simulation represents the real world. this document defines a number of key terms, discusses fundamental concepts, and specifies general procedures for conducting verification and validation of computational fluid dynamics simulations. the document's goal is to provide a foundation for the major issues and concepts in verification and validation. however, this document does not recommend standards in these areas because a number of important issues are not yet resolved. it is hoped that the guidelines will aid in the research, development, and use of computational fluid dynamics simulations by establishing common terminology and methodology for verification and validation. the terminology and methodology should also be useful in other engineering and science disciplines.},
author = {{american institute of aeronautics and astronautics (aiaa)}},
file = {:users/roy/documents/knowledge/bibliographie/2002/american institute of aeronautics and astronautics (aiaa){\_}guide for the verification and validation of computational fluid dynamics simu.pdf:pdf},
number = {g-077-1998(2002)},
pages = {19},
title = {{guide for the verification and validation of computational fluid dynamics simulations}},
volume = {1998},
year = {2002}
}
@incollection{maitre2016a,
address = {cham},
author = {maı̂tre, olivier p. le and knio, omar m},
booktitle = {handbook of uncertainty quantification},
doi = {10.1007/978-3-319-11259-6_18-1},
editor = {ghanem, roger and higdon, david and owhadi, houman},
file = {:users/roy/documents/knowledge/bibliographie/2015/maı̂tre, knio{\_}multiresolution analysis for uncertainty quantification.pdf:pdf},
isbn = {978-3-319-11259-6},
pages = {1--36},
publisher = {springer international publishing},
title = {{multiresolution analysis for uncertainty quantification}},
year = {2015}
}
@article{belia2005,
abstract = {little is known about researchers' understanding of confidence intervals (cis) and standard error (se) bars. authors of journal articles in psychology, behavioral neuroscience, and medicine were invited to visit a web site where they adjusted a figure until they judged 2 means, with error bars, to be just statistically significantly different (p ? .05). results from 473 respondents suggest that many leading researchers have severe misconceptions about how error bars relate to statistical significance, do not adequately distinguish cis and se bars, and do not appreciate the importance of whether the 2 means are independent or come from a repeated measures design. better guidelines for researchers and less ambiguous graphical con- ventions are needed before the advantages of cis for research communication can be realized.},
author = {Belia, Sarah and Fidler, Fiona and Williams, Jennifer and Cumming, Geoff},
doi = {10.1037/1082-989x.10.4.389},
file = {:users/roy/documents/knowledge/bibliographie/2005/belia et al.{\_}researchers misunderstand confidence intervals and standard error bars.pdf:pdf},
journal = {Psychological Methods},
keywords = {confidence intervals,error bars,standard error,statistical,statistical cognition},
number = {4},
pages = {389--396},
pmid = {16392994},
title = {{Researchers misunderstand confidence intervals and standard error bars}},
volume = {10},
year = {2005}
}
@article{sirovich1987,
abstract = {the author reviews and further develops the karhunen-loeve procedure within the context of fluid mechanics. he then applies this to the point of obtaining practical methods for the determination of coherent structures of a turbulent flow},
author = {sirovich, lawrence},
file = {:users/roy/documents/knowledge/bibliographie/1987/sirovich{\_}turbulence and the dynamics of coherent structures part i coherent structures.pdf:pdf},
journal = {quarterly of applied mathematics},
number = {3},
pages = {561--571},
title = {{turbulence and the dynamics of coherent structures part i: coherent structures}},
volume = {xlv},
year = {1987}
}
@book{james2013,
abstract = {we study a family of "classical" orthogonal polynomials which satisfy (apart from a 3-term recurrence relation) an eigenvalue problem with a differential operator of dunkl-type. these polynomials can be obtained from the little {\$}q{\$}-jacobi polynomials in the limit {\$}q=-1{\$}. we also show that these polynomials provide a nontrivial realization of the askey-wilson algebra for {\$}q=-1{\$}.},
address = {new york, ny},
archiveprefix = {arxiv},
arxivid = {1011.1669},
author = {james, gareth and witten, daniela and hastie, trevor and tibshirani, robert},
booktitle = {performance evaluation},
doi = {10.1007/978-1-4614-7138-7},
eprint = {1011.1669},
file = {:users/roy/documents/knowledge/bibliographie/2013/james et al.{\_}an introduction to statistical learning.pdf:pdf},
isbn = {978-1-4614-7137-0},
keywords = {bittorrent,p2p file sharing,performance modeling},
month = {nov},
number = {9-12},
pages = {856--875},
publisher = {springer new york},
series = {springer texts in statistics},
title = {{an introduction to statistical learning}},
volume = {103},
year = {2013}
}
@article{wang2016a,
abstract = {turbulence modeling is a critical component in numerical simulations of industrial flows based on reynolds-averaged navier-stokes (rans) equations. however, after decades of efforts in the turbulence modeling community, universally applicable rans models with predictive capabilities are still lacking. recently, data-driven methods have been proposed as a promising alternative to the traditional approaches of turbulence model development. in this work we propose a data-driven, physics-informed machine learning approach for predicting discrepancies in rans modeled reynolds stresses. the discrepancies are formulated as functions of the mean flow features. by using a modern machine learning technique based on random forests, the discrepancy functions are first trained with benchmark flow data and then used to predict reynolds stresses discrepancies in new flows. the method is used to predict the reynolds stresses in the flow over periodic hills by using two training flow scenarios of increasing difficulties: (1) the flow in the same periodic hills geometry yet at a lower reynolds number, and (2) the flow in a different hill geometry with a similar recirculation zone. excellent predictive performances were observed in both scenarios, demonstrating the merits of the proposed method. improvement of rans modeled reynolds stresses enabled by the proposed method is an important step towards predictive turbulence modeling, where the ultimate goal is to predict the quantities of interest (e.g., velocity field, drag, lift) more accurately by solving rans equations with the reynolds stresses obtained therefrom.},
archiveprefix = {arxiv},
arxivid = {1606.07987},
author = {wang, jian-xun and wu, jin-long and xiao, heng},
eprint = {1606.07987},
file = {:users/roy/documents/knowledge/bibliographie/2016/wang, wu, xiao{\_}physics-informed machine learning for predictive turbulence modeling using data to improve rans modeled reynolds stresses.pdf:pdf},
journal = {arxiv.org},
keywords = {boussinesq assumption,data-driven modeling,equations,machine learning,model-form uncertainty,reynolds-averaged navier,stokes,turbulence modeling},
pages = {1--41},
title = {{physics-informed machine learning for predictive turbulence modeling: using data to improve rans modeled reynolds stresses}},
year = {2016}
}
@article{iooss2013a,
author = {iooss, bertrand},
file = {:users/roy/documents/knowledge/bibliographie/2013/iooss{\_}planification d'exp{\'{e}}riences num{\'{e}}riques.pdf:pdf},
title = {{planification d'exp{\'{e}}riences num{\'{e}}riques}},
year = {2013}
}
@article{lappo2009,
author = {lappo, v and habashi, w},
file = {:users/roy/documents/knowledge/bibliographie/2009/lappo, habashi{\_}reduced order podkriging modeling for real-time 3d cfd.pdf:pdf},
keywords = {flight simulators,in-flight icing,proper orthogonal decomposition,real-time cfd,reduced order models},
number = {mvd},
title = {{reduced order pod/kriging modeling for real-time 3d cfd}},
year = {2009}
}
@article{allard2011,
author = {allard, alexandre and fischer, nicolas and didieux, franck and guillaume, eric and iooss, bertrand},
file = {:users/roy/documents/knowledge/bibliographie/2011/allard et al.{\_}evaluation of the most influent input variables on quantities of interest in a fire simulation.pdf:pdf},
journal = {journal de la soci{\'{e}}t{\'{e}} fran{\c{c}}aise de statistique},
keywords = {60k35,ams 2000 subject classifications,analyse de sensibilit{\'{e}},fire engineering,incertitude de mesure,indices,indices de sobol,ing{\'{e}}nierie du feu,local polynomial smoother,measurement uncertainty,mots-cl{\'{e}}s,polyn{\^{o}}mes locaux,sensitivity analysis,sobol},
number = {1},
pages = {103--117},
title = {{evaluation of the most influent input variables on quantities of interest in a fire simulation}},
volume = {152},
year = {2011}
}

@article{jones2001,
abstract = {this paper presents a taxonomy of existing approaches for using response surfaces for global optimization. each method is illustrated with a simple numerical example that brings out its advantages and disadvantages. the central theme is that methods that seem quite reasonable often have non-obvious failure modes. understanding these failure modes is essential for the development of practical algorithms that fulfill the intuitive promise of the response surface approach.},
author = {jones, donald r.},
doi = {10.1023/a:1012771025575},
file = {:users/roy/documents/knowledge/bibliographie/2001/jones{\_}a taxonomy of global optimization methods based on response surfaces.pdf:pdf},
isbn = {0925-5001},
issn = {09255001},
journal = {journal of global optimization},
keywords = {global optimization,kriging,response surface,splines},
number = {4},
pages = {345--383},
title = {{a taxonomy of global optimization methods based on response surfaces}},
volume = {21},
year = {2001}
}
@article{conference2011,
author = {conference, international and display, auditory},
file = {:users/roy/documents/knowledge/bibliographie/2011/conference, display{\_}using sound to represent uncertainty in future climate projections for the united kingdom nick bearman school of env.pdf:pdf},
journal = {methodology},
title = {{using sound to represent uncertainty in future climate projections for the united kingdom nick bearman school of environmental sciences , university of east anglia , norwich , nr4 7tj , united kingdom}},
year = {2011}
}
@article{viana2014,
abstract = {the use of metamodeling techniques in the design and analysis of computer experiments has progressed remarkably in the past 25 years, but how far has the field really come? this is the question addressed in this paper, namely, the extent to which the use of metamodeling techniques in multidisciplinary design optimization have evolved in the 25 years since the seminal paper on design and analysis of computer experiments by sacks et al. (“design and analysis of computer experiments,” statistical science, vol. 4, no. 4, 1989, pp. 409–435). rather than a technical review of the entire body of metamodeling literature, the focus is on the evolution and motivation for advancements in metamodeling with some discussion on the research itself; not surprisingly, much of the current research motivation is the same as it was in the past. based on current research thrusts in the field, multifidelity approximations and ensembles (i.e., sets) of metamodels, as well as the availability of metamodels within commercial software, are emphasized. design space exploration and visualization via metamodels are also presented as they rely heavily on metamodels for rapid design evaluations during exploration. the closing remarks offer insight into future research directions, mostly motivated by the need for new capabilities and the ability to handle more complex simulations.},
author = {viana, felipe a. c. and simpson, timothy w. and balabanov, vladimir and toropov, vasilli},
doi = {10.2514/1.j052375},
file = {:users/roy/documents/knowledge/bibliographie/2014/viana et al.{\_}special section on multidisciplinary design optimization metamodeling in multidisciplinary design optimization how far have.pdf:pdf},
isbn = {0001-1452},
issn = {0001-1452},
journal = {aiaa journal},
number = {4},
pages = {670--690},
title = {{special section on multidisciplinary design optimization: metamodeling in multidisciplinary design optimization: how far have we really come?}},
volume = {52},
year = {2014}
}
@incollection{lee2002,
abstract = {the goal of maximum entropy sampling is to choose a most informative subset of s random variables from a set of n random variables, subject to side con-straints. a typical side constraint might be a budget restriction, where we have a cost for observing each random variable. other possibilities include logical constraints (e.g. multiple choice or precedence con-straints). in many situations, we can assume that the random variables are gaussian, or that they can be suitably transformed. we briefly set our notation. we assume that we have n gaussian random variables y j , j 2 n d f1, 2, . . . , ng 1 our goal is to choose the 'most informative' subset s from n, having s elements, possibly subject to additional constraints j2s a ij {\"{a}} b i , i 2 m d f1, 2, . . . , mg 2 we let y s denote the set of random variables indexed by s, and we let f s denote the joint density function of y s . our measure of information, which we seek to maximize, is the boltzmann–shannon entropy h n d deeln f s y s we assume that the random variables have a joint gaussian distribution, and we let c denote the covari-ance matrix for y n . then, letting c[s, t] denote the submatrix of c with rows indexed by s and columns indexed by t, we have that c[s, s] is the covari-ance matrix of y s . it turns out that in this gaussian case, the entropy h n is just an increasing linear function of h n d ln det c[s, s] 4},
author = {lee, jon},
booktitle = {encyclopedia of environmetrics},
file = {:users/roy/documents/knowledge/bibliographie/2002/lee{\_}maximum entropy sampling.pdf:pdf},
pages = {1229--1234},
title = {{maximum entropy sampling}},
volume = {3},
year = {2002}
}
@article{congedo2013,
abstract = {numerous comparisons between reynolds-averaged navier–stokes (rans) and large-eddy simulation (les) modeling have already been performed for a large variety of turbulent flows in the context of fully deterministic flows, that is, with fixed flow and model parameters. more recently, rans and les have been separately assessed in conjunction with stochastic flowand/or model parameters. the present paper performs a comparison of the rans k ? ? model and the les dynamic smagorinsky model for turbulent flow in a pipe geometry subject to uncertain inflow conditions. the influence of the experimental uncertainties on the computed flow is analyzed using a non-intrusive polynomial chaos approach for two flow configurations (with or without swirl). measured quantities including an estimation of the measurement error are then compared with the statistical representation (mean value and variance) of their rans and les numerical approximations in order to check whether experiment/simulation discrepancies can be explained within the uncertainty inherent to the studied configuration. the statistics of the rans prediction are found in poor agreement with experimental results when the flow is characterized by a strong swirl, whereas the computationally more expensive les prediction remains statistically well inside the measurement intervals for the key flow quantities.},
author = {congedo, pietro marco and duprat, cedric and balarac, guillaume and corre, christophe},
doi = {10.1002/fld.3743},
file = {:users/roy/documents/knowledge/bibliographie/2013/congedo et al.{\_}numerical prediction of turbulent flows using reynolds-averaged navier-stokes and large-eddy simulation with uncertain in.pdf:pdf},
issn = {02712091},
journal = {international journal for numerical methods in fluids},
keywords = {turbulence modeling,uncertainties quantification},
month = {may},
number = {3},
pages = {341--358},
title = {{numerical prediction of turbulent flows using reynolds-averaged navier-stokes and large-eddy simulation with uncertain inflow conditions}},
volume = {72},
year = {2013}
}
@article{draper1995a,
author = {draper, david},
file = {:users/roy/documents/knowledge/bibliographie/1995/draper{\_}assessment and propagation of model uncertainty.pdf:pdf},
isbn = {00359246},
journal = {journal of the royal statistical society b},
keywords = {bayes factors,calibration,forecastino,hierarchical models,inference,model specification,overfitting,prediction,robustness,sensitivity analysis,uncertainty assessment},
number = {1},
pages = {45--97},
title = {{assessment and propagation of model uncertainty}},
volume = {57},
year = {1995}
}

@article{damblin2013,
abstract = {quantitative assessment of the uncertainties tainting the results of computer simulations is nowadays a major topic of interest in both industrial and scientific communities. one of the key issues in such studies is to get information about the output when the numerical simulations are expensive to run. this paper considers the problem of exploring the whole space of variations of the computer model input variables in the context of a large dimensional exploration space. various properties of space filling designs are justified: interpoint-distance, discrepancy, mini- mum spanning tree criteria. a specific class of design, the optimized latin hypercube sample, is considered. several optimization algorithms, coming from the literature, are studied in terms of convergence speed, robustness to subprojection and space filling properties of the resulting de- sign. some recommendations for building such designs are given. finally, another contribution of this paper is the deep analysis of the space filling properties of the design 2d-subprojections.},
annote = {tells that the star discrepancy is not good with d{\textgreater}10. a better approach is to use the centered or wrap around discrepancy. best of all seems to be c2},
author = {damblin, guillaume and couplet, mathieu and iooss, bertrand and damblin, guillaume and couplet, mathieu and iooss, bertrand},
file = {:users/roy/documents/knowledge/bibliographie/2013/damblin et al.{\_}numerical studies of space filling designs optimization of latin hypercube samples and subprojection properties.pdf:pdf},
journal = {journal of simulation},
keywords = {latin hypercube sampling,computer experiment,discrepancy,optimal design},
pages = {276--289},
title = {{numerical studies of space filling designs : optimization of latin hypercube samples and subprojection properties}},
year = {2013}
}
@article{fisher1919,
abstract = {several attempts have already been made to interpret the well-established results of biometry in accordance with the mendelian scheme of inheritance. it is here attempted to ascertain the biometrical properties of a population of a more general type than has hitherto been examined, inheritance in which follows this scheme. it is hoped that in this way it will be possible to make a more exact analysis of the causes of human variability. the great body of available statistics show us that the deviations of a human measurement from its mean follow very closely the normal law of errors, and, therefore, that the variability may be uniformly measured by the standard deviation corresponding to the square root of the mean square error. when there are two independent causes of variability capable of producing in an otherwise uniform population distributions with standard deviations $\sigma$1 and $\sigma$2, it is found that the distribution, when both causes act together, has a standard deviation . it is therefore desirable in analysing the causes of variability to deal with the square of the standard deviation as the measure of variability. we shall term this quantity the variance of the normal population to which it refers, and we may now ascribe to the constituent causes fractions or percentages of the total variance which they together produce. it is desirable on the one hand that the elementary ideas at the basis of the calculus of correlations should be clearly understood, and easily expressed in ordinary language, and on the other that loose phrases about the “percentage of causation,” which obscure the essential distinction between the individual and the population, should be carefully avoided.},
author = {fisher, r a},
doi = {doi:10.1017/s0080456800012163},
file = {:users/roy/documents/knowledge/bibliographie/1919/fisher{\_}xv.--the correlation between relatives on the supposition of mendelian inheritance.pdf:pdf},
isbn = {1755-6910},
issn = {0080-4568},
journal = {transactions of the royal society of edinburgh},
number = {july 2012},
pages = {399--433},
pmid = {12019254},
title = {{xv.--the correlation between relatives on the supposition of mendelian inheritance.}},
volume = {52},
year = {1919}
}
@book{walpole2012,
author = {walpole, ronald e and myers, raymond h and myers, sharon l and ye, keying},
file = {:users/roy/documents/knowledge/bibliographie/2012/walpole et al.{\_}probability and statistics for engineers and scientists.pdf:pdf},
isbn = {0132047675},
pages = {816},
title = {{probability and statistics for engineers and scientists}},
volume = {9th},
year = {2012}
}
@book{miettinen1999,
author = {miettinen, kaisa m},
file = {:users/roy/documents/knowledge/bibliographie/1999/miettinen{\_}nonlinear multiobjective optimization.pdf:pdf},
publisher = {kluwer academic},
title = {{nonlinear multiobjective optimization}},
year = {1999}
}
@inproceedings{zhu2016,
abstract = {the durability of highly loaded gas turbine blade is significantly impacted by high heat transfer. the heat transfer performance of the gas turbine endwall can be varied significantly due to the impacts of uncertainties in the manufacturing process and operation conditions. in this work, an uncertainty quantification (uq) method is proposed by integrating generalized polynomial chaos expansions, non- intrusive spectral projection and smolyak sparse grids. then coupled with three dimensional (3d) reynolds-averaged navier-stokes (rans) solutions, an uncertainty quantification procedure is carried out for heat transfer performance of highly loaded blade endwall. wherein, the effects of the variation of geometric parameters and operation conditions are taken into account. specifically, the endwall heat transfer performance of a typical highly loaded turbine blade named pack-b is numerically investigated. the turbulence intensity tu and inlet reynolds number reinlet of inlet flow are considered as flow condition uncertainty parameters. as geometrical uncertainty parameters, the radius r and minimum angle ? of blade root fillet are considered. these uncertainty factors have important influence on the secondary flow structure, resulting in significant variation of heat transfer performance of the endwall. non-intrusive polynomial chaos (nipc) is used to build a surrogate to reduce the quantity of the time consuming cfd simulation. a total of 137 sparse-grid-based design-of- experiment computations were carried out to build the high- fidelity surrogate. using above method, the probability density function of the nusselt number ( nu ) of the endwall is obtained. the overall variation of nu can be more than 10{\%} due to the effect of the uncertainty factors. finally, the sensitivity analysis shows that re inlet has the most important influence on heat transfer performance of the whole endwall and other uncertainty factors also have significant effect on heat transfer performance at some local regions of the endwall such as wake region and middle part of blade passage.},
address = {seoul},
author = {zhu, peiyuan and yan, yong and song, liming and li, jun and feng, zhenping},
booktitle = {turbomachinery technical conference and exposition gt2016},
file = {:users/roy/documents/knowledge/bibliographie/2016/zhu et al.{\_}uncertainty quantification of heat transfer for a highly loaded gas turbine blade endwall using polynomial chaos.pdf:pdf},
pages = {1--13},
title = {{uncertainty quantification of heat transfer for a highly loaded gas turbine blade endwall using polynomial chaos}},
year = {2016}
}
@article{iooss2017b,
abstract = {complex computer codes are often too time expensive to be directly used to perform uncertainty, sensitivity, optimization and robustness analyses. a widely accepted method to circumvent this problem consists in replacing cpu-time expensive computer models by cpu inexpensive mathematical functions, called metamodels. for example, the gaussian process (gp) model has shown strong capabilities to solve practical problems , often involving several interlinked issues. however, in case of high dimensional experiments (with typically several tens of inputs), the gp metamodel building process remains difficult, even unfeasible, and application of variable selection techniques cannot be avoided. in this paper, we present a general methodology allowing to build a gp metamodel with large number of inputs in a very efficient manner. while our work focused on the gp metamodel, its principles are fully generic and can be applied to any types of metamodel. the objective is twofold: estimating from a minimal number of computer experiments a highly predictive metamodel. this methodology is successfully applied on an industrial computer code.},
archiveprefix = {arxiv},
arxivid = {1704.07090},
author = {iooss, bertrand and marrel, amandine},
eprint = {1704.07090},
file = {:users/roy/documents/knowledge/bibliographie/2017/iooss, marrel{\_}an efficient methodology for the analysis and modeling of computer experiments with large number of inputs.pdf:pdf},
keywords = {analysis,computer experiments,gaussian process,metamodel,sensitivity,uncertainty quantification},
month = {apr},
pages = {1--12},
title = {{an efficient methodology for the analysis and modeling of computer experiments with large number of inputs}},
year = {2017}
}
@article{tissot2012,
abstract = {this paper deals with the random balance design method (rbd) and its hybrid approach, rbd-fast. both these global sensitivity analysis methods originate from fourier amplitude sensitivity test (fast) and consequently face the main problems inherent to discrete harmonic analysis. we present here a general way to correct a bias which occurs when estimating sensitivity indices (sis) of any order - except total si of single factor or group of factors - by the random balance design method (rbd) and its hybrid version, rbd-fast. in the rbd case, this positive bias has been recently identified in a paper by xu and gertner [1]. following their work, we propose a bias correction method for first-order sis estimates in rbd. we then extend the correction method to the sis of any order in rbd-fast. at last, we suggest an efficient strategy to estimate all the first- and second-order sis using rbd-fast. {\textcopyright} 2012 elsevier ltd.},
author = {tissot, jean yves and prieur, cl{\'{e}}mentine},
doi = {10.1016/j.ress.2012.06.010},
file = {:users/roy/documents/knowledge/bibliographie/2012/tissot, prieur{\_}bias correction for the estimation of sensitivity indices based on random balance designs.pdf:pdf},
issn = {09518320},
journal = {reliability engineering and system safety},
keywords = {bias correction,global sensitivity analysis,rbd-fast,random balance design,sensitivity indices},
pages = {205--213},
title = {{bias correction for the estimation of sensitivity indices based on random balance designs}},
volume = {107},
year = {2012}
}
@misc{iooss2013,
author = {iooss, bertrand},
file = {:users/roy/documents/knowledge/bibliographie/2013/iooss{\_}algorithmes d 'optimisation et propri{\'{e}}t{\'{e}}s en sous-projections des plans « space filling ».pdf:pdf},
title = {{algorithmes d 'optimisation et propri{\'{e}}t{\'{e}}s en sous-projections des plans « space filling »}},
year = {2013}
}
@article{campolongo2011,
author = {campolongo, francesca and saltelli, andrea and cariboni, jessica},
doi = {10.1016/j.cpc.2010.12.039},
file = {:users/roy/documents/knowledge/bibliographie/2011/campolongo, saltelli, cariboni{\_}from screening to quantitative sensitivity analysis . a unified approach.pdf:pdf},
issn = {0010-4655},
journal = {computer physics communications},
keywords = {sensitivity analysis},
number = {4},
pages = {978--988},
publisher = {elsevier b.v.},
title = {{from screening to quantitative sensitivity analysis . a unified approach}},
volume = {182},
year = {2011}
}
@article{dinesha2012,
abstract = {in this paper, we explore a novel idea of using high dynamic range (hdr) technology for uncertainty visualization. we focus on scalar volumetric data sets where every data point is associated with scalar uncertainty. we design a transfer function that maps each data point to a color in hdr space. the luminance component of the color is exploited to capture uncertainty. we modify existing tone mapping techniques and suitably integrate them with volume ray casting to obtain a low dynamic range (ldr) image. the resulting image is displayed on a conventional 8-bits-per-channel display device. the usage of hdr mapping reveals fine details in uncertainty distribution and enables the users to interactively study the data in the context of corresponding uncertainty information. we demonstrate the utility of our method and evaluate the results using data sets from ocean modeling.},
author = {dinesha, vijeth and adabala, neeharika and natarajan, vijay},
doi = {10.1007/s00371-011-0614-7},
file = {:users/roy/documents/knowledge/bibliographie/2012/dinesha, adabala, natarajan{\_}uncertainty visualization using hdr volume rendering.pdf:pdf},
isbn = {0178-2789},
issn = {01782789},
journal = {visual computer},
keywords = {high dynamic range imaging,ray casting,tone mapping,transfer function design,uncertainty visualization,volume rendering},
number = {3},
pages = {265--278},
title = {{uncertainty visualization using hdr volume rendering}},
volume = {28},
year = {2012}
}
@misc{wan,
author = {wan, eric a and merwe, rudolph van der and rd, n w walker},
file = {:users/roy/documents/knowledge/bibliographie/unknown/wan, merwe, rd{\_}the unscented kalman filter for nonlinear estimation.pdf:pdf},
title = {{the unscented kalman filter for nonlinear estimation}}
}
@article{khalil2015,
abstract = {we address the forward uncertainty quantification problem in large eddy simulation (les) of a turbu- lent non-premixed hydrocarbon flame, focusing on parametric uncertainty. more specifically, we examine the effect of uncertainty in the smagorinsky coefficient and turbulent prandtl and schmidt numbers on spe- cific quantities of interest. to conduct this analysis 25 les simulations are performed, from which a sur- rogate model is built, based on polynomial chaos expansion, for the quantities of interest. this enables global sensitivity analysis and forward propagation of uncertainty, providing marginal and joint distribu- tions on the quantities of interest. a non-intrusive method is used to construct the surrogate models, avoid- ing the need to modify the deterministic les forward solver. the accuracy of the surrogates is examined using global error measures. the results provide insights into the underlying structure of the les simula- tion, the impact of varying parameters on specific observables, and correlations among different quantities of interest.},
author = {khalil, mohammad and lacaze, guilhem and oefelein, joseph c. and najm, habib n.},
doi = {10.1016/j.proci.2014.05.030},
file = {:users/roy/documents/knowledge/bibliographie/2014/khalil et al.{\_}uncertainty quantification in les of a turbulent bluff-body stabilized flame.pdf:pdf},
issn = {15407489},
journal = {proceedings of the combustion institute},
keywords = {global sensitivity analysis,large eddy simulation,model surrogate,turbulent combustion,uncertainty quantification},
number = {2},
pages = {1147--1156},
title = {{uncertainty quantification in les of a turbulent bluff-body stabilized flame}},
volume = {35},
year = {2014}
}


@article{colladomorata2012,
abstract = {recent developments and demonstrations for the prediction of turbulent flows around blades point to large eddy simulations (les) as a very promising tool. indeed and despite the fact that this numerical method still requires modeling and intense computing effort compared to reynolds average navier-stokes (rans), this fully unsteady simulation technique provides valuable information on the turbulent flow otherwise inaccessible. theoretical limits and scales of wall bounded flows are now well mastered in simple cases but complex industrial applications usually introduce unknowns and mechanisms that are difficult to apprehend beforehand especially with les which is usually computationally intensive and bounded to code scalability, mesh quality, modeling performances and computer power. in this specific context, few studies directly address the use of fully structured versus unstructured, implicit versus explicit flow solvers and their respective impact for les modeling of complex wall bounded flows. to partly address these important issues, two dedicated structured and unstructured computational solvers are applied and assessed by comparing the predictions of the heat transfer around the experimental high pressure turbine blade profile cascade of arts et al. [6]. first, both les predictions are compared to rans modeling with a particular interest for the accuracy/cost ratio and improvement of the physical phenomena around the blade. les's are then detailed and further investigated to assess their ability to reproduce the inlet turbulence effect on heat transfer and the development of the transitioning boundary layer around the blade. quantitative comparisons against experimental findings show excellent agreement especially on the pressure side of the profile. detailed analysis of the flow predictions provided by both the structured and unstructured solvers underline the importance of long stream-wise streaky structures responsible for the augmentation of the heat transfer and leading to the transition of the suction-side boundary layer. ?? 2012 elsevier ltd. all rights reserved.},
author = {{collado morata}, e. and gourdain, n. and duchaine, f. and gicquel, l.y.m.},
doi = {10.1016/j.ijheatmasstransfer.2012.05.072},
file = {:users/roy/documents/knowledge/bibliographie/2012/collado morata et al.{\_}effects of free-stream turbulence on high pressure turbine blade heat transfer predicted by structured and unstruc.pdf:pdf},
isbn = {0017-9310},
issn = {00179310},
journal = {international journal of heat and mass transfer},
keywords = {boundary layer transition,heat transfer,les,turbine guide vane,unsteady flow},
month = {oct},
number = {21-22},
pages = {5754--5768},
publisher = {elsevier ltd},
title = {{effects of free-stream turbulence on high pressure turbine blade heat transfer predicted by structured and unstructured les}},
volume = {55},
year = {2012}
}
@article{villemonteix2009a,
abstract = {in many global optimization problems motivated by engineering applications,$\backslash$nthe number of function evaluations is severely limited by time or$\backslash$ncost. to ensure that each of these evaluations usefully contributes$\backslash$nto the localization of good candidates for the role of global minimizer,$\backslash$na stochastic model of the function can be built to conduct a sequential$\backslash$nchoice of evaluation points. based on gaussian processes and kriging,$\backslash$nthe authors have recently introduced the informational approach to$\backslash$nglobal optimization (iago) which provides a one-step optimal choice$\backslash$nof evaluation points in terms of reduction of uncertainty on the$\backslash$nlocation of the minimizers. to do so, the probability density of$\backslash$nthe minimizers is approximated using conditional simulations of the$\backslash$ngaussian process model behind kriging. in this paper, an empirical$\backslash$ncomparison between the underlying sampling criterion called conditional$\backslash$nminimizer entropy (cme) and the standard expected improvement sampling$\backslash$ncriterion (ei) is presented. classical test functions are used as$\backslash$nwell as sample paths of the gaussian model and an industrial application.$\backslash$nthey show the interest of the cme sampling criterion in terms of$\backslash$nevaluation savings.},
author = {villemonteix, julien and vazquez, emmanuel and sidorkiewicz, maryan and walter, eric},
doi = {10.1007/s10898-008-9313-y},
file = {:users/roy/documents/knowledge/bibliographie/2009/villemonteix et al.{\_}global optimization of expensive-to-evaluate functions an empirical comparison of two sampling criteria.pdf:pdf},
issn = {0925-5001},
journal = {journal of global optimization},
keywords = {expected improvement,global optimization,kriging},
month = {mar},
number = {2-3},
pages = {373--389},
title = {{global optimization of expensive-to-evaluate functions: an empirical comparison of two sampling criteria}},
volume = {43},
year = {2009}
}
@techreport{altman2002,
author = {altman, christopher},
file = {:users/roy/documents/knowledge/bibliographie/2002/altman{\_}quantum uncertainty.pdf:pdf},
number = {december},
title = {{quantum uncertainty}},
year = {2002}
}
@article{najm2009,
abstract = {the quantification of uncertainty in computational fluid dynamics (cfd) predictions is both a significant challenge and an important goal. probabilistic uncertainty quantification (uq) methods have been used to propagate uncertainty from model inputs to outputs when input uncertainties are large and have been characterized probabilistically. polynomial chaos (pc) methods have found increased use in probabilistic uq over the past decade. this review describes the use of pc expansions for the representation of random variables/fields and discusses their utility for the propagation of uncertainty in computational models, focusing on cfd models. many cfd applications arc considered, including flow in porous media, incompressible and compressible flows, and thermofluid and reacting flows. the review examines each application area, focusing on the demonstrated use of pc uq and the associated challenges. cross-cutting challenges with time unsteadiness and long time horizons are also discussed.},
author = {najm, habib n},
doi = {10.1146/annurev.fluid.010908.165248},
file = {:users/roy/documents/knowledge/bibliographie/2009/najm{\_}uncertainty quantification and polynomial chaos techniques in computational fluid dynamics.pdf:pdf},
isbn = {0066-4189$\backslash$r1545-4479},
issn = {0066-4189},
journal = {annual review of fluid mechanics},
keywords = {cfd,pc,polynomial chaos,uq},
month = {jan},
number = {1},
pages = {35--52},
title = {{uncertainty quantification and polynomial chaos techniques in computational fluid dynamics}},
volume = {41},
year = {2009}
}
@article{baudoui2012,
abstract = {this phd thesis deals with the optimization under uncertainty of expensive functions in the context of aeronautical systems design. first, we develop a multiobjective robust optimization strategy based on surrogate models. beyond providing a faster representation of the initial functions, these models facilitate the computation of the solutions' robustness with respect to the problem uncertainties. the modeling error is controlled through a new design of experiments enrichment approach that allows improving several models concurrently in the possibly optimal regions of the search space. this strategy is applied to the pollutant emission minimization of a turbomachine combustion chamber whose injectors can clog unpredictably. we subsequently present a heuristic method dedicated to multidisciplinary robust optimization. it relies on local robustness management within disciplines exposed to uncertain parameters, in order to avoid the implementation of a full uncertainty propagation through the system. an applicability criterion is proposed to check the validity of this approach a posteriori using data collected during the optimization. this methodology is applied to an aircraft design case where the surface of the vertical tail is not known accurately},
author = {baudoui, vincent},
file = {:users/roy/documents/knowledge/bibliographie/2012/baudoui{\_}optimisation robuste multiobjectifs par mod{\`{e}}les de substitution.pdf:pdf},
keywords = {aircraft design,combustion,design of experiments,multidisciplinary optimization,multiobjective,optimization,robust optimization,surrogate models,uncertainty},
title = {{optimisation robuste multiobjectifs par mod{\`{e}}les de substitution}},
year = {2012}
}
@techreport{iaccarino,
author = {iaccarino, gianluca},
file = {:users/roy/documents/knowledge/bibliographie/2012/iaccarino{\_}introduction to uncertainty quantification.pdf:pdf},
title = {{introduction to uncertainty quantification}},
year = {2012}
}
@article{kim2017,
abstract = {information visualizations use interactivity to enable user-driven querying of visualized data. however, users' interac-tions with their internal representations, including their ex-pectations about data, are also critical for a visualization to support learning. we present multiple graphically-based tech-niques for eliciting and incorporating a user's prior knowledge about data into visualization interaction. we use controlled experiments to evaluate how graphically eliciting forms of prior knowledge and presenting feedback on the gap between prior knowledge and the observed data impacts a user's ability to recall and understand the data. we find that participants who are prompted to reflect on their prior knowledge by pre-dicting and self-explaining data outperform a control group in recall and comprehension. these effects persist when partici-pants have moderate or little prior knowledge on the datasets. we discuss how the effects differ based on text versus visual presentations of data. we characterize the design space of graphical prediction and feedback techniques and describe design recommendations.},
author = {kim, yea-seul and reinecke, katharina and hullman, jessica},
doi = {10.1145/3025453.3025592},
file = {:users/roy/documents/knowledge/bibliographie/2017/kim, reinecke, hullman{\_}explaining the gap visualizing one's predictions improves recall and comprehension of data.pdf:pdf},
isbn = {9781450346559},
journal = {proceedings of the 2017 chi conference on human factors in computing systems - chi '17},
keywords = {author keywords information visualization,internal representations of data,prediction,self-explanation},
pages = {1375--1386},
title = {{explaining the gap: visualizing one's predictions improves recall and comprehension of data}},
year = {2017}
}
@incollection{zhang2016,
abstract = {model verification and validation (v{\&}v) are essential before a model can be implemented in practice. integrating model v{\&}v into the process of model development can help reduce the risk of errors, enhance the accuracy of the model, and strengthen the confidence of the decision-maker in model results. besides v{\&}v, uncertainty quantification (uq) techniques are used to verify and validate computationalmodels.modeling intelligent adversaries is different from and more difficult than modeling non-intelligent agents.however, modeling intelligent adversaries is critical to infrastructure protection and national security. model v{\&}v and uq for intelligent adversaries present a big challenge. this chapter first reviews the concepts of model v{\&}v and uq in the literature and then discusses model v{\&}v and uq for intelligent adversaries. some v{\&}v techniques for modeling intelligent adversaries are provided which could be beneficial to model developers and decision-makers facing with intelligent adversaries.},
author = {zhang, jing and zhuang, jun},
booktitle = {handbook of uncertainty quantification},
doi = {10.1007/978-3-319-11259-6_44-1},
file = {:users/roy/documents/knowledge/bibliographie/2015/zhang, zhuang{\_}validation, verification, and uncertainty quantification for models with intelligent adversaries.pdf:pdf},
isbn = {978-3-319-12384-4},
keywords = {decision making,intelligent adversaries,model validation and verification,validation techniques},
pages = {1--19},
publisher = {springer international publishing},
title = {{validation, verification, and uncertainty quantification for models with intelligent adversaries}},
year = {2015}
}
@article{karpathy2014a,
abstract = {we present a model that generates natural language descriptions of images and their regions. our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. our alignment model is based on a novel combination of convolutional neural networks over image regions, bidirectional recurrent neural networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. we then describe a multimodal recurrent neural network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. we demonstrate that our alignment model produces state of the art results in retrieval experiments on flickr8k, flickr30k and mscoco datasets. we then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.},
archiveprefix = {arxiv},
arxivid = {1412.2306},
author = {karpathy, andrej and fei-fei, li},
eprint = {1412.2306},
file = {:users/roy/documents/knowledge/bibliographie/2014/karpathy, fei-fei{\_}deep visual-semantic alignments for generating image descriptions.pdf:pdf},
month = {dec},
title = {{deep visual-semantic alignments for generating image descriptions}},
year = {2014}
}
@article{vlad2010,
abstract = {the paper discusses the main ideas of the chaos theory and presents mainly the importance of the nonlinearities in the mathematical models. chaos and order are apparently two opposite terms. the fact that in chaos can be found a certain precise symmetry (feigenbaum numbers) is even more surprising. as an illustration of the ubiquity of chaos, three models among many other existing models that have chaotic features are presented here: the nonlinear feedback profit model, one model for the simulation of the exchange rate and one application of the chaos theory in the capital markets.},
archiveprefix = {arxiv},
arxivid = {1001.3492},
author = {vlad, sorin and pascu, paul and morariu, nicolae},
eprint = {1001.3492},
file = {:users/roy/documents/knowledge/bibliographie/2010/vlad, pascu, morariu{\_}chaos models in economics.pdf:pdf},
journal = {journal of computing},
keywords = {bifurcation diagram,chaos,complex behavior,nonlinear systems},
month = {jan},
number = {1},
pages = {79--83},
title = {{chaos models in economics}},
volume = {2},
year = {2010}
}
@article{hayashi2013,
author = {hayashi, nobuyuki and ujihara, tomomi and chen, ronggang and irie, kazue and ikezaki, hidekazu},
doi = {10.1016/j.foodres.2013.01.017},
file = {:users/roy/documents/knowledge/bibliographie/2013/hayashi et al.{\_}objective evaluation methods for the bitter and astringent taste intensities of black and oolong teas by a taste sensor.pdf:pdf},
issn = {09639969},
journal = {food research international},
keywords = {astringent taste,bitter taste,black tea,oolong tea,taste sensor},
month = {oct},
number = {2},
pages = {816--821},
publisher = {elsevier ltd},
title = {{objective evaluation methods for the bitter and astringent taste intensities of black and oolong teas by a taste sensor}},
volume = {53},
year = {2013}
}
@article{whitaker2013,
abstract = {ensembles of numerical simulations are used in a variety of applications, such as meteorology or computational solid mechanics, in order to quantify the uncertainty or possible error in a model or simulation. deriving robust statistics and visualizing the variability of an ensemble is a challenging task and is usually accomplished through direct visualization of ensemble members or by providing aggregate representations such as an average or pointwise probabilities. in many cases, the interesting quantities in a simulation are not dense fields, but are sets of features that are often represented as thresholds on physical or derived quantities. in this paper, we introduce a generalization of boxplots, called contour boxplots, for visualization and exploration of ensembles of contours or level sets of functions. conventional boxplots have been widely used as an exploratory or communicative tool for data analysis, and they typically show the median, mean, confidence intervals, and outliers of a population. the proposed contour boxplots are a generalization of functional boxplots, which build on the notion of data depth. data depth approximates the extent to which a particular sample is centrally located within its density function. this produces a center-outward ordering that gives rise to the statistical quantities that are essential to boxplots. here we present a generalization of functional data depth to contours and demonstrate methods for displaying the resulting boxplots for two-dimensional simulation data in weather forecasting and computational fluid dynamics.},
author = {Whitaker, Ross T and Mirzargar, Mahsa and Kirby, Robert M},
doi = {10.1109/tvcg.2013.143},
file = {:users/roy/documents/knowledge/bibliographie/2013/whitaker et al.{\_}contour boxplots a method for characterizing uncertainty in feature sets from simulation ensembles.pdf:pdf},
isbn = {1077-2626},
issn = {10772626},
journal = {{IEEE} Transactions on Visualization and Computer Graphics},
keywords = {boxplots,uncertainty visualization,band depth,ensemble visualization,order statistics},
number = {12},
pages = {2713--2722},
pmid = {24051838},
title = {{Contour boxplots: a method for characterizing uncertainty in feature sets from simulation ensembles}},
volume = {19},
year = {2013}
}
@article{he2013,
abstract = {a one-dimensional non-intrusive polynomial chaos (pc) method is applied in uncertainty quantification (uq) studies for cfd-based ship performances simulations. the uncertainty properties of expected value (ev) and standard deviation (sd) are evaluated by solving the pc coefficients from a linear system of algebraic equations. the one-dimensional pc with the legendre polynomials is applied to: (1) stochastic input domain and (2) cumulative distribution function (cdf) image domain, allowing for more flexibility. the pc method is validated with the monte-carlo benchmark results in several high-fidelity, cfd-based, ship uq problems, evaluating the geometrical, operational and environmental uncertainties for the delft catamaran 372. convergence is studied versus pc order p for both ev and sd, showing that high order pc is not necessary for present applications. comparison is carried out for pc with/without the least square minimization when solving the pc coefficients. the least square minimization, using larger number of cfd samples, is recommended for current test cases. the study shows the potentials of pc method in robust design optimization (rdo) and reliability-based design optimization (rbdo) of ship hydrodynamic performances. {\textcopyright} 2013 publishing house for journal of hydrodynamics.},
author = {he, wei and diez, matteo and campana, emilio fortunato and stern, frederick and zou, zao-jian},
doi = {10.1016/s1001-6058(13)60410-2},
file = {:users/roy/documents/knowledge/bibliographie/2013/he et al.{\_}a one-dimensional polynomial chaos method in cfd–based uncertainty quanti-fication for ship hydrodynamic performance.pdf:pdf},
issn = {10016058},
journal = {journal of hydrodynamics, ser. b},
keywords = {legendre polynomials,polynomial chaos (pc) method,uncertainty quantification (uq),ship design},
month = {oct},
number = {5},
pages = {655--662},
title = {{a one-dimensional polynomial chaos method in cfd–based uncertainty quanti-fication for ship hydrodynamic performance}},
volume = {25},
year = {2013}
}
@article{eldredge2016,
abstract = {an effective approach to the model vuq process by means of direct collaboration between computationalist and experimental data analyst is proposed. an analysis of data from a laminar helium plume experiment provides a demonstration of the proposed collaboration process. consistency analysis serves a central role in the collaboration. it takes the data and uncertainties from both analyst and computationalist and provides an objective and quantifiable measure of agreement between the two. despite the simplicity of the laminar helium system and the computational model, certain phenomena brought to light in the collaboration process make it difficult to find quantitative agreement in the data. these phenomena include the unsteady behavior of air flow in an open room, and the presence of helium permeation to the region near the plume. important sources of error in the simulation include uncertainty in the room temperature (295.15 to 305.15 k), uncertainty in the helium inlet velocity (0.1215 m s {\{}{\$}{\}}{\{}\backslash{\}}frac {\{}{\{}{\}}m{\{}{\}}{\}}{\{}{\{}{\}}s{\{}{\}}{\}}{\{}{\$}{\}} to 0.1415 m s {\{}{\$}{\}}{\{}\backslash{\}}frac {\{}{\{}{\}}m{\{}{\}}{\}}{\{}{\{}{\}}s{\{}{\}}{\}}{\{}{\$}{\}} ), and uncertainty in local helium permeation (0 {\{}{\%}{\}} to 3 {\{}{\%}{\}} by mass.) the collaboration process allows for a better understanding of the phenomena affecting the plume and the relative sensitivies of the system to these phenomena.},
author = {eldredge, weston m. and t{\'{o}}th, p{\'{a}}l and centauri, laurie and eddings, eric g. and kelly, kerry e. and ring, terry a. and sch{\"{o}}nbucher, axel and thornock, jeremy n. and smith, philip j.},
doi = {10.1007/s10494-016-9708-7},
file = {:users/roy/documents/knowledge/bibliographie/2016/eldredge et al.{\_}a collaboration-based approach to cfd model validation and uncertainty quantification (vuq) using data from a laminar he.pdf:pdf},
isbn = {1049401697},
issn = {15731987},
journal = {flow, turbulence and combustion},
keywords = {buoyant plumes,cfd,collaboration,data set consistency,model validation},
number = {2},
pages = {427--449},
title = {{a collaboration-based approach to cfd model validation and uncertainty quantification (vuq) using data from a laminar helium plume}},
volume = {97},
year = {2016}
}
@article{morris1991,
abstract = {a computational model is a representation of some physical or other system of interest, first expressed mathematically and then implemented in the form of a computer program; it may be viewed as a function of inputs that, when evaluated, produces outputs. motivation for this article comes from computational models that are deterministic, complicated enough to make classical mathematical analysis impractical and that have a moderate-to-large number of inputs. the problem of designing computational experiments to determine which inputs have important effects on an output is considered. the proposed experimental plans are composed of individually randomized one-factor-at-a-time designs, and data analysis is based on the resulting random sample of observed elementary effects, those changes in an output due solely to changes in a particular input. advantages of this approach include a lack of reliance on assumptions of relative sparsity of important inputs, monotonicity of outputs with respect to inputs, or adequacy of a low-order polynomial as an approximation to the computational model.},
author = {Morris, Max D},
doi = {10.2307/1269043},
file = {:users/roy/documents/knowledge/bibliographie/1991/morris{\_}factorial sampling plans for preliminary computational experiments.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {computational model,factor screening,latin hypercube sampling,sensitivity analysis},
number = {2},
pages = {161--174},
title = {{Factorial sampling plans for preliminary computational experiments}},
volume = {33},
year = {1991}
}
@inproceedings{simpson1998,
address = {reston, virigina},
author = {simpson, timothy w. and mistree, farrokh and korte, john and mauery, timothy},
booktitle = {7th aiaa/usaf/nasa/issmo symposium on multidisciplinary analysis and optimization},
doi = {10.2514/6.1998-4755},
file = {:users/roy/documents/knowledge/bibliographie/1998/simpson et al.{\_}comparison of response surface and kriging models for multidisciplinary design optimization.pdf:pdf},
isbn = {9780000000002},
month = {sep},
pages = {1--11},
publisher = {american institute of aeronautics and astronautics},
title = {{comparison of response surface and kriging models for multidisciplinary design optimization}},
year = {1998}
}
@article{witteveen2009,
abstract = {uncertainty quantification (uq) is particularly important in transonic flow problems owing to the amplification of input variability across shock waves. in this study we focus on the transonic flow over the rae 2822 airfoil subject to a combination of uncertainties in the mach number, angle of attack, and thickness–to–chord ratio. we represent the vari-ability in the form of uniform probability distributions. this problem corresponds to the external flow test case of the workshop on quantification of cfd uncertainties (hirsch et al. 2009) organized by the european sixth framework programme research project nodesim–cfd on non–deterministic simulation for cfd–based design methodolo-gies (hirsch et al. 2006). the test problem poses specific difficulties for uq methods due to the presence of a discontinuity in the pressure field, although smooth response surfaces for integral quantities of lift, drag, and pitching moment are expected. two uq methods are compared to assess their ability to approximate smooth response surfaces in multi– dimensional probability spaces efficiently and to maintain robustness in the presence of discontinuities. the increased attention for uq methodologies originates from the experience that con-ventional methods such as the monte carlo approach are too computationally intensive for application to computational fluid dynamics (cfd) problems. on the other hand, the stochastic collocation (sc) method (babu{\v{s}}ka et al. 2007) based on gauss quadrature sampling and lagrangian polynomial interpolation in parameter space, although quite efficient, has been shown to have difficulty approximating higher–dimensional probability spaces and discontinuous responses. also separated solution approximations have been developed to achieve a linear increase of computational costs with dimension (doostan {\&} iaccarino 2009), but those are applied only to smooth problems. for robust approximation of discontinuous responses, multi–element sc (foo et al. 2008) and stochastic galerkin (le matre et al. 2004) methods have been proposed. these approaches are usually based on discretizing the probability space and then using surface reconstruction techniques. for higher–order interpolations these methods can still result in local oscillations and overshoots. often not all samples in an element can be reused after refinement, and tensor product extensions to higher dimensions are required, which compromises the efficiency of multi–element discretizations. motivated by the rae 2822 test case, we develop in this paper a simplex elements stochastic collocation (sesc) method that combines a robust approximation of discon-tinuous responses with an efficient discretization in multi–dimensional probability spaces. the sesc method is an extension of the simplex elements method with newton–cotes quadrature (witteveen et al. 2009a) to higher order interpolation and randomized sam-pling. results are compared with those of the sc method based on clenshaw–curtis quadrature. the geometric uncertainty in the thickness–to–chord ratio is treated using a general purpose explicit mesh deformation method based on inverse distance weighting 94 j. a. s. witteveen et al. (idw) interpolation (witteveen {\&} bijl 2009b) of the surface displacements to the interior of the spatial grid. the presentation of the test case results is organized as follows. the sc methods are introduced in section 2. in section 3 the rae 2822 test case is described. the uncer-tainty quantification results are presented and compared in section 4. the discussion is concluded by a summary of the future plans in section 5. 2. stochastic collocation methods},
author = {witteveen, j a s and doostan, a and pe{\v{c}}nik, r and iaccarino, d g},
file = {:users/roy/documents/knowledge/bibliographie/2009/witteveen et al.{\_}uncertainty quantification of the transonic flow around the rae 2822 airfoil.pdf:pdf},
journal = {center for turbulence research annual research briefs},
title = {{uncertainty quantification of the transonic flow around the rae 2822 airfoil}},
year = {2009}
}

@book{niederreiter1992,
abstract = {series:cbms the nsf-cbms regional research conference on random number generation and quasi-monte carlo methods was held at the university of alaska at fair- banks from august 13-17, 1990. the present lecture notes are an expanded written record of a series of ten talks presented by the author as the principal speaker at that conference. it was the aim of this series of lectures to familiar- ize a selected group of researchera with important recent developments in the related areas of quasi-monte carlo methods and uniform pseudorandom number generation. accordingly, the exposition concentrates on recent work in these areas and stresses the interplay between uniform pseudorandom numbers and quasi-monte carlo methods. to make these lecture notes more accessible to nonspecialists, some background material was added.},
author = {niederreiter, harald},
doi = {10.1137/1.9781611970081},
file = {:users/roy/documents/knowledge/bibliographie/1992/niederreiter{\_}random number generation and quasi-monte carlo methods.pdf:pdf},
isbn = {978-0-89871-295-7},
month = {jan},
publisher = {society for industrial and applied mathematics},
title = {{random number generation and quasi-monte carlo methods}},
year = {1992}
}
@inproceedings{kohavi1995,
author = {kohavi, ron},
booktitle = {international joint conference on artificial intelligence},
file = {:users/roy/documents/knowledge/bibliographie/1995/kohavi{\_}a study of cross-validation and bootstrap for accuracy estimaiton and model selection.pdf:pdf},
title = {{a study of cross-validation and bootstrap for accuracy estimaiton and model selection}},
year = {1995}
}
@article{mckerns2011,
abstract = {key questions that scientists and engineers typically want to address can be formulated in terms of predictive science. questions such as: "how well does my computational model represent reality?", "what are the most important parameters in the problem?", and "what is the best next experiment to perform?" are fundamental in solving scientific problems. mystic is a framework for massively-parallel optimization and rigorous sensitivity analysis that enables these motivating questions to be addressed quantitatively as global optimization problems. often realistic physics, engineering, and materials models may have hundreds of input parameters, hundreds of constraints, and may require execution times of seconds or longer. in more extreme cases, realistic models may be multi-scale, and require the use of high-performance computing clusters for their evaluation. predictive calculations, formulated as a global optimization over a potential surface in design parameter space, may require an already prohibitively large simulation to be performed hundreds, if not thousands, of times. the need to prepare, schedule, and monitor thousands of model evaluations, and dynamically explore and analyze results, is a challenging problem that requires a software infrastructure capable of distributing and managing computations on large-scale heterogeneous resources. in this paper, we present the design behind an optimization framework, and also a framework for heterogeneous computing, that when utilized together, can make computationally intractable sensitivity and optimization problems much more tractable.},
archiveprefix = {arxiv},
arxivid = {1202.1056},
author = {mckerns, michael m. and strand, leif and sullivan, tim and fang, alta and aivazis, michael a g},
eprint = {1202.1056},
file = {:users/roy/documents/knowledge/bibliographie/2011/mckerns et al.{\_}building a framework for predictive science.pdf:pdf},
month = {feb},
number = {scipy},
pages = {1--12},
title = {{building a framework for predictive science}},
year = {2012}
}
@article{duchaine2013,
abstract = {determination of heat loads is a key issue in the design of gas turbines. in order to opti- mize the cooling, an exact knowledge of the heat flux and temperature distributions on the airfoils surface is necessary. heat transfer is influenced by various factors, like pres- sure distribution, wakes, surface curvature, secondary flow effects, surface roughness, free stream turbulence, and separation. each of these phenomenons is a challenge for nu- merical simulations. among numerical methods, large eddy simulations (les) offers new design paths to diminish development costs of turbines through important reductions of the number of experimental tests. in this study, les is coupled with a thermal solver in order to investigate the flow field and heat transfer around a highly loaded low pressure water-cooled turbine vane at moderate reynolds number (150,000). the meshing strategy (hybrid grid with layers of prisms at the wall and tetrahedra elsewhere) combined with a high fidelity les solver gives accurate predictions of the wall heat transfer coefficient for isothermal computations. mesh convergence underlines the known result that wall- resolved les requires discretizations for which yþ is of the order of one. the analysis of the flow field gives a comprehensive view of the main flow features responsible for heat transfer, mainly the separation bubble on the suction side that triggers transition to a tur- bulent boundary layer and the massive separation region on the pressure side. conjugate heat transfer computation gives access to the temperature distribution in the blade, which is in good agreement with experimental measurements. finally, given the uncertainty on the coolant water temperature provided by experimentalists, uncertainty quantification allows apprehension of the effect of this parameter on the temperature distribution.},
author = {duchaine, florent and maheu, nicolas and moureau, vincent and balarac, guillaume and moreau, st{\'{e}}phane},
doi = {10.1115/1.4025165},
file = {:users/roy/documents/knowledge/bibliographie/2013/duchaine et al.{\_}large-eddy simulation and conjugate heat transfer around a low-mach turbine blade.pdf:pdf},
issn = {0889-504x},
journal = {journal of turbomachinery},
month = {oct},
number = {5},
pages = {051015},
title = {{large-eddy simulation and conjugate heat transfer around a low-mach turbine blade}},
volume = {136},
year = {2013}
}

@techreport{turns2011,
file = {:users/roy/documents/knowledge/bibliographie/2016/unknown{\_}reference guide openturns 1.7.pdf:pdf},
title = {{reference guide openturns 1.7}},
year = {2016}
}
@article{cordier2003,
author = {cordier, laurent and bergmann, michel},
file = {:users/roy/documents/knowledge/bibliographie/2003/cordier, bergmann{\_}two typical applications of pod coherent structures eduction and reduced order modelling.pdf:pdf},
journal = {post-processing of experimental and numerical data},
title = {{two typical applications of pod: coherent structures eduction and reduced order modelling}},
year = {2003}
}
@article{radovic1996,
author = {radovic, igor and sobol', i.m and tichy, robert f},
file = {:users/roy/documents/knowledge/bibliographie/1996/radovic, sobol', tichy{\_}quasi-monte carlo methods for numerical integration comparison of different low discrepancy sequences.pdf:pdf},
journal = {journal of monte calo methods and applications},
keywords = {low discrepancy sequences,monte carlo method,numerical integration,quasi-monte carlo method,tributed sequences,uniformly dis-},
number = {1},
pages = {1--14},
title = {{quasi-monte carlo methods for numerical integration: comparison of different low discrepancy sequences}},
volume = {2},
year = {1996}
}
@article{evensen2004,
author = {evensen, geir},
doi = {10.1007/s10236-004-0099-2},
file = {:users/roy/documents/knowledge/bibliographie/2004/evensen{\_}sampling strategies and square root analysis schemes for the enkf.pdf:pdf},
journal = {ocean dynamics},
keywords = {data assimilation {\'{a}} ensemble,kalman filter},
number = {54},
pages = {539--560},
title = {{sampling strategies and square root analysis schemes for the enkf}},
year = {2004}
}
@article{ge2015,
abstract = {sensitivity analysis (sa) is able to identify the most influential parameters of a given model. application of sa is usually critical for reducing the complexity in the subsequent model calibration and use. unfortunately it is hardly applied, especially when the model is in the form of a computationally expensive black-box computer program. a possible solution concerns applying sa to the metamodel (i.e., an approximation of the computationally expensive model) instead. among the other options, the use of gaussian process metamodels (also known as kriging metamodels) has been recently proposed for the sa of computationally expensive traffic simulation models. however, the main limitation of this approach is its dependence on the model dimensionality. when the model is high-dimensional, the estimation of the kriging metamodel may still be problematic due to its high computational cost. in order to overcome this problem, in the present paper, the kriging-based approach has been combined with the quasi-optimized trajectory based elementary effects (quasi-otee) approach for the sa of high-dimensional models. the quasi-otee sa is used first to screen the influential and non-influential parameters of a high-dimensional model; then the kriging-based sa is used to calculate the variance-based sensitivity indices, and to rank the most influential parameters in a more accurate way. the application of the proposed sequential sa is illustrated with several numerical experiments. results show that the method can properly identify the most influential parameters and their ranks, while the number of model evaluations is considerably less than the variance-based sa (e.g., in one of the tests the sequential sa requires over 50 times less model evaluations than the variance-based sa).},
author = {ge, qiao and ciuffo, biagio and menendez, monica},
doi = {10.1016/j.ress.2014.08.009},
file = {:users/roy/documents/knowledge/bibliographie/2015/ge, ciuffo, menendez{\_}combining screening and metamodel-based methods an efficient sequential approach for the sensitivity analysis of mo.pdf:pdf},
issn = {09518320},
journal = {reliability engineering {\&} system safety},
keywords = {high-dimensional and computationally expensive mod,metamodel,screening,sensitivity analysis,variance-based approach},
month = {feb},
pages = {334--344},
publisher = {elsevier},
title = {{combining screening and metamodel-based methods: an efficient sequential approach for the sensitivity analysis of model outputs}},
volume = {134},
year = {2015}
}
@article{barabasi2007,
author = {barab{\'{a}}si, albert-l{\'{a}}szl{\'{o}}},
doi = {10.1109/mcs.2007.384127},
file = {:users/roy/documents/knowledge/bibliographie/2007/barab{\'{a}}si{\_}from network structure.pdf:pdf},
issn = {0272-1708},
journal = {ieee control systems magazine},
number = {august},
pages = {33--42},
title = {{from network structure}},
year = {2007}
}
@article{courrier2014,
abstract = {the main objective of this paper is to propose an optimization strategy which uses partially converged data to minimize the computational effort associated with an optimization procedure. the framework of this work is the optimization of assemblies involving contact and friction. several tools have been developed in order to use a surrogate model as an alternative to the actual mechanical model. then, the global optimization can be carried out using this surrogate model, which is much less expensive. this approach has two drawbacks: the cpu time required to generate the surrogate model and the inaccuracy of this model. in order to alleviate these drawbacks, we propose to minimize the cpu time by using partially converged data and then to apply a correction strategy. two methods are tested in this paper. the first one consists in updating a partially converged metamodel using global enrichment. the second one consists in seeking the global minimum using the weighted expected improvement. one can achieve a time saving of about 10 when seeking the global minimum. {\textcopyright} 2013 elsevier ltd. all rights reserved.},
author = {courrier, n. and boucard, p.-a. and soulier, b.},
doi = {10.1016/j.advengsoft.2013.09.008},
file = {:users/roy/documents/knowledge/bibliographie/2014/courrier, boucard, soulier{\_}the use of partially converged simulations in building surrogate models.pdf:pdf},
issn = {09659978},
journal = {advances in engineering software},
keywords = {evofusion,global optimization,latin method,metamodel,partially converged data},
month = {jan},
pages = {186--197},
publisher = {elsevier ltd},
title = {{the use of partially converged simulations in building surrogate models}},
volume = {67},
year = {2014}
}
@article{chen2010,
abstract = {we address covariance estimation in the sense of minimum mean-squared error (mmse) when the samples are gaussian distributed. specifically, we consider shrinkage methods which are suitable for high dimensional problems with a small number of samples (large p small n). first, we improve on the ledoit-wolf (lw) method by conditioning on a sufficient statistic. by the rao-blackwell theorem, this yields a new estimator called rblw, whose mean-squared error dominates that of lw for gaussian variables. second, to further reduce the estimation error, we propose an iterative approach which approximates the clairvoyant shrinkage estimator. convergence of this iterative method is established and a closed form expression for the limit is determined, which is referred to as the oracle approximating shrinkage (oas) estimator. both rblw and oas estimators have simple expressions and are easily implemented. although the two methods are developed from different perspectives, their structure is identical up to specified constants. the rblw estimator provably dominates the lw method for gaussian samples. numerical simulations demonstrate that the oas approach can perform even better than rblw, especially when n is much less than p . we also demonstrate the performance of these techniques in the context of adaptive beamforming.},
archiveprefix = {arxiv},
arxivid = {0907.4698},
author = {chen, yilun and wiesel, ami and eldar, yonina c. and hero, alfred o.},
doi = {10.1109/tsp.2010.2053029},
eprint = {0907.4698},
file = {:users/roy/documents/knowledge/bibliographie/2010/chen et al.{\_}shrinkage algorithms for mmse covariance estimation.pdf:pdf},
isbn = {1053-587x},
issn = {1053587x},
journal = {ieee transactions on signal processing},
keywords = {beamforming,covariance estimation,minimum mean-squared error (mmse),shrinkage},
number = {10},
pages = {5016--5029},
pmid = {1000198468},
title = {{shrinkage algorithms for mmse covariance estimation}},
volume = {58},
year = {2010}
}
@article{sobol2009,
abstract = {the variance-based method of sobol sensitivity indices is very popular among practitioners due to its efficiency and easiness of interpretation. however, for high-dimensional models the direct application of this method can be very time consuming and prohibitively expensive to use. one of the alternative global sensitivity analysis methods known as the method of derivative based global sensitivity measures (dgsm) has recently become popular among practitioners. it has a link with the morris screening method and sobol sensitivity indices. dgsm are very easy to implement and evaluate numerically. the computational time required for numerical evaluation of dgsm is generally much lower than that for estimation of sobol sensitivity indices. we present a survey of recent advances in dgsm and new results concerning new lower and upper bounds on the values of sobol total sensitivity indices. using these bounds it is possible in most cases to get a good practical estimation of the values of sobol total sensitivity indices. several examples are used to illustrate an application of dgsm.},
archiveprefix = {arxiv},
arxivid = {1605.07830},
author = {kucherenko, s. and song, s.},
doi = {10.1007/978-3-319-33507-0_23},
eprint = {1605.07830},
file = {:users/roy/documents/knowledge/bibliographie/2009/kucherenko, song{\_}derivative-based global sensitivity measures and their link with sobol sensitivity indices.pdf:pdf},
isbn = {0378-4754},
issn = {03784754},
journal = {mathematics and computers in simulation},
keywords = {derivative based global sensitivity measure,global sensitivity index,morris method,quasi monte carlo method},
month = {may},
number = {10},
pages = {3009--3017},
title = {{derivative-based global sensitivity measures and their link with sobol sensitivity indices}},
volume = {79},
year = {2009}
}
@article{toal2013,
abstract = {the selection of stationary or non-stationary$\backslash$r$\backslash$nkriging to create a surrogate model of a black box$\backslash$r$\backslash$nfunction requires apriori knowledge of the nature of$\backslash$r$\backslash$nresponse of the function as these techniques are bet-$\backslash$r$\backslash$nter at representing some types of responses than oth-$\backslash$r$\backslash$ners. while an adaptive technique has been previously$\backslash$r$\backslash$nproposed to adjust the level of stationarity within the$\backslash$r$\backslash$nsurrogate model such a model can be prohibitively ex-$\backslash$r$\backslash$npensive to construct for high dimensional problems. an$\backslash$r$\backslash$nalternative approach is to employ a surrogate model$\backslash$r$\backslash$nconstructed from an ensemble of stationary and non-$\backslash$r$\backslash$nstationary kriging models. the following paper assesses$\backslash$r$\backslash$nthe accuracy and optimization performance of such a$\backslash$r$\backslash$nmodelling strategy using a number of analytical func-$\backslash$r$\backslash$ntions and engineering design problems.},
author = {toal, david j. and keane, andy j.},
doi = {10.1007/s00158-012-0866-5},
file = {:users/roy/documents/knowledge/bibliographie/2013/toal, keane{\_}performance of an ensemble of ordinary, universal, non-stationary and limit kriging predictors.pdf:pdf},
issn = {1615147x},
journal = {structural and multidisciplinary optimization},
keywords = {ensemble,kriging,surrogate modelling},
number = {6},
pages = {893--903},
title = {{performance of an ensemble of ordinary, universal, non-stationary and limit kriging predictors}},
volume = {47},
year = {2013}
}
@incollection{gratiet2016,
abstract = {global sensitivity analysis is now established as a powerful approach for determining the key random input parameters that drive the uncertainty of model output predictions. yet the classical computation of the so-called sobol' indices is based on monte carlo simulation, which is not affordable when computa- tionally expensive models are used, as it is the case in most applications in engineering and applied sciences. in this respect metamodels such as polynomial chaos expansions (pce) and gaussian processes (gp) have received tremendous attention in the last few years, as they allow one to replace the original, taxing model by a surrogate which is built from an experimental design of limited size. then the surrogate can be used to compute the sensitivity indices in negligible time. in this chapter an introduction to each technique is given, with an emphasis on their strengths and limitations in the context of global sensitivity analysis. in particular, sobol' (resp. total sobol') indices can be computed analytically from the pce coefficients. in contrast, confidence intervals on sensitivity indices can be derived straightforwardly from the properties of gps. the performance of the two techniques is finally compared on three well-known analytical benchmarks (ishigami, g-sobol', and morris functions) as well as on a realistic engineering application (deflection of a truss structure).},
author = {gratiet, lo{\"{i}}c le and marelli, stefano and sudret, bruno},
booktitle = {handbook of uncertainty quantification},
doi = {10.1007/978-3-319-11259-6_38-1},
file = {:users/roy/documents/knowledge/bibliographie/2015/gratiet, marelli, sudret{\_}metamodel-based sensitivity analysis polynomial chaos expansions and gaussian processes.pdf:pdf},
isbn = {978-3-319-12384-4},
keywords = {error estimation,gaussian process regression,kriging,model selection,polynomial chaos expansions,sobol' indices},
pages = {1--37},
publisher = {springer international publishing},
title = {{metamodel-based sensitivity analysis: polynomial chaos expansions and gaussian processes}},
year = {2015}
}

@inproceedings{martinez2015,
author = {martinez, felipe aguirre},
booktitle = {openturns users day 8},
file = {:users/roy/documents/knowledge/bibliographie/2015/martinez{\_}uncertainty treatment in dispersion modelling of accidental releases.pdf:pdf},
title = {{uncertainty treatment in dispersion modelling of accidental releases}},
year = {2015}
}
@article{lilley1977,
abstract = {consideration is given to the major features of the characterization of swirl flow combustion, with emphasis on application to practical combustors. recent experimental work is surveyed first with special regard to the main effects of swirl on the performance, stability, and combustion intensity of flames in combustors. since solution of the basic governing equations yields predictions that are realistic only if the physical processes are sufficiently well expressed in mathematical form and suitable computational methods of solution are employed, these details are discussed. the treatment is brief, since extensive reviews are available in the literature. it is possible to predict major features of these swirling flows and some solutions are exhibited. the achievements and current status of serial flow combustion are summarized.},
author = {Lilley, David G.},
doi = {10.2514/3.60756},
file = {:users/roy/documents/knowledge/bibliographie/1977/lilley{\_}swirl flows in combustion a review.pdf:pdf},
isbn = {0001-1452},
issn = {0001-1452},
journal = {{AIAA} Journal},
number = {8},
pages = {1063--1078},
title = {{Swirl flows in combustion: a review}},
volume = {15},
year = {1977}
}
@inproceedings{mazzo2016,
abstract = {the present paper presents a numerical study of the impact of tip gap uncertainties in a multistage turbine. it is well known that the rotor gap can change the gas turbine efficiency but the impact of the random variation of the clearance height has not been investigated before. in this paper the radial seals clearance of a datum shroud geometry, representative of steam turbine industrial practice, was systematically varied and numerically tested. by using a non-intrusive uncertainty quantification simulation based on a sparse arbitrary moment based approach, it is possible to predict the radial distribution of uncertainty in stagnation pressure and yaw angle at the exit of the turbine blades. this work shows that the impact of gap uncertainties propagates radially from the tip towards the hub of the turbine and the complete span is affected by a variation of the rotor tip gap. this amplification of the uncertainty is mainly due to the low aspect ratio of the turbine and a similar behavior is expected in high pressure turbines.},
author = {Mazzoni, C. and Ahlfeld, R. and Rosic, B. and Montomoli, F.},
booktitle = {International symposium on transport phenomena and dynamics of rotating machinery},
file = {:users/roy/documents/knowledge/bibliographie/2016/mazzoni et al.{\_}uncertainty quantification of leakages in a multistage simulation and comparison with experiments.pdf:pdf},
keywords = {cfd,tip leakage,uncertainty quantification},
title = {{Uncertainty quantification of leakages in a multistage simulation and comparison with experiments}},
year = {2016}
}
@article{tompson2016,
abstract = {real-time simulation of fluid and smoke is a long standing problem in computer graphics, where state-of-the-art approaches require large compute resources, making real-time applications often impractical. in this work, we propose a data-driven approach that leverages the approximation power of deep-learning methods with the precision of standard fluid solvers to obtain both fast and highly realistic simulations. the proposed method solves the incompressible euler equations following the standard operator splitting method in which a large, often ill-condition linear system must be solved. we propose replacing this system by learning a convolutional network (convnet) from a training set of simulations using a semi-supervised learning method to minimize long-term velocity divergence. convnets are amenable to efficient gpu implementations and, unlike exact iterative solvers, have fixed computational complexity and latency. the proposed hybrid approach restricts the learning task to a linear projection without modeling the well understood advection and body forces. we present real-time 2d and 3d simulations of fluids and smoke; the obtained results are realistic and show good generalization properties to unseen geometry.},
archiveprefix = {arxiv},
arxivid = {1607.03597},
author = {Tompson, Jonathan and Schlachter, Kristofer and Sprechmann, Pablo and Perlin, Ken},
eprint = {1607.03597},
file = {:users/roy/documents/knowledge/bibliographie/2016/tompson et al.{\_}accelerating eulerian fluid simulation with convolutional networks.pdf:pdf},
month = {jul},
title = {{Accelerating eulerian fluid simulation with convolutional networks}},
year = {2016}
}
@article{bonneau2014,
abstract = {{\textcopyright} springer-verlag london 2014. the goal of visualization is to effectively and accurately communicate data. visualization research has often overlooked the errors and uncertainty which accompany the scientific process and describe key characteristics used to fully understand the data. the lack of these representations can be attributed, in part, to the inherent difficulty in defining, characterizing, and controlling this uncertainty, and in part, to the difficulty in including additional visual metaphors in a well designed, potent display. however, the exclusion of this information cripples the use of visualization as a decision making tool due to the fact that the display is no longer a true representation of the data. this systematic omission of uncertainty commands fundamental research within the visualization community to address, integrate, and expect uncertainty information. in this chapter, we outline sources and models of uncertainty, give an overview of the state-of-the-art, provide general guidelines, outline small exemplary applications, and finally, discuss open problems in uncertainty visualization.},
author = {Bonneau, Georges Pierre and Hege, Hans Christian and Johnson, Chris R. and Oliveira, Manuel M. and Potter, Kristin and Rheingans, Penny and Schultz, Thomas},
doi = {10.1007/978-1-4471-6497-5_1},
file = {:users/roy/documents/knowledge/bibliographie/2014/bonneau et al.{\_}overview and state-of-the-art of uncertainty visualization.pdf:pdf},
isbn = {1447164962},
issn = {2197666x},
journal = {Mathematics and Visualization},
pages = {3--27},
title = {{Overview and state-of-the-art of uncertainty visualization}},
volume = {37},
year = {2014}
}
@article{sobol2001,
abstract = {global sensitivity indices for rather complex mathematical models can be efficiently computed by monte carlo (or quasi-monte carlo) methods. these indices are used for estimating the influence of individual variables or groups of variables on the model output.},
author = {sobol', i.m},
doi = {10.1016/s0378-4754(00)00270-6},
file = {:users/roy/documents/knowledge/bibliographie/2001/sobol'{\_}global sensitivity indices for nonlinear mathematical models and their monte carlo estimates.pdf:pdf},
issn = {03784754},
journal = {mathematics and computers in simulation},
keywords = {mathematical modelling,monte carlo method,quasi-monte carlo method,sensitivity analysis},
number = {1-3},
pages = {271--280},
title = {{global sensitivity indices for nonlinear mathematical models and their monte carlo estimates}},
volume = {55},
year = {2001}
}
@article{courrier2016,
abstract = {this paper deals with the advantages of using variable-fidelity metamodeling strategies in order to develop a valid metamodel more rapidly than by using traditional methods. in our mechanical assembly},
author = {Courrier, Nicolas and Boucard, Pierre Alain and Soulier, Bruno},
doi = {10.1007/s10898-015-0345-9},
file = {:users/roy/documents/knowledge/bibliographie/2016/courrier, boucard, soulier{\_}variable-fidelity modeling of structural analysis of assemblies.pdf:pdf},
issn = {15732916},
journal = {Journal of global optimization},
keywords = {cokriging,kriging,metamodel,partially converged data,variable-fidelity},
number = {3},
pages = {577--613},
publisher = {springer us},
title = {{Variable-fidelity modeling of structural analysis of assemblies}},
volume = {64},
year = {2016}
}
@incollection{iooss2015,
author = {iooss, bertrand and lema{\^{i}}tre, paul},
booktitle = {uncertainty management in simulation-optimization of complex systems},
chapter = {5},
doi = {10.1007/978-1-4899-7547-8_5},
file = {:users/roy/documents/knowledge/bibliographie/2015/iooss, lema{\^{i}}tre{\_}a review on global sensitivity analysis methods.pdf:pdf},
isbn = {9781489975478},
issn = {1387-666x},
pages = {101--122},
title = {{a review on global sensitivity analysis methods}},
volume = {59},
year = {2015}
}
@article{isaac2015,
abstract = {the majority of research on efficient and scalable algorithms in computational science and engineering has focused on the forward problem: given parameter inputs, solve the governing equations to determine output quantities of interest. in contrast, here we consider the broader question: given a (large-scale) model containing uncertain parameters, (possibly) noisy observational data, and a prediction quantity of interest, how do we construct efficient and scalable algorithms to (1) infer the model parameters from the data (the deterministic inverse problem), (2) quantify the uncertainty in the inferred parameters (the bayesian inference problem), and (3) propagate the resulting uncertain parameters through the model to issue predictions with quantified uncertainties (the forward uncertainty propagation problem)?we present efficient and scalable algorithms for this end-to-end, data-to-prediction process under the gaussian approximation and in the context of modeling the flow of the antarctic ice sheet and its effect on loss of grounded ice to the ocean. the ice is modeled as a viscous, incompressible, creeping, shear-thinning fluid. the observational data come from satellite measurements of surface ice flow velocity, and the uncertain parameter field to be inferred is the basal sliding parameter, represented by a heterogeneous coefficient in a robin boundary condition at the base of the ice sheet. the prediction quantity of interest is the present-day ice mass flux from the antarctic continent to the ocean.we show that the work required for executing this data-to-prediction process-measured in number of forward (and adjoint) ice sheet model solves-is independent of the state dimension, parameter dimension, data dimension, and the number of processor cores. the key to achieving this dimension independence is to exploit the fact that, despite their large size, the observational data typically provide only sparse information on model parameters. this property can be exploited to construct a low rank approximation of the linearized parameter-to-observable map via randomized svd methods and adjoint-based actions of hessians of the data misfit functional.},
archiveprefix = {arxiv},
arxivid = {arxiv:1410.1221v1},
author = {isaac, tobin and petra, noemi and stadler, georg and ghattas, omar},
doi = {10.1016/j.jcp.2015.04.047},
eprint = {arxiv:1410.1221v1},
file = {:users/roy/documents/knowledge/bibliographie/2015/isaac et al.{\_}scalable and efficient algorithms for the propagation of uncertainty from data through inference to prediction for large-sc.pdf:pdf},
issn = {00219991},
journal = {journal of computational physics},
keywords = {adjoint-based hessian,antarctic ice sheet,bayesian inference,data-to-prediction,ice sheet flow modeling,inexact newton-krylov method,inverse problems,low-rank approximation,nonlinear stokes equations,preconditioning,uncertainty quantification},
month = {sep},
pages = {348--368},
title = {{scalable and efficient algorithms for the propagation of uncertainty from data through inference to prediction for large-scale problems, with application to flow of the antarctic ice sheet}},
volume = {296},
year = {2015}
}
@inproceedings{wheeler2015,
abstract = {in this paper we establish a benchmark data set of a generic high-pressure turbine vane generated by direct numerical simu- lation (dns) to resolve fully the flow. the test conditions for this case are a reynolds number of 0.57 million and an exit mach number of 0.9, which is representative of a modern tran- sonic high-pressure turbine vane. in this study we first com- pare the simulation results with previously published experimen- tal data. we then investigate how turbulence affects the surface flowphysics and heat transfer. an analysis of the development of loss through the vane passage is also performed. the results in- dicate that free-stream turbulence tends to induce streaks within the near wall flow, which augment the surface heat transfer. tur- bulent breakdown is observed over the late suction surface, and this occurs via the growth of two-dimensional kelvin-helmholtz spanwise roll-ups, which then develop into lambda vortices cre- ating large local peaks in the surface heat transfer. turbulent dissipation is found to significantly increase losses within the trailing-edge region of the vane. nomenclature},
address = {montr{\'{e}}al},
author = {Wheeler, Andrew P S and Sandberg, Richard D and Sandham, Neil D and Pichler, Richard and Michelassi, Vittorio and Laskowski, Greg},
booktitle = {{ASME} Turbo Expo 2015: turbine technical conference and exposition},
file = {:users/roy/documents/knowledge/bibliographie/2015/wheeler et al.{\_}direct numerical simulations of a high pressure turbine vane.pdf:pdf},
pages = {1--12},
title = {{Direct numerical simulations of a high pressure turbine vane}},
year = {2015}
}
@article{romano2016,
abstract = {given an image, we wish to produce an image of larger size with significantly more pixels and higher image quality. this is generally known as the single image super-resolution (sisr) problem. the idea is that with sufficient training data (corresponding pairs of low and high resolution images) we can learn set of filters (i.e. a mapping) that when applied to given image that is not in the training set, will produce a higher resolution version of it, where the learning is preferably low complexity. in our proposed approach, the run-time is more than one to two orders of magnitude faster than the best competing methods currently available, while producing results comparable or better than state-of-the-art. a closely related topic is image sharpening and contrast enhancement, i.e., improving the visual quality of a blurry image by amplifying the underlying details (a wide range of frequencies). our approach additionally includes an extremely efficient way to produce an image that is significantly sharper than the input blurry one, without introducing artifacts such as halos and noise amplification. we illustrate how this effective sharpening algorithm, in addition to being of independent interest, can be used as a pre-processing step to induce the learning of more effective upscaling filters with built-in sharpening and contrast enhancement effect.},
archiveprefix = {arxiv},
arxivid = {1606.01299},
author = {romano, yaniv and isidoro, john and milanfar, peyman},
doi = {10.1109/tci.2016.2629284},
eprint = {1606.01299},
file = {:users/roy/documents/knowledge/bibliographie/2017/romano, isidoro, milanfar{\_}raisr rapid and accurate image super resolution.pdf:pdf},
issn = {2333-9403},
journal = {ieee transactions on computational imaging},
month = {mar},
number = {1},
pages = {110--125},
title = {{raisr: rapid and accurate image super resolution}},
volume = {3},
year = {2017}
}
@book{toro2009,
abstract = {high resolution upwind and centred methods are today a mature generation of computational techniques applicable to a wide range of engineering and scientific disciplines, computational fluid dynamics (cfd) being the most prominent up to now. this textbook gives a comprehensive, coherent and practical presentation of this class of techniques. the book is designed to provide readers with an understanding of the basic concepts, some of the underlying theory, the ability to critically use the current research papers on the subject, and, above all, with the required information for the practical implementation of the methods. direct applicability of the methods include: compressible, steady, unsteady, reactive, viscous, non-viscous and free surface flows. for this third edition the book was thoroughly revised and contains substantially more, and new material both in its fundamental as well as in its applied parts.},
author = {toro, eleuterio f.},
booktitle = {book},
doi = {10.1007/b7976},
file = {:users/roy/documents/knowledge/bibliographie/2009/toro{\_}riemann solvers and numerical methods for fluid dynamics-a practical introduction.pdf:pdf},
isbn = {9783540252023},
keywords = {computational fluid mechanics,convection-reaction-diffusion problems,godunov methods,hyperbolic conservation laws,random choice methods,total variaton diminishing},
pages = {749},
title = {{riemann solvers and numerical methods for fluid dynamics-a practical introduction}},
year = {2009}
}
@article{cavazzuti2008,
abstract = {heat transfer enhancing surfaces are of interest for a wide range of industrial applications. the aim of this article is to provide a robust automated method for the design of two-dimensional enhanced surfaces. multiobjective optimization algorithms are employed; the competing objectives addressed are the maximization of the heat transfer and the minimization of the pressure drop for re¬†=¬†1,000 and pr¬†=¬†0.74. the surfaces are parameterized with b√{\textcopyright}zier curves, and a finite-volume solver is used for the computational fluid dynamics analysis. the optimization is based on different algorithms used sequentially. finally, a robust design assessment analysis is carried out on two configurations.},
author = {cavazzuti, m. and corticelli, m. a.},
doi = {10.1080/10407780802289335},
file = {:users/roy/documents/knowledge/bibliographie/2008/cavazzuti, corticelli{\_}optimization of heat exchanger enhanced surfaces through multiobjective genetic algorithms.pdf:pdf},
issn = {1040-7782},
journal = {numerical heat transfer, part a: applications},
number = {6},
pages = {603--624},
title = {{optimization of heat exchanger enhanced surfaces through multiobjective genetic algorithms}},
volume = {54},
year = {2008}
}
@article{faure2009,
abstract = {halton sequences have always been quite popular with practitioners, in part because of their intuitive definition and ease of implementation. however, in their original form, these sequences have also been known for their inadequacy to integrate functions in moderate to large dimensions, in which case (t, s)-sequences such as the sobol' sequence are usually preferred. to overcome this problem, one possible approach is to include permutations in the definition of halton sequences thereby obtaining generalized halton sequences-an idea that goes back to almost thirty years ago, and that has been studied by many researchers in the last few years. in parallel to these efforts, an important improvement in the upper bounds for the discrepancy of halton sequences has been made by atanassov in 2004. together, these two lines of research have revived the interest in halton sequences. in this article, we review different generalized halton sequences that have been proposed recently, and compare them by means of numerical experiments. we also propose a new generalized halton sequence which, we believe, offers a practical advantage over the surveyed constructions, and that should be of interest to practitioners.},
author = {faure, henri and lemieux, christiane},
doi = {10.1145/1596519.1596520},
file = {:users/roy/documents/knowledge/bibliographie/2009/faure, lemieux{\_}generalized halton sequences in 2008.pdf:pdf},
issn = {10493301},
journal = {acm transactions on modeling and computer simulation},
month = {oct},
number = {4},
pages = {1--31},
title = {{generalized halton sequences in 2008}},
volume = {19},
year = {2009}
}
@article{joe2008,
author = {joe, stephen and kuo, frances y.},
doi = {10.1137/070709359},
file = {:users/roy/documents/knowledge/bibliographie/2008/joe, kuo{\_}constructing sobol sequences with better two-dimensional projections.pdf:pdf},
issn = {1064-8275},
journal = {siam journal on scientific computing},
keywords = {sobol' sequences,digital nets and sequences,numerical integration,quasi-monte carlo methods,two-dimensional projections},
month = {jan},
number = {5},
pages = {2635--2654},
title = {{constructing sobol sequences with better two-dimensional projections}},
volume = {30},
year = {2008}
}
@article{shahriari2016a,
abstract = {—big data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. the construction of such systems involves many distributed design choices. the end products (e.g., rec-ommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involves many tunable config-uration parameters. these parameters are often specified and hard-coded into the software by various developers or teams. if optimized jointly, these parameters can result in significant improvements. bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. it promises greater automation so as to increase both product quality and human productivity. this review paper introduces bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
archiveprefix = {arxiv},
arxivid = {arxiv:1011.1669v3},
author = {shahriari, bobak and swersky, kevin and wang, ziyu and adams, ryan p. and {de freitas}, nando},
doi = {10.1109/jproc.2015.2494218},
eprint = {arxiv:1011.1669v3},
file = {:users/roy/documents/knowledge/bibliographie/2016/shahriari et al.{\_}taking the human out of the loop a review of bayesian optimization.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {proceedings of the ieee},
keywords = {decision making,design of experiments,genomic medicine,optimization,response surface methodology,statistical learning},
number = {1},
pages = {148--175},
pmid = {25246403},
title = {{taking the human out of the loop: a review of bayesian optimization}},
volume = {104},
year = {2016}
}
@article{lu2017,
abstract = {predictive analytics embraces an extensive range of techniques including statistical modeling, machine learning, and data mining and is applied in business intelligence, public health, disaster management and response, and many other fields. to date, visualization has been broadly used to support tasks in the predictive analytics pipeline. primary uses have been in data cleaning, exploratory analysis, and diagnostics. for example, scatterplots and bar charts are used to illustrate class distributions and responses. more recently, extensive visual analytics systems for feature selection, incremental learning, and various prediction tasks have been proposed to support the growing use of complex models, agent-specific optimization, and comprehensive model comparison and result exploration. such work is being driven by advances in interactive machine learning and the desire of end-users to understand and engage with the modeling process. in this state-of-the-art report, we catalogue recent advances in the visualization community for supporting predictive analytics. first, we define the scope of predictive analytics discussed in this article and describe how visual analytics can support predictive analytics tasks in a predictive visual analytics (pva) pipeline. we then survey the literature and categorize the research with respect to the proposed pva pipeline. systems and techniques are evaluated in terms of their supported interactions, and interactions specific to predictive analytics are discussed. we end this report with a discussion of challenges and opportunities for future research in predictive visual analytics.},
author = {lu, yafeng and garcia, rolando and hansen, brett and gleicher, michael and macijewski, ross},
doi = {10.1111/cgf.13210},
file = {:users/roy/documents/knowledge/bibliographie/2017/lu et al.{\_}the state-of-the-art in predictive visual analytics.pdf:pdf},
issn = {01677055},
journal = {eurovis},
number = {3},
title = {{the state-of-the-art in predictive visual analytics}},
volume = {36},
year = {2017}
}
@inproceedings{mader2017,
abstract = {{\textcopyright} 2017 american institute of aeronautics and astronautics inc, aiaa. all rights reserved. the d8 “double-bubble” is a new transport aircraft configuration proposed by mit that has the potential to provide large improvements in fuel efficiency. it has been studied at the conceptual level and in low-speed wind tunnel tests, but there was, up to now, no higher fidelity transonic design. in this study, we use the conceptual design definition as a starting point and develop a high-fidelity aero-structural optimization problem for the d8 configuration using the mach framework. via gradient-based optimization techniques with coupled rans and shell fea, the d8 is optimized at three different cruise mach numbers (0.72, 0.78, 0.84). the preliminary results presented confirm that the planforms designed with the conceptual level tools perform well and as predicted, and that some small improvements can be gained from further optimization. in spite of its limitations, this work confirms the viability of the d8 wing-body in the transonic regime and provides preliminary transonic geometries for future work.},
author = {mader, c.a. and kenway, g.k. and martins, j.r.r.a. and uranga, a.},
booktitle = {18th aiaa/issmo multidisciplinary analysis and optimization conference, 2017},
doi = {10.2514/6.2017-4436},
file = {:users/roy/documents/knowledge/bibliographie/2017/mader et al.{\_}aerostructural optimization of the d8 wing with varying cruise mach numbers.pdf:pdf},
isbn = {9781624105074},
number = {june},
pages = {1--22},
title = {{aerostructural optimization of the d8 wing with varying cruise mach numbers}},
year = {2017}
}

@article{roache1997,
abstract = {this review coversverification,validation, confirmation and related subjects for computational fluid dynamics (cfd), including error taxonomies, error estima- tion and banding, convergence rates, surrogate estimators, nonlinear dynamics, and error estimation for grid adaptation vs quantification of uncertainty.},
author = {roache, p j},
doi = {10.1146/annurev.fluid.29.1.123},
file = {:users/roy/documents/knowledge/bibliographie/1997/roache{\_}quantification of uncertainty in computational fluid dynamics.pdf:pdf},
issn = {0066-4189},
journal = {annual review of fluid mechanics},
keywords = {computational,error estimation,numerical uncertainty,validation,verification},
month = {jan},
number = {1},
pages = {123--160},
title = {{quantification of uncertainty in computational fluid dynamics}},
volume = {29},
year = {1997}
}
@article{masoliver2011,
abstract = {in recent years there has been a considerable increase in the publishing of textbooks and monographs covering what was formerly known as random or irregular deterministic motion, now named by the more fashionable term of deterministic chaos. there is still substantial interest in a matter that is included in many graduate and even undergraduate courses on classical mechanics. based on the hamiltonian formalism, the main objective of this article is to provide, from the physicist's point of view, an overall and intuitive review of this broad subject (with some emphasis on the kam theorem and the stability of planetary motions) which may be useful to both students and instructors.},
archiveprefix = {arxiv},
arxivid = {1012.4384},
author = {masoliver, jaume and ros, ana},
doi = {10.1088/0143-0807/32/2/016},
eprint = {1012.4384},
file = {:users/roy/documents/knowledge/bibliographie/2011/masoliver, ros{\_}integrability and chaos the classical uncertainty.pdf:pdf},
issn = {0143-0807},
journal = {european journal of physics},
month = {mar},
number = {2},
pages = {431--458},
title = {{integrability and chaos: the classical uncertainty}},
volume = {32},
year = {2011}
}
@article{bierig2014,
author = {bierig, claudio and chernov, alexey},
file = {:users/roy/documents/knowledge/bibliographie/2014/bierig, chernov{\_}estimation of arbitrary order central statistical moments by the multilevel monte carlo method.pdf:pdf},
keywords = {differential equation,multilevel monte carlo,random obstacle,rough surface,statistical moments,stochastic partial,uncertainty quantification,variational inequality},
title = {{estimation of arbitrary order central statistical moments by the multilevel monte carlo method}},
year = {2014}
}

@article{iooss2009,
author = {iooss, bertrand},
file = {:users/roy/documents/knowledge/bibliographie/2009/iooss{\_}construction et utilisation de m{\'{e}}tamod{\`{e}}les.pdf:pdf},
title = {{construction et utilisation de m{\'{e}}tamod{\`{e}}les}},
year = {2009}
}
@article{qua,
author = {qu, zening and member, student and hullman, jessica},
file = {:users/roy/documents/knowledge/bibliographie/unknown/qu, member, hullman{\_}keeping multiple views consistent constraints , validations , and exceptions in visualization authoring.pdf:pdf},
title = {{keeping multiple views consistent : constraints , validations , and exceptions in visualization authoring}}
}
@article{chen2017,
abstract = {this tutorial provides a gentle introduction to kernel density estimation (kde) and recent advances regarding confidence bands and geometric/topological features. we begin with a discussion of basic properties of kde: the convergence rate under various metrics, density derivative estimation, and bandwidth selection. then, we introduce common approaches to the construction of confidence intervals/bands, and we discuss how to handle bias. next, we talk about recent advances in the inference of geometric and topological features of a density function using kde. finally, we illustrate how one can use kde to estimate a cumulative distribution function and a receiver operating characteristic curve. we provide r implementations related to this tutorial at the end.},
archiveprefix = {arxiv},
arxivid = {1704.03924},
author = {chen, yen-chi},
eprint = {1704.03924},
file = {:users/roy/documents/knowledge/bibliographie/2017/chen{\_}a tutorial on kernel density estimation and recent advances.pdf:pdf},
keywords = {bootstrap,confidence bands,kernel density estimation,nonparametric statistics},
title = {{a tutorial on kernel density estimation and recent advances}},
year = {2017}
}
@phdthesis{loeven2010,
abstract = {when modeling physical systems, several sources of uncertainty are present. for example, variability in boundary conditions like free stream velocity or ambient pressure are always present. furthermore, uncertainties in geometry arise from production tolerances, wear or unknown deformations under loading. uncertainties in computational fluid dynamics (cfd) simulations can have a significant impact on the computed aerodynamic performance. since cfd simulations are computationally intensive, an efficient uncertainty quantification approach is required.$\backslash$n$\backslash$nthe main objective of this research is to obtain an efficient approach for uncertainty quantification in cfd simulations. this was achieved by focusing on efficient uncertainty propagation and the practical applicability to a wide range of test cases. the probabilistic collocation method was developed as an efficient non-intrusive uncertainty propagation method. it is based on the polynomial chaos framework and shows spectral convergence with respect to the polynomial chaos order. its effectiveness was demonstrated on several flow cases using a commercial cfd solver.$\backslash$n$\backslash$nfor cases with a discontinuous response or involving long time integration, modifications of the probabilistic collocation method were used to efficiently propagate the uncertainties. a multi-element formulation was successfully applied to capture the discontinuous response of a stall flutter problem. furthermore, a time-independent parameterization was used to efficiently propagate uncertainties in case of vortex shedding behind a circular cylinder, which required long time integration.$\backslash$n$\backslash$ngeometric uncertainties were shown to have a significant influence on the aerodynamic performance. since geometric uncertainties affect the shape, a new computational grid should be computed for every collocation point in the probabilistic collocation method. to efficiently treat geometric uncertainties in cfd, a grid deformation technique was used.$\backslash$n$\backslash$nmost cfd simulations in this thesis involved solving the reynolds-averaged navier-stokes equations. this required a turbulence model to close the system of equations. turbulence models often contain several parameters that are tuned to computed or measured simplified flow problems, which introduces uncertainty in the model. uncertainty quantification was applied to the parameters of the k-ǫ turbulence model in combination with wall functions in the cases of flow over a flat plate and flow around a naca0012 airfoil. the drag coefficient showed a coefficient of variation of 3-4{\%} for both cases. the wall function parameters $\kappa$ and c and the model parameter c$\mu$ proved to affect the solution most. general conclusions require more test cases, like a shear layer and an expanding jet.$\backslash$n$\backslash$ncompressor rotors are components of a gasturbine that are highly sensitive to operational and geometrical uncertainties. operational uncertainties like static outlet pressure and the total pressure profile at the inlet of the rotor were considered. the probabilistic collocation method was validated using a monte carlo simulation using 10,000 latin hypercube samples. it was shown that the mass flow was most sensitive to the uncertainty in the total pressure profile at the inlet.$\backslash$n$\backslash$nmultiple uncertainties were shown to be effectively handled using a two-step approach. the first step was a screening of the parameters. a sensitivity analysis was used to identify the most important parameters of the problem. here it was assumed that all parameters are independent and have no combined effects. secondly, the probability density functions of the most important parameters are propagated using the probabilistic collocation method. the probabilistic radial basis function approach was developed as an alternative efficient approach for multiple uncertain parameters. to obtain an accuracy of 0.01-0.001 for the mean and variance, the cfd test cases required 10-35 support points for 3 uncertain parameters. close agreement between the probabilistic radial basis function approach and a monte carlo simulation using 10,000 latin hypercube samples was shown for flow around a rae2822 airfoil with three uncertain parameters.$\backslash$n$\backslash$nit can be concluded that the probabilistic collocation method and adapted versions are capable of efficiently propagating uncertainties in cfd simulations. the development of the probabilistic radial basis function approach provided an efficient alternative for cases with multiple uncertain parameters. from the test cases it became clear that there is not a single method that is most efficient for all possible cases.$\backslash$n$\backslash$nuncertainty quantification increases the reliability of cfd computations, since the effect of uncertain parameters on the output of interest is quantified. it was shown that small coefficients of variation of uncertain parameters can lead to a significant variability of the aerodynamic performance. taking uncertainties into account in cfd simulation is therefore of great importance and with the current state of technology feasible for many real world applications.},
author = {loeven, g. j. a.},
file = {:users/roy/documents/knowledge/bibliographie/2010/loeven{\_}efficient uncertainty quantification in computational fluid dynamics.pdf:pdf},
isbn = {9789088911712},
keywords = {probabilistic stochastic collocation polynomial ch,icient uncertainty,quantification in computational},
pages = {219},
title = {{efficient uncertainty quantification in computational fluid dynamics}},
year = {2010}
}
@article{campolongo2007a,
abstract = {in 1991 morris proposed an effective screening sensitivity measure to identify the few important factors in models with many factors. the method is based on computing for each input a number of incremental ratios, namely elementary effects, which are then averaged to assess the overall importance of the input. despite its value, the method is still rarely used and instead local analyses varying one factor at a time around a baseline point are usually employed. in this piece of work we propose a revised version of the elementary effects method, improved in terms of both the definition of the measure and the sampling strategy. in the present form the method shares many of the positive qualities of the variance-based techniques, having the advantage of a lower computational cost, as demonstrated by the analytical examples. the method is employed to assess the sensitivity of a chemical reaction model for dimethylsulphide (dms), a gas involved in climate change. results of the sensitivity analysis open up the ground for model reconsideration: some model components may need a more thorough modelling effort while some others may need to be simplified. {\textcopyright} 2006 elsevier ltd. all rights reserved.},
author = {campolongo, francesca and cariboni, jessica and saltelli, andrea},
doi = {10.1016/j.envsoft.2006.10.004},
file = {:users/roy/documents/knowledge/bibliographie/2007/campolongo, cariboni, saltelli{\_}an effective screening design for sensitivity analysis of large models(2).pdf:pdf},
isbn = {13648152 (issn)},
issn = {13648152},
journal = {environmental modelling and software},
keywords = {dimethylsulphide (dms),effective sampling strategy,model-free methods,screening problem,sensitivity analysis},
number = {10},
pages = {1509--1518},
title = {{an effective screening design for sensitivity analysis of large models}},
volume = {22},
year = {2007}
}
@article{laurenceau2008,
abstract = {in this paper, we compare the global accuracy of different strategies to build response surfaces by varying sampling methods and modeling techniques. the aerodynamic test functions are obtained by deforming the shape of a transonic airfoil. for comparisons, a robust strategy for model fit using a new efficient initialization technique followed by a gradient optimization was applied. first, a study of different sampling methods proves that including a posteriori information on the function to sample distribution can improve accuracy over classical space-filling methods such as latin hypercube sampling. second, comparing kriging and gradient-enhanced kriging on two- to six-dimensional test cases shows that interpolating gradient vectors drastically improves response-surface accuracy. although direct and indirect cokriging have equivalent formulations, the indirect cokriging outperforms the direct approach.the slow linear phase of error convergencewhen increasing sample size is not avoided by cokriging. thus, the number of samples needed to have a globally accurate surface stays generally out of reach for problems considering more than four design variables.},
author = {laurenceau, j. and sagaut, pierre},
doi = {10.2514/1.32308},
file = {:users/roy/documents/knowledge/bibliographie/2008/laurenceau, sagaut{\_}building efficient response surfaces of aerodynamic functions with kriging and cokriging.pdf:pdf},
issn = {0001-1452},
journal = {aiaa journal},
number = {2},
pages = {498--507},
title = {{building efficient response surfaces of aerodynamic functions with kriging and cokriging}},
volume = {46},
year = {2008}
}
@inproceedings{ponweiser2008,
abstract = {surrogate model-based optimization is a well-known technique for optimizing expensive black-box functions. by applying this function approximation, the number of real problem evaluations can be reduced because the optimization is performed on the model. in this case two contradictory targets have to be achieved: increasing global model accuracy and exploiting potentially optimal areas. the key to these targets is the criterion for selecting the next point, which is then evaluated on the expensive black-box function - the dasiainfill sampling criterionpsila. therefore, a novel approach - the dasiaclustered multiple generalized expected improvementpsila (cmgei) - is introduced and motivated by an empirical study. furthermore, experiments benchmarking its performance compared to the state of the art are presented.},
author = {ponweiser, wolfgang and wagner, tobias and vincze, markus},
booktitle = {2008 ieee congress on evolutionary computation (ieee world congress on computational intelligence)},
doi = {10.1109/cec.2008.4631273},
file = {:users/roy/documents/knowledge/bibliographie/2008/ponweiser, wagner, vincze{\_}clustered multiple generalized expected improvement a novel infill sampling criterion for surrogate models.pdf:pdf},
isbn = {978-1-4244-1822-0},
issn = {978-1-4244-1822-0},
month = {jun},
pages = {3515--3522},
publisher = {ieee},
title = {{clustered multiple generalized expected improvement: a novel infill sampling criterion for surrogate models}},
year = {2008}
}
@article{wang2016,
abstract = {an important task of uncertainty quantification is to identify the probability of undesired events, in particular, system failures, caused by various sources of uncertainties. in this work we consider the construction of gaussian process surrogates for failure detection and failure probability estimation. in particular, we consider the situation that the underlying computer models are extremely expensive, and in this setting, determining the sampling points in the state space is of essential importance. we formulate the problem as an optimal experimental design for bayesian inferences of the limit state (i.e., the failure boundary) and propose an efficient numerical scheme to solve the resulting optimization problem. in particular, the proposed limit-state inference method is capable of determining multiple sampling points at a time, and thus it is well suited for problems where multiple computer simulations can be performed in parallel. the accuracy and performance of the proposed method is demonstrated by both academic and practical examples.},
archiveprefix = {arxiv},
arxivid = {1509.04613},
author = {wang, hongqiao and lin, guang and li, jinglai},
doi = {10.1016/j.jcp.2016.02.053},
eprint = {1509.04613},
file = {:users/roy/documents/knowledge/bibliographie/2016/wang, lin, li{\_}gaussian process surrogates for failure detection a bayesian experimental design approach.pdf:pdf},
issn = {10902716},
journal = {journal of computational physics},
keywords = {bayesian inference,experimental design,failure detection,gaussian processes,monte carlo,response surfaces,uncertainty quantification},
pages = {247--259},
publisher = {elsevier inc.},
title = {{gaussian process surrogates for failure detection: a bayesian experimental design approach}},
volume = {313},
year = {2016}
}
@article{huang2013,
author = {Huang, Likeng and Gao, Zhenghong and Zhang, Dehu},
doi = {10.1016/j.cja.2013.02.004},
file = {:users/roy/documents/knowledge/bibliographie/2013/huang, gao, zhang{\_}research on multi-fidelity aerodynamic optimization methods.pdf:pdf},
issn = {10009361},
journal = {Chinese journal of aeronautics},
keywords = {aerodynamics,co-kriging,multi-fidelity,optimization,surrogate model},
month = {apr},
number = {2},
pages = {279--286},
publisher = {chinese society of aeronautics and astronautics},
title = {{Research on multi-fidelity aerodynamic optimization methods}},
volume = {26},
year = {2013}
}
@phdthesis{zertuche2015,
author = {Zertuche, Federico},
file = {:users/roy/documents/knowledge/bibliographie/2015/zertuche{\_}utilisation de simulateurs multi-fid{\'{e}}lit{\'{e}} pour les {\'{e}}tudes d'incertitudes dans les codes de calcul.pdf:pdf},
school = {universit{\'{e}} de grenoble},
title = {{Utilisation de simulateurs multi-fid{\'{e}}lit{\'{e}} pour les {\'{e}}tudes d'incertitudes dans les codes de calcul}},
year = {2015}
}
@article{ng2014,
author = {Ng, Leo W. T. and Willcox, Karen E.},
doi = {10.1002/nme.4761},
file = {:users/roy/documents/knowledge/bibliographie/2014/ng, willcox{\_}multifidelity approaches for optimization under uncertainty.pdf:pdf},
issn = {00295981},
journal = {International journal for numerical methods in engineering},
keywords = {multi-fidelity optimization,design under uncertainty,model reduction,multifidelity,ods,optimization,probabilistic meth-,probabilistic methods,stochastic problems,stochastic problems"},
month = {dec},
number = {10},
pages = {746--772},
title = {{Multifidelity approaches for optimization under uncertainty}},
volume = {100},
year = {2014}
}
@article{fossati2015,
abstract = {centroidal voronoi tessellation, leave-one-out cross validation, proper orthogonal decomposition, and multidimensional interpolation are integrated to define a reduced-order modeling approach for the parametric evaluation of steady aerodynamic loads. the proper orthogonal decomposition-based methodology allows reducing the number of degrees of freedom of the problem while maintaining good accuracy for the solution of complex three-dimensional viscous turbulent flows. as a result, it yields fairly accurate solutions at a fraction of the time required by standard computational fluid dynamics approaches. three-dimensional examples for fixed- and rotary-wing cases of industrial relevance are used to assess the method in the cases of subsonic and transonic flow conditions.},
author = {Fossati, Marco},
doi = {10.2514/1.j053755},
file = {:users/roy/documents/knowledge/bibliographie/2015/fossati{\_}evaluation of aerodynamic loads via reduced-order methodology.pdf:pdf},
issn = {0001-1452},
journal = {{AIAA} journal},
number = {8},
pages = {1--17},
title = {{Evaluation of aerodynamic loads via reduced-order methodology}},
volume = {53},
year = {2015}
}
@article{efron1977,
author = {Efron, Bradley and Morris, Carl},
file = {:users/roy/documents/knowledge/bibliographie/1977/efron, morris{\_}stein's paradox in statistics.pdf:pdf},
title = {{Stein's paradox in statistics}},
year = {1977}
}
@article{wilkinson1999,
abstract = {dot plots represent individual observations in a batch of data with$\backslash$nsymbols, usually circular dots. they have been used for more than$\backslash$n100 years to depict distributions in detail. hand-drawn examples$\backslash$nshow their authors' efforts to arrange symbols so that they are as$\backslash$nnear as possible to their proper locations on a scale without overlapping$\backslash$nenough to obscure each other. recent computer programs that attempt$\backslash$nto reproduce these historical plots have unfortunately resorted to$\backslash$nsimple histogram binning instead of using methods that follow the$\backslash$nrules for the hand-drawn examples. this article introduces an algorithm$\backslash$nthat more accurately represents the dot plots cited in the literature.},
author = {Wilkinson, Leland},
doi = {10.2307/2686111},
file = {:users/roy/documents/knowledge/bibliographie/1999/wilkinson{\_}dot plots.pdf:pdf},
isbn = {0003-1305},
issn = {00031305},
journal = {The american statistician},
keywords = {dotplot,graphics,histogram,kernel density estimation},
number = {3},
pages = {276},
title = {{Dot plots}},
volume = {53},
year = {1999}
}
@article{price1763,
author = {Price, Richard},
file = {:users/roy/documents/knowledge/bibliographie/1763/price{\_}an essay towards solving a problem in the doctrine of chances.pdf:pdf},
journal = {philosophical transactions of the royal society of london},
pages = {370--418},
title = {{An essay towards solving a problem in the doctrine of chances}},
volume = {53},
year = {1763}
}
@article{greene,
author = {greene, c and woodward, r and pal, s and santoro, r},
file = {:users/roy/documents/knowledge/bibliographie/unknown/greene et al.{\_}design and study of a lox-gh2 swirl injector for rocket applications.pdf:pdf},
title = {{design and study of a lox-gh2 swirl injector for rocket applications}}
}
@article{rao2017,
abstract = {0 0 m o n t h 2 0 1 7 | v o l 0 0 0 | n a t u r e | 1 1,2 . one strategy aims for electrochemical conversions powered by electricity from renewable sources 3–5 , but photochemical approaches driven by sunlight are also conceivable 6 . a considerable challenge in both approaches is the development of efficient and selective catalysts, ideally based on cheap and earth-abundant elements rather than expensive precious metals 7 . of the molecular photo-and electrocatalysts reported, only a few catalysts are stable and selective for co 2 reduction; moreover, these catalysts produce primarily co or hcooh, and catalysts capable of generating even low to moderate yields of highly reduced hydrocarbons remain rare 8–17 . here we show that an iron tetraphenylporphyrin complex functionalized with trimethylammonio groups, which is the most efficient and selective molecular electro-catalyst for converting co 2 to co known 18–20 , can also catalyse the eight-electron reduction of co 2 to methane upon visible light irradiation at ambient temperature and pressure. we find that the catalytic system, operated in an acetonitrile solution containing a photosensitizer and sacrificial electron donor, operates stably over several days. co is the main product of the direct co 2 photoreduction reaction, but a two-pot procedure that first reduces co 2 and then reduces co generates methane with a selectivity of up to 82 per cent and a quantum yield (light-to-product efficiency) of 0.18 per cent. however, we anticipate that the operating principles of our system may aid the development of other molecular catalysts for the production of solar fuels from co 2 under mild conditions. iron tetraphenylporphyrins electrochemically reduced to the fe 0 species have been shown to be the most efficient molecular catalysts for the co 2 -to-co conversion 18,19 . the nucleophilic fe centre binds to co 2 and the fe–co 2 adduct is further protonated and reduced to afford co upon cleavage of one c–o bond. substitution of the four para-phenyl hydrogens by trimethylammonio groups 18,20 (fe-p-tma 1, table 1) led to co formation with selectivity close to 95{\%} in water at ph 7 as well as in aprotic solvent such as n,n-dimethylformamide (dmf), at low overpotentials and with excellent stability (1 day). the fact that the standard redox potential of the fe i},
author = {rao, heng and schmidt, luciana c. and bonin, julien and robert, marc},
doi = {10.1038/nature23016},
file = {:users/roy/documents/knowledge/bibliographie/2017/rao et al.{\_}visible-light-driven methane formation from co2 with a molecular iron catalyst.pdf:pdf},
issn = {0028-0836},
journal = {nature},
number = {7665},
pages = {74--77},
publisher = {nature publishing group},
title = {{visible-light-driven methane formation from co2 with a molecular iron catalyst}},
volume = {548},
year = {2017}
}
@article{gershon1992,
abstract = {methods are presented for the visualization of fuzzy data based on the sensitivity of the human visual system to motion and dynamic changes, and the ease of which electronic display devices can change their display. the methods include taking an otherwise static image and displaying in an animation loop either its segmented components or a series of blurred versions of the whole image. this approach was applied to sea-surface temperature data and was found to be effective in showing fuzzy details embedded in the data, and in drawing the viewer's attention. this approach and these methods could play a significant role in the display of browse products for massive data and information systems},
author = {gershon, n d},
doi = {10.1109/visual.1992.235199},
file = {:users/roy/documents/knowledge/bibliographie/1992/gershon{\_}visualization of fuzzy data using generalized animation.pdf:pdf},
isbn = {0-8186-2897-9},
journal = {ieee conference on visualization},
keywords = {animation,computer displays,computer vision,data visualization,drives,fuzzy sets,fuzzy systems,motion detection,shape,visual system,visualization,blurred versions,browse products,computer animation,data analysis,data visualisation,dynamic changes,electronic display devices,fuzzy data,generalized animation,human visual system,information systems,massive data systems,motion,sea-surface temperature data,segmented components,static image},
pages = {268--273},
title = {{visualization of fuzzy data using generalized animation}},
year = {1992}
}
@inproceedings{schroeder2014,
abstract = {the development of the aerodynamic database for the space launch system (sls) booster separation environment has presented many challenges because of the complex physics of the flow around three independent bodies due to proximity effects and jet inter- actions from the booster separation motors and the core stage engines. this aerodynamic environment is difficult to simulate in a wind tunnel experiment and also difficult to simu- late with computational fluid dynamics. the database is further complicated by the high dimensionality of the independent variable space, which includes the orientation of the core stage, the relative positions and orientations of the solid rocket boosters, and the thrust lev- els of the various engines. moreover, the clearance between the core stage and the boosters during the separation event is sensitive to the aerodynamic uncertainties of the database. this paper will present the development process for version 3 of the sls booster separa- tion aerodynamic database and the statistics-based uncertainty quantification process for the database.},
address = {reston, virginia},
author = {chan, david t. and dalle, derek j. and rogers, stuart e. and pinier, jeremy t. and wilcox, floyd j. and gomez, reynaldo j.},
booktitle = {54th aiaa aerospace sciences meeting},
doi = {10.2514/6.2016-0798},
file = {:users/roy/documents/knowledge/bibliographie/2016/chan et al.{\_}space launch system booster separation aerodynamic database development and uncertainty quantification.pdf:pdf},
isbn = {978-1-62410-393-3},
month = {jan},
pages = {206--242},
publisher = {american institute of aeronautics and astronautics},
title = {{space launch system booster separation aerodynamic database development and uncertainty quantification}},
year = {2016}
}
@article{herman2013,
abstract = {the increase in spatially distributed hydrologic modeling warrants a corresponding increase in diagnos- tic methods capable of analyzing complex models with large numbers of parameters. sobol? sensitivity analysis has proven to be a valuable tool for diagnostic analyses of hy- drologic models. however, for many spatially distributed models, the sobol? method requires a prohibitive number of model evaluations to reliably decompose output variance across the full set of parameters.we investigate the potential of the method of morris, a screening-based sensitivity ap- proach, to provide results sufficiently similar to those of the sobol? method at a greatly reduced computational expense. the methods are benchmarked on the hydrology laboratory research distributed hydrologic model (hl-rdhm) over a six-month period in the blue river watershed, oklahoma, usa. the sobol? method required over six million model evaluations to ensure reliable sensitivity indices, correspond- ing to more than 30 000 computing hours and roughly 180 gigabytes of storage space.we find that the method of mor- ris is able to correctly screen the most and least sensitive parameters with 300 times fewer model evaluations, requir- ing only 100 computing hours and 1 gigabyte of storage space. the method of morris proves to be a promising di- agnostic approach for global sensitivity analysis of highly parameterized, spatially distributed hydrologic models.},
author = {herman, j. d. and kollat, j. b. and reed, p. m. and wagener, t.},
doi = {10.5194/hess-17-2893-2013},
file = {:users/roy/documents/knowledge/bibliographie/2013/herman et al.{\_}technical note method of morris effectively reduces the computational demands of global sensitivity analysis for distribut.pdf:pdf},
isbn = {1607-7938},
issn = {16077938},
journal = {hydrology and earth system sciences},
number = {7},
pages = {2893--2903},
title = {{technical note: method of morris effectively reduces the computational demands of global sensitivity analysis for distributed watershed models}},
volume = {17},
year = {2013}
}
@article{breunig2000,
abstract = {for many kdd applications, such as detecting criminal activities in e-commerce, finding the rare instances or the outliers, can be more interesting than finding the common patterns. existing work in out- lier detection regards being an outlier as a binary property. in this paper, we contend that for many scenarios, it is more meaningful to assign to each object a degree of being an outlier. this degree is called the local outlier factor (lof) of an object. it is local in that the degree depends on how isolated the object is with respect to the surrounding neighborhood. we give a detailed formal analysis showing that lof enjoys many desirable properties. using real- world datasets, we demonstrate that lof can be used to find outliers which appear to be meaningful, but can otherwise not be identified with existing approaches. finally, a careful performance evaluation of our algorithm confirms we show that our approach of finding lo- cal outliers can be practical.},
author = {breunig, markus m. and kriegel, hans-peter and ng, raymond t. and sander, j{\"{o}}rg},
doi = {10.1145/335191.335388},
file = {:users/roy/documents/knowledge/bibliographie/2000/breunig et al.{\_}lof identifying density-based local outliers.pdf:pdf},
isbn = {1581132182},
issn = {01635808},
journal = {proceedings of the 2000 acm sigmod international conference on management of data},
keywords = {database mining,outlier detection},
pages = {1--12},
title = {{lof: identifying density-based local outliers}},
year = {2000}
}
@article{ibrekk1987,
abstract = {nine pictorial displays for communicating quantitative information about the value of an uncertain quantity, x, were evaluated for their ability to communicate 2, p(x {\textgreater} a) and p( b {\textgreater} x {\textgreater} a) to well-educated semi- and nontechnical subjects. different displays performed best in different applications. cumulative distribution functions alone can severely mislead some subjects in estimating the mean. a “rusty” knowledge of statistics did not improve performance, and even people with a good basic knowledge of statistics did not perform as well as one would like. until further experiments are performed, the authors recommend the use of a cumulative distribution function plotted directly above a probability density function with the same horizontal scale, and with the location of the mean clearly marked on both curves.},
author = {ibrekk, harald and morgan, m granger},
file = {:users/roy/documents/knowledge/bibliographie/1987/ibrekk, morgan{\_}graphical communication of uncertain quantities to nontechnical people.pdf:pdf},
journal = {risk analysis},
keywords = {graphical communication,risk communication,uncertainty},
number = {4},
title = {{graphical communication of uncertain quantities to nontechnical people}},
volume = {7},
year = {1987}
}
@article{stipcevic,
abstract = {random numbers are needed in many areas: cryptography,monte carlo computation and simulation, industrial testing and labeling, hazard games, gambling, etc. our assumption has been that random numbers cannot be computed; because computers operate in deterministic way, they cannot produce random numbers. instead, random numbers are best obtained using physical (true) random number generators (trng), which operate by mea- suring a well controlled and specially prepared physical process. randomness of a trng can be precisely, scientifically characterized and measured. es- pecially valuable are the information-theoretic provable rngs, which, at the state of the art, seem to be possible only by exploiting randomness inherent to certain quantum systems. on the other hand, current industry standards dictate the use of rngs based on free running oscillators (fro) whose ran- domness is derived from electronic noise present in logic circuits and which cannot be strictly proven as uniformly random, but offer easier technological realization. the fro approach is currently used in 3rd and 4th generation fpga and asic hardware, unsuitable for realization of quantum rngs. in this chapter we compare weak and strong aspects of the two approaches. fi- nally, we discuss several examples where use of a true rng is critical and show how it can significantly improve security of cryptographic systems, and discuss industrial and research challenges that prevent widespread use of trngs.},
author = {stipcevic, mario and {kaya ko{\c{c}}}, {\c{c}}etin},
file = {:users/roy/documents/knowledge/bibliographie/unknown/stipcevic, kaya ko{\c{c}}{\_}true random number generators.pdf:pdf},
pages = {1--45},
title = {{true random number generators}}
}
@incollection{cavazzuti2013,
author = {cavazzuti, marco},
booktitle = {optimization methods: from theory to design},
doi = {10.1007/978-3-642-31187-1_2},
file = {:users/roy/documents/knowledge/bibliographie/2013/cavazzuti{\_}design of experiments.pdf:pdf},
pages = {13--42},
publisher = {springer berlin heidelberg},
title = {{design of experiments}},
year = {2013}
}
@article{smirnov2001,
abstract = {abstract a random flow generation (rfg) technique is presented, which can be used for initial/inlet boundary generation in les (large-eddy-simulations) or particle tracking in les/rans (reynolds-averaged navier-stokes) computations of turbulent flows. the ...},
author = {smirnov, a. and shi, s. and celik, i.},
doi = {10.1115/1.1369598},
file = {:users/roy/documents/knowledge/bibliographie/2001/smirnov, shi, celik{\_}random flow generation technique for large eddy simulations and particle-dynamics modeling.pdf:pdf},
isbn = {0098-2202},
issn = {00982202},
journal = {journal of fluids engineering},
number = {2},
pages = {359},
title = {{random flow generation technique for large eddy simulations and particle-dynamics modeling}},
volume = {123},
year = {2001}
}
@inproceedings{mycek2017,
author = {mycek, paul and lozzo, matthias de and roy, pamphile t. and ricci, sophie},
booktitle = {cemracs},
file = {:users/roy/documents/knowledge/bibliographie/2017/mycek et al.{\_}multilevel monte carlo estimation of covariances in the context of open-channel flow simulations.pdf:pdf},
title = {{multilevel monte carlo estimation of covariances in the context of open-channel flow simulations}},
year = {2017}
}
@article{chevalier2014,
abstract = {stepwise uncertainty reduction (sur) strategies aim at$\backslash$nconstructing a sequence of points for evaluating a function f in$\backslash$nsuch a way that the residual uncertainty about a quantity of$\backslash$ninterest progressively decreases to zero. using such strategies$\backslash$nin the framework of gaussian process modeling has been shown to$\backslash$nbe efficient for estimating the volume of excursion of f above a$\backslash$nfixed threshold. however, sur strategies remain cumbersome to use$\backslash$nin practice because of their high computational complexity, and$\backslash$nthe fact that they deliver a single point at each iteration. in$\backslash$nthis article we introduce several multipoint sampling criteria,$\backslash$nallowing the selection of batches of points at which f can be$\backslash$nevaluated in parallel. such criteria are of particular interest$\backslash$nwhen f is costly to evaluate and several cpus are simultaneously$\backslash$navailable. we also manage to drastically reduce the computational$\backslash$ncost of these strategies through the use of closed form formulas.$\backslash$nwe illustrate their performances in various numerical$\backslash$nexperiments, including a nuclear safety test case. basic notions$\backslash$nabout kriging, auxiliary problems, complexity calculations, r$\backslash$ncode, and data are available online as supplementary materials.},
author = {chevalier, cl{\'{e}}ment and bect, julien and ginsbourger, david and vazquez, emmanuel and picheny, victor and richet, yann},
doi = {10.1080/00401706.2013.860918},
file = {:users/roy/documents/knowledge/bibliographie/2014/chevalier et al.{\_}fast parallel kriging-based stepwise uncertainty reduction with application to the identification of an excursion set.pdf:pdf},
issn = {0040-1706},
journal = {technometrics},
month = {oct},
number = {4},
pages = {455--465},
title = {{fast parallel kriging-based stepwise uncertainty reduction with application to the identification of an excursion set}},
volume = {56},
year = {2014}
}
@article{legratiet2012,
abstract = {in many practical cases, a sensitivity analysis or an optimization of a complex time consuming computer code requires to build a fast running approximation of it - also called surrogate model. we consider in this paper the problem of building a surrogate model of a complex computer code which can be run at different levels of accuracy. the co-kriging based surrogate model is a promising tool to build such an approximation. the idea is to improve the surrogate model by using fast and less accurate versions of the code. we present here a new approach to perform a multi-fidelity co-kriging model which is based on a recursive formulation. the strength of this new method is that the co-kriging model is built through a series of independent kriging models. from them, some properties of classical kriging models can naturally be extended to the presented co-kriging model such as a fast cross-validation procedure. moreover, based on a bayes linear formulation, an extension of the universal kriging equations are provided for the co-kriging model. finally, the proposed model has the advantage to reduce the computational complexity compared to the previous models. the multi-fidelity model is successfully applied to emulate a hydrodynamic simulator. this real example illustrates the efficiency of the recursive model.},
archiveprefix = {arxiv},
arxivid = {1210.0686},
author = {{Le Gratiet}, Loic},
eprint = {1210.0686},
file = {:users/roy/documents/knowledge/bibliographie/2012/le gratiet{\_}recursive co-kriging model for design of computer experiments with multiple levels of fidelity with an application to hydrody.pdf:pdf},
journal = {International journal for uncertainty quantification},
keywords = {cross-validation,fast,multi-fidelity computer code,recursive model,surrogate models,uncertainty quantification,universal co-kriging},
month = {oct},
number = {5},
pages = {365--386},
title = {{Recursive co-kriging model for design of computer experiments with multiple levels of fidelity with an application to hydrodynamic}},
volume = {4},
year = {2012}
}

@incollection{girard2005,
abstract = {with the gaussian process model, the predictive distribu- tion of the output corresponding to a new given input is gaussian. but if this input is uncertain or noisy, the predictive distribution becomes non-gaussian. we present an analytical approach that consists of com- puting only the mean and variance of this new distribution (gaussian approximation). we show how, depending on the form of the covariance function of the process, we can evaluate these moments exactly or ap- proximately (within a taylor approximation of the covariance function). we apply our results to the iterative multiple-step ahead prediction of non-linear dynamic systems with propagation of the uncertainty as we predict ahead in time. finally, using numerical examples, we compare the gaussian approximation to the numerical approximation of the true predictive distribution by simple monte-carlo.},
author = {girard, agathe and murray-smith, roderick},
booktitle = {switching and learning in feedback systems},
doi = {10.1007/978-3-540-30560-6_7},
file = {:users/roy/documents/knowledge/bibliographie/2005/girard, murray-smith{\_}gaussian processes prediction at a noisy input and application to iterative multiple-step ahead forecasting of time.pdf:pdf},
pages = {158--184},
title = {{gaussian processes: prediction at a noisy input and application to iterative multiple-step ahead forecasting of time-series}},
volume = {2},
year = {2005}
}
@article{mihalas2015a,
abstract = {some preliminary results of a project aiming to develop tools for$\backslash$r$\backslash$nadding sound associated to medical data are presented. the description$\backslash$r$\backslash$nof our sonification procedure is followed by two different examples.$\backslash$r$\backslash$nthe first refers to monitoring the heart rate (hr) during exercise,$\backslash$r$\backslash$neither in clinical settings or in self monitoring conditions. the$\backslash$r$\backslash$nsecond example is an application from molecular biology / cellular$\backslash$r$\backslash$nkinetics, for analysis of protein-protein interaction, with a specific$\backslash$r$\backslash$nreference to a computer simulation of p53 ��� mdm2 interaction, which$\backslash$r$\backslash$nexhibits, under certain conditions, an oscillatory behavior. pending$\backslash$r$\backslash$nissues and future work are finally discussed.},
author = {mihalas, george and popescu, lucian and andor, minodora and paralescu, sorin and tudor, anca and neagu, adrian},
file = {:users/roy/documents/knowledge/bibliographie/2015/mihalas et al.{\_}adding sound to medical data representation.pdf:pdf},
journal = {proceedings of the 21th international conference on auditory display (icad2015)},
pages = {325--326},
title = {{adding sound to medical data representation}},
year = {2015}
}
@article{cordier2006,
abstract = {la d{\'{e}}termination num{\'{e}}rique d'une solution (m{\^{e}}me approch{\'{e}}e) d'un processus physique non lin{\'{e}}aire et complexe (par sa g{\'{e}}om{\'{e}}trie ou son caract{\`{e}}re multiphysique par exemple) n{\'{e}}cessite encore de nos jours des ressources informatiques cons{\'{e}}quentes, que ce soit en temps de calcul ou en occupation m{\'{e}}moire. pour fixer les ordres de grandeur d'un probl{\`{e}}me r{\'{e}}aliste, spalart et al. (1997) estiment que pour une voilure d'avion aux conditions de vol de croisi{\`{e}}re i.e. pour re = o(107), il est n{\'{e}}cessaire d'employer environ 1011 points et d'int{\'{e}}grer les {\'{e}}quations de navier-stokes sur environ 5 106 pas de temps. il para{\^{i}}t donc d{\'{e}}licat, avec les approches num{\'{e}}riques utilis{\'{e}}es actuellement en m{\'{e}}canique des fluides ({\'{e}}l{\'{e}}ments finis, volumes finis, . . .), de pouvoir aborder de mani{\`{e}}re courante des applications pour lesquelles, soit un grand nombre de r{\'{e}}solutions du syst{\`{e}}me d'{\'{e}}tat est n{\'{e}}cessaire (m{\'{e}}thodes de continuation, {\'{e}}tudes param{\'{e}}triques, probl{\`{e}}mes d'optimisation ou de contr{\^{o}}le optimal, . . .), soit une solution en temps r{\'{e}}el est recherch{\'{e}}e (contr{\^{o}}le actif en boucle ferm{\'{e}}e par exemple). id{\'{e}}alement, nous aimerions ramener le nombre important de degr{\'{e}}s de libert{\'{e}} g{\'{e}}n{\'{e}}ralement n{\'{e}}cessaire {\`{a}} la description dynamique du syst{\`{e}}me physique (108 points de grille, par exemple, dans le cas d'un {\'{e}}coulement de canal pourtant acad{\'{e}}mique analys{\'{e}} r{\'{e}}cemment par bewley, 2001) {\`{a}} quelques degr{\'{e}}s de libert{\'{e}} en interaction effective. ainsi, en acceptant de payer le co{\^{u}}t d'une (ou plusieurs) r{\'{e}}solutions du mo- d{\`{e}}le d{\'{e}}taill{\'{e}}, nous pourrions par la suite, pour le m{\^{e}}me co{\^{u}}t num{\'{e}}rique, r{\'{e}}aliser un tr{\`{e}}s grand nombre de simulations du mod{\`{e}}le r{\'{e}}duit de dynamique.},
author = {cordier, l and bergmann, m},
file = {:users/roy/documents/knowledge/bibliographie/2006/cordier, bergmann{\_}r{\'{e}}duction de dynamique par d{\'{e}}composition orthogonale aux valeurs propres (pod).pdf:pdf},
journal = {ecole de printemps ocet},
title = {{r{\'{e}}duction de dynamique par d{\'{e}}composition orthogonale aux valeurs propres (pod)}},
volume = {7563},
year = {2006}
}
@article{chatterjee2007,
abstract = {the anscombe dataset is popular for teaching the importance of graphics in data analysis. it consists of four datasets that have identical summary statistics (e.g., mean, standard deviation, and correlation) but dissimilar data graphics (scatterplots). in this article,we provide a general procedure to generate datasetswith identical summary statistics but dissimilar graphics by using a genetic algorithm based approach.},
author = {chatterjee, sangit and firat, aykut},
doi = {10.1198/000313007x220057},
file = {:users/roy/documents/knowledge/bibliographie/2007/chatterjee, firat{\_}generating data with identical statistics but dissimilar graphics a follow up to the anscombe dataset.pdf:pdf},
journal = {the american statistician},
keywords = {genetic algorithms,nonlinear optimization,ortho-normalization},
number = {3},
pages = {248--254},
title = {{generating data with identical statistics but dissimilar graphics : a follow up to the anscombe dataset}},
volume = {61},
year = {2007}
}
@book{cavazzuti2013a,
address = {berlin, heidelberg},
annote = {optimization not based on the surrogates variance.

classical algorithms review.},
author = {cavazzuti, marco},
doi = {10.1007/978-3-642-31187-1},
file = {:users/roy/documents/knowledge/bibliographie/2013/cavazzuti{\_}optimization methods.pdf:pdf},
isbn = {978-3-642-31186-4},
publisher = {springer berlin heidelberg},
title = {{optimization methods}},
year = {2013}
}
@article{zhou2011,
abstract = {we propose a flexible yet computationally efficient approach for building gaussian process models for computer experiments with both qualitative and quantitative factors. this approach uses the hypersphere parameterization to model the correlations of the qualitative factors, thus avoiding the need of directly solving optimization problems with positive definite constraints. the effectiveness of the proposed method is successfully illustrated by several examples.},
author = {zhou, qiang and qian, peter z. g. and zhou, shiyu},
doi = {10.1198/tech.2011.10025},
file = {:users/roy/documents/knowledge/bibliographie/2011/zhou, qian, zhou{\_}a simple approach to emulation for computer models with qualitative and quantitative factors.pdf:pdf},
isbn = {10.1198/tech.2011.10025},
issn = {0040-1706},
journal = {technometrics},
keywords = {computer experiment,hypersphere decomposition,kriging},
number = {3},
pages = {266--273},
title = {{a simple approach to emulation for computer models with qualitative and quantitative factors}},
volume = {53},
year = {2011}
}
@article{anscombe1973,
author = {Anscombe, F. J.},
doi = {10.2307/2682899},
file = {:users/roy/documents/knowledge/bibliographie/1973/anscombe{\_}graphs in statistical analysis.pdf:pdf},
journal = {the american statistician},
number = {1},
pages = {17--21},
title = {{graphs in statistical analysis}},
volume = {27},
year = {1973}
}
@article{kennedy2000,
annote = {null},
author = {kennedy, marc c and o'hagan, anthony},
file = {:users/roy/documents/knowledge/bibliographie/2000/kennedy, o'hagan{\_}predicting the output from a complex computer code when fast approximations are available(2).pdf:pdf},
journal = {biometrika},
number = {1},
pages = {1--13},
title = {{predicting the output from a complex computer code when fast approximations are available}},
volume = {87},
year = {2000}
}


@inproceedings{baudin2016,
address = {r{\'{e}}union island},
author = {baudin, micha{\"{e}}l and boumhaout, khalid and delage, thibault and iooss, bertrand and martinez, jean-marc},
booktitle = {8th international conference on sensitivity analysis of model output,},
file = {:users/roy/documents/knowledge/bibliographie/2016/baudin et al.{\_}numerical stability of sobol' indices estimation formula.pdf:pdf},
title = {{numerical stability of sobol' indices estimation formula}},
year = {2016}
}

@phdthesis{legratiet2013,
author = {{Le Gratiet}, Loic},
file = {:users/roy/documents/knowledge/bibliographie/2013/le gratiet{\_}multi-fidelity gaussian process regression for computer experiments.pdf:pdf},
school = {Universit{\'{e}} de paris-diderot},
title = {{Multi-fidelity gaussian process regression for computer experiments}},
year = {2013}
}
@article{cutajar2016,
abstract = {the composition of multiple gaussian processes as a deep gaussian process (dgp) enables a deep probabilistic nonparametric approach to flexibly tackle complex machine learning problems with sound quantification of uncertainty. existing inference approaches for dgp models have limited scalability and are notoriously cumbersome to construct. in this work, we introduce a novel formulation of dgps based on random feature expansions that we train using stochastic variational inference. this yields a practical learning framework which significantly advances the state-of-the-art in inference for dgps, and enables accurate quantification of uncertainty. we extensively showcase the scalability and performance of our proposal on several datasets with up to 8 million observations, and various dgp architectures with up to 30 hidden layers.},
archiveprefix = {arxiv},
arxivid = {1610.04386},
author = {cutajar, kurt and bonilla, edwin v. and michiardi, pietro and filippone, maurizio},
eprint = {1610.04386},
file = {:users/roy/documents/knowledge/bibliographie/2016/cutajar et al.{\_}random feature expansions for deep gaussian processes.pdf:pdf},
journal = {arxiv preprint},
month = {oct},
title = {{random feature expansions for deep gaussian processes}},
year = {2016}
}
@book{montomoli2015,
author = {montomoli, francesco and carnevale, mauro and d'ammaro, antonio and massini, michela and salvadori, simone},
booktitle = {springer international publishing},
doi = {10.1007/978-3-319-00885-1},
file = {:users/roy/documents/knowledge/bibliographie/2015/montomoli et al.{\_}uncertainty quantification in computational fluid dynamics and aircraft engines.pdf:pdf},
isbn = {9783319146805},
title = {{uncertainty quantification in computational fluid dynamics and aircraft engines}},
volume = {92},
year = {2015}
}
@inproceedings{pellegrini2016,
abstract = {a multi-fidelity global metamodel is presented for uncertainty quantification of com- putationally expensive simulations. the multi-fidelity approximation is built as the sum of a low-fidelity-trained metamodel and the metamodel of the difference (error) between high- and low-fidelity simulations. the metamodel is based on dynamic stochastic radial basis functions, which provide the prediction along with the associated uncertainty. new training points are added where the prediction uncertainty is largest, according to an adaptive sampling procedure. the prediction uncertainty of both the low-fidelity and the error metamodel are considered for the adaptive training of the low- and high-fidelity metamodels, respectively. the method is ap- plied to a steady fluid-structure interaction (fsi) problem of a 3d naca 0009 stainless steel hydrofoil. two functions are considered simultaneously, namely lift and drag coefficients, ver- sus angle of attack and reynolds number. two problems are presented: in the first problem the high-fidelity evaluations are obtained through steady fsi computer simulations, whereas in the second problem they are given by available experimental data from literature. low-fidelity eval- uations are provided in both cases by steady hydrodynamic simulations. the overall uncertainty of the multi-fidelity metamodel is used as a convergence criterion.},
address = {crete island},
author = {pellegrini, riccardo and leotardi, cecilia and iemma, umberto and campana, emilio f. and diez, matteo},
booktitle = {european congress on computational methods in applied sciences and engineering},
file = {:users/roy/documents/knowledge/bibliographie/2016/pellegrini et al.{\_}a multi-fidelity adaptive sampling method for metamodel-based uncertainty quantification of computer simulations.pdf:pdf},
keywords = {adaptive sampling method,fluid-structure interaction,multi-fidelity metamodel,uncertainty quantification},
number = {june},
title = {{a multi-fidelity adaptive sampling method for metamodel-based uncertainty quantification of computer simulations}},
year = {2016}
}
@inproceedings{szegedy2014,
archiveprefix = {arxiv},
arxivid = {1409.4842},
author = {szegedy, christian and {wei liu} and {yangqing jia} and sermanet, pierre and reed, scott and anguelov, dragomir and erhan, dumitru and vanhoucke, vincent and rabinovich, andrew},
booktitle = {2015 ieee conference on computer vision and pattern recognition (cvpr)},
doi = {10.1109/cvpr.2015.7298594},
eprint = {1409.4842},
file = {:users/roy/documents/knowledge/bibliographie/2015/szegedy et al.{\_}going deeper with convolutions.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {10636919},
month = {jun},
pages = {1--9},
pmid = {24920543},
publisher = {ieee},
title = {{going deeper with convolutions}},
year = {2015}
}
@article{toal2015,
abstract = {surrogate models or metamodels are commonly used to exploit expensive computational simulations within a design optimization framework. the application of multi-fidelity surrogate modeling approaches has recently been gaining ground due to the potential for further reductions in simulation effort over single fidelity approaches. however, given a black box problem when exactly should a designer select a multi-fidelity approach over a single fidelity approach and vice versa? using a series of analytical test functions and engineering design examples from the literature, the following paper illustrates the potential pitfalls of choosing one technique over the other without a careful consideration of the optimization problem at hand. these examples are then used to define and validate a set of guidelines for the creation of a multi-fidelity kriging model. the resulting guidelines state that the different fidelity functions should be well correlated, that the amount of low fidelity data in the model should be greater than the amount of high fidelity data and that more than 10 {\{}{\%}{\}} and less than 80 {\{}{\%}{\}} of the total simulation budget should be spent on low fidelity simulations in order for the resulting multi-fidelity model to perform better than the equivalent costing high fidelity model.},
author = {toal, david j. j.},
doi = {10.1007/s00158-014-1209-5},
file = {:users/roy/documents/knowledge/bibliographie/2015/toal{\_}some considerations regarding the use of multi-fidelity kriging in the construction of surrogate models.pdf:pdf},
issn = {1615-147x},
journal = {structural and multidisciplinary optimization},
keywords = {kriging,multi-fidelity,surrogate modeling},
number = {6},
pages = {1223--1245},
title = {{some considerations regarding the use of multi-fidelity kriging in the construction of surrogate models}},
volume = {51},
year = {2015}
}
@article{morris1995,
abstract = {recent work by johnson et al. (j. statist. plann. inference 26 (1990) 131-148) establishes equivalence of the maximin distance design criterion and an entropy criterion motivated by function prediction in a bayesian setting. the latter criterion has been used by currin et al. (j. amer. statist. assoc. 86 (1991) 953-963) to design experiments for which the motivating application is approximation of a complex deterministic computer model. because computer experiments often have a large number of controlled variables (inputs), maximin designs of moderate size are often concentrated in the corners of the cuboidal design region, i.e. each input is represented at only two levels. here we will examine some maximin distance designs constructed within the class of latin hypercube arrangements. the goal of this is to find designs which offer a compromise between the entropy/maximin criterion, and good projective properties in each dimension (as guaranteed by latin hypercubes). a simulated annealing search algorithm is presented for constructing these designs, and patterns apparent in the optimal designs are discussed. {\textcopyright} 1995.},
author = {morris, max d. and mitchell, toby j.},
doi = {10.1016/0378-3758(94)00035-t},
file = {:users/roy/documents/knowledge/bibliographie/1995/morris, mitchell{\_}exploratory designs for computational experiments.pdf:pdf},
isbn = {0378-3758},
issn = {03783758},
journal = {journal of statistical planning and inference},
keywords = {bayesian prediction,computer experiment,computer model,interpolation,latin hypercube design,maximin design,random functions},
number = {3},
pages = {381--402},
title = {{exploratory designs for computational experiments}},
volume = {43},
year = {1995}
}
@article{bierig2015,
author = {bierig, claudio and chernov, alexey},
doi = {10.1007/s00211-014-0676-3},
file = {:users/roy/documents/knowledge/bibliographie/2015/bierig, chernov{\_}convergence analysis of multilevel monte carlo variance estimators and application for random obstacle problems.pdf:pdf},
issn = {0029599x},
journal = {numerische mathematik},
keywords = {65c05,65c30,65k15,65n30,65n55},
number = {4},
pages = {579--613},
publisher = {springer berlin heidelberg},
title = {{convergence analysis of multilevel monte carlo variance estimators and application for random obstacle problems}},
volume = {130},
year = {2015}
}
@article{daviller2017,
abstract = {large-eddy simulation (les) has become a potent tool to investigate instabili- ties in swirl flows even for complex, industrial geometries. however, the accurate prediction of pressure losses on these complex flows remains difficult. the paper identifies localised near-wall resolution issues as an important factor to improve accuracy and proposes a solu- tion with an adaptive mesh h-refinement strategy relying on the tetrahedral fully automatic mmg3dlibrary of dapogny et al. (j. comput. phys. 262, 358-378, 2014)usinganovelsen- sor based on the dissipation of kinetic energy. using a joint experimental and numerical les study, the methodology is first validated on a simple diaphragm flow before to be applied on a swirler with two counter-rotating passages. the results demonstrate that the new sen- sor and adaptation approach can effectively produce the desired local mesh refinement to match the target losses, measured experimentally. results shows that the accuracy of pres- sure losses prediction is mainly controlled by the mesh quality and density in the swirler passages. the refinement also improves the computed velocity and turbulence profiles at the swirler outlet, compared to piv results. the significant improvement of results confirms that the sensor is able to identify the relevant physics of turbulent flows that is essential for the overall accuracy of les. finally, in the appendix, an additional comparison of the sensor fields on tetrahedral and hexahedral meshes demonstrates that the methodology is broadly applicable to all mesh types.},
author = {Daviller, Guillaume and Brebion, Maxence and Xavier, Pradip and Staffelbach, Gabriel and M{\"{u}}ller, Jens-Dominik and Poinsot, Thierry},
doi = {10.1007/s10494-017-9808-z},
file = {:users/roy/documents/knowledge/bibliographie/2017/daviller et al.{\_}a mesh adaptation strategy to predict pressure losses in les of swirled flows.pdf:pdf},
issn = {1386-6184},
journal = {Flow, Turbulence and Combustion},
keywords = {adaptive mesh r,les,pressure losses,swirl injector},
month = {mar},
title = {A mesh adaptation strategy to predict pressure losses in {LES} of swirled flows},
year = {2017}
}
@misc{package2015,
author = {{le gratiet}, loic},
file = {:users/roy/documents/knowledge/bibliographie/2015/le gratiet{\_}package muficokriging.pdf:pdf},
title = {{package "muficokriging"}},
year = {2015}
}
@article{iooss2010b,
abstract = {cet article a pour objectif d'effectuer un survol rapide, mais dans un cadre m{\'{e}}thodologique relativement complet, des diff{\'{e}}rentes m{\'{e}}thodes d'analyse de sensibilit{\'{e}} globale d'un mod{\`{e}}le num{\'{e}}rique. faisant appel {\`{a}} de nombreux outils statistiques (r{\'{e}}gression, lissage, tests, apprentissage, techniques de monte carlo, . . . ), celles-ci permettent de d{\'{e}}terminer quelles sont les variables d'entr{\'{e}}e d'un mod{\`{e}}le qui contribuent le plus {\`{a}} une quantit{\'{e}} d'int{\'{e}}r{\^{e}}t calcul{\'{e}}e {\`{a}} l'aide de ce mod{\`{e}}le (par exemple la variance d'une variable de sortie). trois grandes classes de m{\'{e}}thodes sont ainsi distingu{\'{e}}es : le criblage (tri grossier des entr{\'{e}}es les plus influentes parmi un grand nombre), les mesures d'importance (indices quantitatifs donnant l'influence de chaque entr{\'{e}}e) et les outils d'exploration du mod{\`{e}}le (mesurant les effets des entr{\'{e}}es sur tout leur domaine de variation). une m{\'{e}}thodologie progressive d'application de ces techniques est illustr{\'{e}}e sur une application {\`{a}} vocation p{\'{e}}dagogique. une synth{\`{e}}se est alors formul{\'{e}}e afin de situer chaque m{\'{e}}thode selon trois axes : co{\^{u}}t en nombre d'{\'{e}}valuations du mod{\`{e}}le, complexit{\'{e}} du mod{\`{e}}le et type d'information apport{\'{e}}e},
author = {iooss, bertrand},
file = {:users/roy/documents/knowledge/bibliographie/2010/iooss{\_}revue sur l'analyse de sensibilit{\'{e}} globale de mod{\`{e}}les num{\'{e}}riques.pdf:pdf},
journal = {journal de la soci{\'{e}}t{\'{e}} fran{\c{c}}aise de statistique},
keywords = {computer code,design of experiments,metamodel,numerical experiment,uncertainty},
number = {1},
pages = {3--25},
title = {{revue sur l'analyse de sensibilit{\'{e}} globale de mod{\`{e}}les num{\'{e}}riques}},
volume = {152},
year = {2010}
}
@incollection{thual2011,
author = {thual, olivier},
chapter = {4},
file = {:users/roy/documents/knowledge/bibliographie/2011/thual{\_}hydraulique {\`{a}} surface libre.pdf:pdf},
pages = {105},
title = {{hydraulique {\`{a}} surface libre}},
year = {2011}
}
@article{kumar2016,
abstract = {this paper presents a comparative analysis of the performance of the incremental ant colony algorithm for continuous optimization (iacor), with different algorithms provided in the nlopt library. the key objective is to understand how various algorithms in the nlopt library perform in combination with the multi-trajectory local search (mtsls1) technique. a hybrid approach has been introduced for the local search strategy, by the use of a parameter that allows for probabilistic selection between mtsls1 and the nlopt algorithm. in case of stagnation, a switch is made based on the algorithm being used in the previous iteration. this paper presents an exhaustive comparison on the performance of these approaches on soft computing (soco) and congress on evolutionary computation (cec) 2014 benchmarks. for both sets of benchmarks, we conclude that the best performing algorithm is a hybrid variant of mtsls1 with bfgs for local search.},
archiveprefix = {arxiv},
arxivid = {1503.03175},
author = {kumar, udit and soman, sumit and jayadeva},
doi = {10.1016/j.swevo.2015.10.005},
eprint = {1503.03175},
file = {:users/roy/documents/knowledge/bibliographie/2016/kumar, soman, jayadeva{\_}benchmarking nlopt and state-of-the-art algorithms for continuous global optimization via iacor.pdf:pdf},
issn = {22106502},
journal = {swarm and evolutionary computation},
keywords = {aco,global optimization,hybrid iacor,iacor-local search,mtsls1,nlopt},
pages = {116--131},
title = {{benchmarking nlopt and state-of-the-art algorithms for continuous global optimization via iacor}},
volume = {27},
year = {2016}
}
@article{saltelli1999,
abstract = {a new method for sensitivity analysis (sa) of model output is introduced. it is based on the fourier amplitude sensitivity test (fast) and allows the computation of the total contribution of each input factor to the output's variance. the term "total" here means that the factor's main effect, as well as all the interaction terms involving that factor, are included. although computationally different, the very same measure of sensitivity is offered by the indices of sobol'. the main advantages of the extended fast are its robustness, especially at low sample size, and its computational efficiency. the computational aspects of the extended fast are described. these include (1) the definition of new sets of parametric equations for the search-curve exploring the input space, (2) the selection of frequencies for the parametric equations, and (3) the procedure adopted to estimate the total contributions. we also address the limitations of other global sa methods and suggest that the total-effect indices are ideally suited to perform a global, quantitative, model-independent sensitivity analysis.},
author = {saltelli, andrea and tarantola, stefano and chan, karen},
file = {:users/roy/documents/knowledge/bibliographie/1999/saltelli, tarantola, chan{\_}a quantitative model-independent method for global sensitivity analysis of model output.pdf:pdf},
journal = {technometrics},
keywords = {computational,fourier amplitude sensitivity,nonlinear and,total sensitivity,indice,model,models,nonmonotonic,test (fast)},
number = {1},
pages = {356},
title = {{a quantitative model-independent method for global sensitivity analysis of model output}},
volume = {41},
year = {1999}
}
@incollection{bilionis2016,
abstract = {classic non-intrusive uncertainty propagation techniques, typically, require a significant number of model evaluations in order to yield convergent statistics. in practice, however, the computational complexity of the underlying computer codes limits significantly the number of observations that one can actually make. in such situations the estimates produced by classic approaches cannot be trusted since the limited number of observations induces additional epistemic uncertainty. the goal of this chapter is to highlight how the bayesian formalism can quantify this epistemic uncertainty and provide robust predictive intervals for the statistics of interest with as few simulations as one has available. it is shown how the bayesian formalism can be materialized by employing the concept of a gaussian process (gp). in addition, several practical aspects that depend on the nature of the underlying response surface, such as the treatment of spatiotemporal variation, and multi-output responses are discussed. the practicality of the approach is demonstrated by propagating uncertainty through a dynamical system and an elliptic partial differential equation.},
author = {bilionis, ilias and zabaras, nicholas},
booktitle = {handbook of uncertainty quantification},
doi = {10.1007/978-3-319-11259-6_16-1},
file = {:users/roy/documents/knowledge/bibliographie/2015/bilionis, zabaras{\_}bayesian uncertainty propagation using gaussian processes.pdf:pdf},
isbn = {978-3-319-12384-4},
keywords = {epistemic uncertainty,expensive computer code,expensive computer simulations gaussian process,uncertainty propagation},
pages = {1--45},
publisher = {springer international publishing},
title = {{bayesian uncertainty propagation using gaussian processes}},
year = {2015}
}
@article{dauphin2014,
abstract = {a central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. gradient descent or quasi-newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-newton methods. we apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
archiveprefix = {arxiv},
arxivid = {1406.2572},
author = {dauphin, yann and pascanu, razvan and gulcehre, caglar and cho, kyunghyun and ganguli, surya and bengio, yoshua},
eprint = {1406.2572},
file = {:users/roy/documents/knowledge/bibliographie/2014/dauphin et al.{\_}identifying and attacking the saddle point problem in high-dimensional non-convex optimization.pdf:pdf},
month = {jun},
pages = {1--14},
title = {{identifying and attacking the saddle point problem in high-dimensional non-convex optimization}},
year = {2014}
}
@article{owhadi2013,
abstract = {we propose a rigorous framework for uncertainty quantification (uq) in which the uq objectives and its assumptions/information set are brought to the forefront. this framework, which we call optimal uncertainty quantification (ouq), is based on the observation that, given a set of assumptions and information about the problem, there exist optimal bounds on uncertainties: these are obtained as values of well-defined optimization problems corresponding to extremizing probabilities of failure, or of deviations, subject to the constraints imposed by the scenarios compatible with the assumptions and information. in particular, this framework does not implicitly impose inappropriate assumptions, nor does it repudiate relevant information. although ouq optimization problems are extremely large, we show that under general conditions they have finite-dimensional reductions. as an application, we develop optimal concentration inequalities (oci) of hoeffding and mcdiarmid type. surprisingly, these results show that uncertainties in input parameters, which propagate to output uncertainties in the classical sensitivity analysis paradigm, may fail to do so if the transfer functions (or probability distributions) are imperfectly known. we show how, for hierarchical structures, this phenomenon may lead to the nonpropagation of uncertainties or information across scales. in addition, a general algorithmic framework is developed for ouq and is tested on the caltech surrogate model for hypervelocity impact and on the seismic safety assessment of truss structures, suggesting the feasibility of the framework for important complex systems. the introduction of this paper provides both an overview of the paper and a self-contained minitutorial on the basic concepts and issues of uq.},
archiveprefix = {arxiv},
arxivid = {1009.0679},
author = {owhadi, h. and scovel, c. and sullivan, t. j. and mckerns, m. and ortiz, m.},
doi = {10.1137/10080782x},
eprint = {1009.0679},
file = {:users/roy/documents/knowledge/bibliographie/2013/owhadi et al.{\_}optimal uncertainty quantification.pdf:pdf},
isbn = {0036-1445},
issn = {0036-1445, 1095-7200},
journal = {siam review},
keywords = {10,10080782x,1137,28e99,60-08,60e15,62g99,65c50,90c26,ams subject classifications,concentration inequalities,doi,generalized chebyshev optimization problems,krein-,markov,sensitivity analysis,type reduction theorems for,uncertainty quantification},
number = {2},
pages = {271--345},
title = {{optimal uncertainty quantification}},
volume = {55},
year = {2013}
}
@book{schwab,
abstract = {the present volume addresses methods and computational aspects of efficient uncertainty quantification (uq) in computational fluid dynamics (cfd). while the general area of computational uncertainty quantification in engineering simula- tions has experienced a massive development in recent years, and is under strong expansion currently, by now key computational issues have been identified and analysis and implementation have progressed to the point where, for several broad classes of pdes with uncertainty, computationalmethodologies are available which are also backed by numerical and mathematical analysis. against this background, and consistent with the scientific focus of the 2011vonkarmann instituteworkshop which initiated the development of the chapters in the present volume, the present volume combines several contributions on efficient methods for uq in cfd which address specific computational issues which arise in the use of the general computational methodologies in cfd problems; some (but not all) of these are: highly nonlinear, unsteady nature of the governing equations, singularity (shock) formation in finite time in pathwise solutions, and the impact of discontinuities on the accuracy and the regularity of statistical quantities even when all data in the problem are smooth. other issues are the corresponding low regularity in the space of uncertain parameters,massive parallelism in forward simulations, necessity for multiscaling and multi-modelling in forward simulations (in particular in the presence of turbulence), uncertain topography and geometry of the flow domain. the lowsolution regularity and the propagation of singularities in solutions of the governing equations prompt the development of the numerical techniqueswhich are specifically adapted to deal with these pheonomena; among them are finite volume methods in the stochastic parameter domain, weno reconstruction and limiting methods for positivity enforcement in computation of probability density functions of random solution quantities, to name but a few. most of these techniques are nonintrusive since, unlike the situation encountered in computational uq in solids and wave propagation, the strong nonlinearity of the governing equation narrows applicable uq methods to essentially those of collocation type. due to the low, pathwise regularity of solutions of nonlinear hyperbolic conservation laws, however, the (in general high) regularity properties of parametric stochastic solutions required for example by spectral collocation methods must be carefully verified in practice. a logical conclusion of these remarks is the prominent role which will be played by stochastic collocation methods and by monte-carlo sampling approaches; in particular, multilevel monte-carlo sampling approaches have proved quite efficient and powerful strategies when solving uq problems in cfd. we are confident that the methods which we found to be viable and robust for the cfd problems considered here will also prove to be applicable to other, “hard” and fully nonlinear computationalmodels in engineering and in the sciences. the notes address computational technicalities of specific issues arising in uq in current cfd applications, in particular uq in output functionals such as lift-, drag- and other, integral quantities of the primitive uncertain variables, estimation of statistical moments, in particular of variance, and the probability of computation of extremal events, and the assessment of the accuracy of statistical quantities in the presence of discretization and other, numerical errors. while these notes focus on computational and implementation aspects of discretization, stability, parallelization of computational uq for problems in cfd. naturally, they impinge on a number of related issues in numerical analysis and high performance computing; we only mention load balancing issues in massively parallel uq simulations and the mathematical regularity of statistical quantities of output functionals; here, the most prominent example is that of statistics of shock locations and profiles where additional regularity of outputs is generated by ensemble averaging of random entropy solutions, so that for example the statistics of shock locations can become lipschitz continuous or more regular, even for hyperbolic equations without any viscosity. as can be expected in a field which is currently undergoing rapid development, the present notes represent only a snapshot in time of this evolving field of computational science and engineering. the present notes are intended to present the key ideas, the description of uq algorithms, and prototypical implementations on a high technical level, which should be accessible, nevertheless by graduate students and researchers in computational science as well as in cfd-related areas of engineering. the background knowledge of the intended readership of this volume is knowl- edge of elementary probability and statistics and solid knowledge of computational fluid dynamics. we very much hope that these notes stimulate further algorithmic and theoretical developments in uq for cfd and, due to the interdisciplinarity nature of uq, also in the adjacent areas of statistics, high-performance computing, and the analysis of partial differential equations with random input data.},
address = {cham},
author = {schwab, christoph},
doi = {10.1007/978-3-319-00885-1},
editor = {bijl, hester and lucor, didier and mishra, siddhartha and schwab, christoph},
file = {:users/roy/documents/knowledge/bibliographie/2013/schwab{\_}uncertainty quantification in computational fluid dynamics.pdf:pdf},
isbn = {978-3-319-00884-4},
publisher = {springer international publishing},
series = {lecture notes in computational science and engineering},
title = {{uncertainty quantification in computational fluid dynamics}},
volume = {92},
year = {2013}
}
@article{li,
abstract = {this study aims at analyzing the combined impact of uncertainties in initial conditions and wind forcing fields in ocean general circulation models (ogcm) using polynomial chaos (pc) expansions. empirical orthogonal functions (eof) are used to formulate both spatial perturbations to initial conditions and space-time wind forcing perturbations, namely in the form of a superposition of modal components with uniformly distributed random amplitudes. the forward deterministic hybrid coordinate ocean model (hycom) is used to propagate input uncertainties in the gulf of mexico (gom) in spring 2010, during the deepwater horizon oil spill, and to generate the ensemble of model realizations based on which pc surrogate models are constructed for both localized and field quantities of interest (qois), focusing specifically on sea surface height (ssh) and mixed layer depth (mld). these pc surrogate models are constructed using basis pursuit denoising methodology, and their performance is assessed through various statistical measures. a global sensitivity analysis is then performed to quantify the impact of individual modes as well as their interactions. it shows that the local ssh at the edge of the gom main current---the loop current---is mostly sensitive to perturbations of the initial conditions affecting the current front, whereas the local mld in the area of the deepwater horizon oil spill is more sensitive to wind forcing perturbations. at the basin scale, the ssh in the deep gom is mostly sensitive to initial condition perturbations, while over the shelf it is sensitive to wind forcing perturbations. on the other hand, the basin mld is almost exclusively sensitive to wind perturbations. for both quantities, the two sources of uncertainty have limited interactions. finally, the computations indicate that whereas local quantities can exhibit complex behavior that necessitates a large number of realizations, the modal analysis of field sensitivities can be suitably achieved with a moderate size ensemble.},
author = {li, guotu and iskandarani, mohamed and h??naff, matthieu le and winokur, justin and {le ma??tre}, olivier p. and knio, omar m},
doi = {10.1007/s10596-016-9581-4},
file = {:users/roy/documents/knowledge/bibliographie/unknown/li et al.{\_}quantifying initial and wind forcing uncertainties in the gulf of mexico.pdf:pdf},
issn = {15731499},
journal = {computational geosciences},
keywords = {basis pursuit denoising,empirical orthogonal function,polynomial chaos expansion,sensitivity analysis},
number = {5},
pages = {1133--1153},
title = {{quantifying initial and wind forcing uncertainties in the gulf of mexico}},
volume = {20},
year = {2016}
}
@inproceedings{matejka2017,
abstract = {datasets which are identical over a number of statistical properties, yet produce dissimilar graphs, are frequently used to illustrate the importance of graphical representations when exploring data. this paper presents a novel method for generating such datasets, along with several examples. our technique varies from previous approaches in that new datasets are iteratively generated from a seed dataset through random perturbations of individual data points, and can be directed towards a desired outcome through a simulated annealing optimization strategy. our method has the benefit of being agnostic to the particular statistical properties that are to remain constant between the datasets, and allows for control over the graphical appearance of resulting output.},
author = {matejka, justin and fitzmaurice, george},
booktitle = {chi conference on human factors in computing systems},
doi = {10.1145/3025453.3025912},
file = {:users/roy/documents/knowledge/bibliographie/2017/matejka, fitzmaurice{\_}same stats , different graphs generating datasets with varied appearance and identical statistics through simulate.pdf:pdf},
pages = {1290--1294},
title = {{same stats , different graphs : generating datasets with varied appearance and identical statistics through simulated annealing}},
year = {2017}
}

@book{clarke1992,
address = {new york, ny},
author = {Clarke, Bertrand and Fokoue, Ernest and Zhang, Hao Helen},
doi = {10.1007/978-1-4612-4380-9},
editor = {kotz, samuel and johnson, norman l.},
file = {:users/roy/documents/knowledge/bibliographie/1992/clarke, fokoue, zhang{\_}breakthroughs in statistics 2.pdf:pdf},
isbn = {978-0-387-94039-7},
pages = {1--694},
publisher = {Springer new york},
series = {Springer Series in Statistics},
title = {{Breakthroughs in Statistics 2}},
year = {1992}
}
@book{handbookuq2017,
abstract = {these lecture notes highlight the mathematical and computational structure relating to the formulation of, and development of algorithms for, the bayesian approach to inverse problems in differential equations. this approach is fundamental in the quantification of uncertainty within applications in volving the blending of mathematical models with data. the finite dimensional situation is described first, along with some motivational examples. then the development of probability measures on separable banach space is undertaken, using a random series over an infinite set of functions to construct draws; these probability measures are used as priors in the bayesian approach to inverse problems. regularity of draws from the priors is studied in the natural sobolev or besov spaces implied by the choice of functions in the random series construction, and the kolmogorov continuity theorem is used to extend regularity considerations to the space of h{\"{o}}lder continuous functions. bayes' theorem is de rived in this prior setting, and here interpreted as finding conditions under which the posterior is absolutely continuous with respect to the prior, and determining a formula for the radon-nikodym derivative in terms of the likelihood of the data. having established the form of the posterior, we then describe various properties common to it in the infinite dimensional setting. these properties include well-posedness, approximation theory, and the existence of maximum a posteriori estimators. we then describe measure-preserving dynamics, again on the infinite dimensional space, including markov chain-monte c arlo and sequential monte carlo methods, and measure-preserving reversible stochastic differential equations. by formulating the theory and algorithms on the underlying infinite dimensional space, we obtain a framework suitable for rigorous analysis of the accuracy of reconstructions, of computational complexity, as well as naturally constructing algorithms which perform well under mesh refinement, since they are inherently well-defined in infinite dimensions.},
address = {cham},
doi = {10.1007/978-3-319-12385-1},
editor = {ghanem, roger and higdon, david and owhadi, houman},
file = {:users/roy/documents/knowledge/bibliographie/2017/unknown{\_}handbook of uncertainty quantification.pdf:pdf},
isbn = {978-3-319-12384-4},
publisher = {springer international publishing},
title = {{handbook of uncertainty quantification}},
year = {2017}
}
@article{puigt2007,
author = {puigt, guillaume},
file = {:users/roy/documents/knowledge/bibliographie/2007/puigt{\_}simulation des {\'{e}}coulements turbulents par une approche rans equations de la physique, analyse math{\'{e}}matique, mod{\`{e}}les num{\'{e}}rique.pdf:pdf},
title = {{simulation des {\'{e}}coulements turbulents par une approche rans : equations de la physique, analyse math{\'{e}}matique, mod{\`{e}}les num{\'{e}}riques}},
year = {2007}
}
@article{passot1987,
abstract = {compressible flows with r.m.s. velocities of the order of the speed of sound are studied with direct numerical simulations using a pseudospectral method. we concentrate on turbulent homogeneous flows in the two-dimensional case. the fluid obeys the navier-stokes equations for a perfect gas, and viscous terms are included explicitly. no modelling of small scales is used. we show that the behaviour of the flow differs sharply at low compared with high r.m.s. mach number ma, with a transition at ma = 0.3. in the large scales, temporal exchanges between longitudinal and solenoidal modes of energy retain an acoustical character; they lead to a slowing down of the decrease of the mach number with time, which occurs with interspersed plateaux corresponding to quiescent periods. when the flow is initially supersonic, the small scales are dominated by shocks behind which vortices form. this vortex production is particularly prominent when two strong shocks collide, with the onset of shear turbulence in the region downstream of the collision. however, at the resolutions reached by our code on a 256 × 256 uniform grid, this mechanism proves insufficient to bring vortices into equipartition with shocks in the small-scale tail of the energy spectrum.},
author = {passot, t. and pouquet, a.},
doi = {10.1017/s0022112087002167},
file = {:users/roy/documents/knowledge/bibliographie/1987/passot, pouquet{\_}numerical simulation of compressible homogeneous flows in the turbulent regime.pdf:pdf},
isbn = {1469-7645},
issn = {0022-1120},
journal = {journal of fluid mechanics},
month = {sep},
number = {-1},
pages = {441},
title = {{numerical simulation of compressible homogeneous flows in the turbulent regime}},
volume = {181},
year = {1987}
}

@article{peter2003,
author = {peter, jacques e.v. and burguburu, st{\'{e}}phane},
file = {:users/roy/documents/knowledge/bibliographie/2003/peter, burguburu{\_}introduction {\`{a}} l'optimisation de forme en a{\'{e}}rodynamique et quelques exemples d'application.pdf:pdf},
journal = {cours onera},
title = {{introduction {\`{a}} l'optimisation de forme en a{\'{e}}rodynamique et quelques exemples d'application}},
year = {2003}
}
@article{iaccarino2014,
author = {iaccarino, gianluca},
file = {:users/roy/documents/knowledge/bibliographie/2014/iaccarino{\_}uncertainty quantification in computational science.pdf:pdf},
journal = {aiaa journal},
title = {{uncertainty quantification in computational science}},
year = {2014}
}
@article{albers2014,
abstract = {many visualization tasks require the viewer to make judgments about aggregate properties of data. recent work has shown that viewers can perform such tasks effectively, for example to efficiently compare the maximums or means over ranges of data. however, this work also shows that such effectiveness depends on the designs of the displays. in this paper, we explore this relationship between aggregation task and visualization design to provide guidance on matching tasks with designs. we combine prior results from perceptual science and graphical perception to suggest a set of design variables that influence performance on various aggregate comparison tasks. we describe how choices in these variables can lead to designs that are matched to particular tasks. we use these variables to assess a set of eight different designs, predicting how they will support a set of six aggregate time series comparison tasks. a crowd-sourced evaluation confirms these predictions. these results not only provide evidence for how the specific visualizations support various tasks, but also suggest using the identified design variables as a tool for designing visualizations well suited for various types of tasks.},
archiveprefix = {arxiv},
arxivid = {nihms150003},
author = {albers, danielle and correll, michael and gleicher, michael},
doi = {10.1145/2556288.2557200},
eprint = {nihms150003},
file = {:users/roy/documents/knowledge/bibliographie/2014/albers, correll, gleicher{\_}task-driven evaluation of aggregation in time series visualization.pdf:pdf},
isbn = {9781450324731},
issn = {15378276},
journal = {proceedings of the sigchi conference on human factors in computing systems chi conference},
pages = {551--560},
pmid = {25343147},
title = {{task-driven evaluation of aggregation in time series visualization.}},
volume = {2014},
year = {2014}
}
@article{kumar2016a,
author = {kumar, dinesh and raisee, mehrdad and lacor, chris},
doi = {10.1016/j.compfluid.2016.08.015},
file = {:users/roy/documents/knowledge/bibliographie/2016/kumar, raisee, lacor{\_}an efficient non-intrusive reduced basis model for high dimensional stochastic problems in cfd.pdf:pdf},
issn = {00457930},
journal = {computers {\&} fluids},
keywords = {polynomial chaos,proper orthogonal deco,stochastic},
month = {oct},
pages = {67--82},
publisher = {elsevier ltd},
title = {{an efficient non-intrusive reduced basis model for high dimensional stochastic problems in cfd}},
volume = {138},
year = {2016}
}
@article{bect2012,
abstract = {this paper deals with the problem of estimating the volume of the excursion set of a function {\$}f:\backslashmathbb{\{}r{\}}{\^{}}d \backslashto \backslashmathbb{\{}r{\}}{\$} above a given threshold, under a probability measure on {\$}\backslashmathbb{\{}r{\}}{\^{}}d{\$} that is assumed to be known. in the industrial world, this corresponds to the problem of estimating a probability of failure of a system. when only an expensive-to-simulate model of the system is available, the budget for simulations is usually severely limited and therefore classical monte carlo methods ought to be avoided. one of the main contributions of this article is to derive sur (stepwise uncertainty reduction) strategies from a bayesian-theoretic formulation of the problem of estimating a probability of failure. these sequential strategies use a gaussian process model of {\$}f{\$} and aim at performing evaluations of {\$}f{\$} as efficiently as possible to infer the value of the probability of failure. we compare these strategies to other strategies also based on a gaussian process model for estimating a probability of failure.},
archiveprefix = {arxiv},
arxivid = {1009.5177},
author = {bect, julien and ginsbourger, david and li, ling and picheny, victor and vazquez, emmanuel},
doi = {10.1007/s11222-011-9241-4},
eprint = {1009.5177},
file = {:users/roy/documents/knowledge/bibliographie/2012/bect et al.{\_}sequential design of computer experiments for the estimation of a probability of failure.pdf:pdf},
isbn = {1122201192},
issn = {0960-3174},
journal = {statistics and computing},
keywords = {computer experiments,gaussian processes,probability of failure,sequential design,stepwise uncertainty reduction},
month = {may},
number = {3},
pages = {773--793},
title = {{sequential design of computer experiments for the estimation of a probability of failure}},
volume = {22},
year = {2012}
}
@article{nguyen2015,
abstract = {the choice of sensitivity analysis methods for a model often relies on the behavior of model outputs. however, many building energy models are “black-box” functions whose behavior of simulated results is usually unknown or uncertain. this situation raises a question of how to correctly choose a sensitivity analysis method and its settings for building simulation. a performance comparison of nine sensitivity analysis methods has been carried out by means of computational experiments and building energy simulation. a comprehensive test procedure using three benchmark functions and two real-world building energy models was proposed. the degree of complexity was gradually increased by carefully-chosen test problems. performance of these methods was compared through the ranking of variables' importance, variables' sensitivity indices, interaction among variables, and computational cost for each method. test results show the consistency between the fourier amplitude sensitivity test (fast) and the sobol method. some evidences found from the tests indicate that performance of other methods was unstable, especially with the non-monotonic test problems.},
author = {nguyen, anh-tuan and reiter, sigrid},
doi = {10.1007/s12273-015-0245-4},
file = {:users/roy/documents/knowledge/bibliographie/2015/nguyen, reiter{\_}a performance comparison of sensitivity analysis methods for building energy models.pdf:pdf},
isbn = {1227301502},
issn = {1996-3599},
journal = {building simulation},
keywords = {comparison,monte carlo approach,morris method,regression-based,sensitivity analysis,variance-based sensitivity analysis},
number = {6},
pages = {651--664},
title = {{a performance comparison of sensitivity analysis methods for building energy models}},
volume = {8},
year = {2015}
}
@phdthesis{senorer2010,
author = {senorer, jean-mathieu},
file = {:users/roy/documents/knowledge/bibliographie/2010/senorer{\_}simulation aux grandes {\'{e}}chelles de l'{\'{e}}coulement diphasique dans un br{\^{u}}leur a{\'{e}}ronautique par une approche euler-lagrange.pdf:pdf},
school = {universit{\'{e}} de toulouse},
title = {{simulation aux grandes {\'{e}}chelles de l'{\'{e}}coulement diphasique dans un br{\^{u}}leur a{\'{e}}ronautique par une approche euler-lagrange}},
year = {2010}
}

@article{marrel2014,
abstract = {to evaluate the consequences on human health of radionuclide releases in the environment, numerical simulators are used to model the radionuclide atmospheric dispersion. these codes can be time consuming and depend on many uncertain variables related to radionuclide, release or weather conditions. these variables are of different kind: scalar, functional and qualitative. given the uncertain parameters, code provides spatial maps of radionuclide concentration for various moments. the objective is to assess how these uncertainties can affect the code predictions and to perform a global sensitivity analysis of code in order to identify the most influential uncertain parameters. this sensitivity analysis often calls for the estimation of variance-based importance measures, called sobol' indices. to estimate these indices, we propose a global methodology combining several advanced statistical techniques which enable to deal with the various natures of the uncertain inputs and the high dimension of model outputs. first, a quantification of input uncertainties is made based on data analysis and expert judgment. then, an initial realistic sampling design is generated and the corresponding code simulations are performed. based on this sample, a proper orthogonal decomposition of the spatial output is used and the main decomposition coefficients are modeled with gaussian process surrogate model. the obtained spatial metamodel is then used to compute spatial maps of sobol' indices, yielding the identification of global and local influence of each input variable and the detection of areas with interactions. the impact of uncertainty quantification step on the results is also evaluated.},
author = {marrel, amandine and perot, nadia and mottet, cl{\'{e}}mentine},
doi = {10.1007/s00477-014-0927-y},
file = {:users/roy/documents/knowledge/bibliographie/2014/marrel, perot, mottet{\_}development of a surrogate model and sensitivity analysis for spatio-temporal numerical simulators.pdf:pdf},
issn = {14363259},
journal = {stochastic environmental research and risk assessment},
keywords = {gaussian process,global sensitivity analysis,metamodel,pod decomposition,spatio-temporal models},
number = {3},
pages = {959--974},
title = {{development of a surrogate model and sensitivity analysis for spatio-temporal numerical simulators}},
volume = {29},
year = {2014}
}
@article{frazier2014,
abstract = {this paper reviews the state-of-the-art of an important, rapidly emerging, manufacturing technology that is alternatively called additive manufacturing (am), direct digital manufacturing, free form fabrication, or 3d printing, etc. a broad contextual overview of metallic am is provided. am has the potential to revo- lutionize the global parts manufacturing and logistics landscape. it enables distributed manufacturing and the productions of parts-on-demand while offering the potential to reduce cost, energy consumption, and carbon footprint. this paper explores the material science, processes, and business consideration associated with achieving these performance gains. it is concluded that a paradigm shift is required in order to fully exploit am potential.},
author = {Frazier, William E.},
doi = {10.1007/s11665-014-0958-z},
file = {:users/roy/documents/knowledge/bibliographie/2014/frazier{\_}metal additive manufacturing a review.pdf:pdf},
isbn = {1059-9495},
issn = {15441024},
journal = {Journal of Materials Engineering and Performance},
keywords = {fabricated metal,modeling processes,powder metallurgy},
number = {6},
pages = {1917--1928},
title = {{Metal additive manufacturing: a review}},
volume = {23},
year = {2014}
}
@article{robinson1991,
abstract = {in animal breeding, best linear unbiased prediction, or blup, is a technique for estimating genetic merits. in general, it is a method of estimating random effects. it can be used to derive the kalman filter, the method of kriging used for ore reserve estimation, credibility theory used to work out insurance premiums, and hoadley's quality measurement plan used to estimate a quality index. it can be used for removing noise from images and for small-area estimation. this paper presents the theory of blup, some examples of its applica- tion and its relevance to the foundations of statistics. understanding of procedures for estimating random effects should help people to understand some complicated and controversial issues about fixed and random effects models and also help to bridge the apparent gulf between the bayesian and classical schools of thought.},
author = {robinson, g. k.},
doi = {10.1214/ss/1177011926},
file = {:users/roy/documents/knowledge/bibliographie/1991/robinson{\_}that blup is a good thing the estimation of random effects.pdf:pdf},
isbn = {08834237},
issn = {0883-4237},
journal = {statistical science},
keywords = {best linear unbiased prediction (blup),kalman filtering,credibility theory,esti- mation of random effects,fixed versus random effects,foundations of statistics,likelihood,parametric em- pirical bayes methods,rank- ing and selection,selection index,small-area estimation},
number = {1},
pages = {15--32},
title = {{that blup is a good thing: the estimation of random effects}},
volume = {6},
year = {1991}
}
@inproceedings{popelin2013,
abstract = {in nuclear engineering studies, uncertainty and sensitivity analyses of simulation computer codes can be faced to the complexity of the input and/or the output variables. if these variables represent a transient or a spatial phenomenon, the difficulty is to provide tool adapted to their functional nature. in this paper, we describe useful visualization tools in the context of uncertainty analysis of model transient outputs. our application involves thermal-hydraulic computations for safety studies of nuclear pressurized water reactors.},
address = {les ulis, france},
archiveprefix = {arxiv},
arxivid = {1402.7141},
author = {Popelin, Anne-Laure and Iooss, Bertrand},
booktitle = {Joint International Conference on Supercomputing in Nuclear Applications},
doi = {10.1051/snamc/201403403},
editor = {Caruge, D. and Calvin, C. and Diop, C.M. and malvagi, F. and Trama, J.-C.},
eprint = {1402.7141},
file = {:users/roy/documents/knowledge/bibliographie/2013/popelin, iooss{\_}visualization tools for uncertainty and sensitivity analyses on thermal-hydraulic transients.pdf:pdf},
isbn = {978-2-7598-1269-1},
keywords = {computer experiment,uncertainty and sensitivity analysis,visualization},
month = {jun},
pages = {03403},
publisher = {edp sciences},
title = {{visualization tools for uncertainty and sensitivity analyses on thermal-hydraulic transients}},
year = {2013}
}
@article{benjamin2017,
abstract = {we propose to change the default p-value threshold for statistical significance for claims of new discoveries from 0.05 to 0.005.},
archiveprefix = {arxiv},
arxivid = {psyarxiv/mky9j},
author = {benjamin, daniel j. and berger, james o. and johannesson, magnus and nosek, brian a. and wagenmakers, e. j. and berk, richard and johnson, valen e.},
doi = {10.17605/osf.io/mky9j},
eprint = {mky9j},
file = {:users/roy/documents/knowledge/bibliographie/2017/benjamin et al.{\_}redefine statistical significance.pdf:pdf},
journal = {psyarxiv},
number = {july 22},
pages = {1--18},
primaryclass = {psyarxiv},
title = {{redefine statistical significance}},
year = {2017}
}
@techreport{peherstorfer2016,
author = {Peherstorfer, Benjamin and Willcox, Karen},
file = {:users/roy/documents/knowledge/bibliographie/2016/peherstorfer, willcox{\_}survey of multifidelity methods in uncertainty propagation, inference, and optimization.pdf:pdf},
institution = {Aerospace computational design laboratory},
keywords = {model reduction,multifidelity,multifidelity optimization,multifidelity statistical inference,multifidelity uncertainty,multifidelity uncertainty propagation,quantification,surrogate models},
pages = {1--57},
title = {{Survey of multifidelity methods in uncertainty propagation, inference, and optimization}},
year = {2016}
}
@article{dwight2009,
author = {dwight, richard p and han, zhong-hua},
doi = {10.2514/6.2009-2276},
file = {:users/roy/documents/knowledge/bibliographie/2009/dwight, han{\_}efficient uncertainty quantification using gradient-enhanced kriging.pdf:pdf},
isbn = {978-1-60086-975-4},
issn = {02734508},
journal = {fluid dynamics},
number = {may},
pages = {4--7},
title = {{efficient uncertainty quantification using gradient-enhanced kriging}},
year = {2009}
}
@article{xiao2010,
abstract = {this paper proposes a novel approach for multi-objective optimization when the criteria of interest rely on a functional output from an expensive-to-evaluate numerical simulator. more specifically, the proposed method is developed in the frame of an automotive application. the aim of this application is to design the shape of an intake port in order to maximize the mass flow (denoted by q) and the tumble (denoted by t), which both depend on a 3d velocity field obtained by numerical flow simulation. since the considered flow simulator is time-consuming, using regular multi-objective genetic algorithms (moga) directly on integral quantities depending on the simulator output is prohibitive. three different reduced order models (roms) are presented. the first one consists in directly kriging the integral quantities q and t on the basis of the outputs computed at an initial design of experiments, and basing the optimization search on the sequentially obtained couples of response surfaces. the other methods explored in the present work consist in building a parametrized representation of the whole velocity field by different variants of the proper orthogonal decomposition (pod). instead of directly kriging q and t at un-sampled locations, the proposed technique is hence to proceed in two steps: first approximate the functional outcome by kriging the pod coefficients, and then compute the integral quantities q and t associated with the approximate 3d field. however, such an approach induces new difficulties since the truncated pod does not preserve the global (integrated) quantities, and that surrogate-based moga with this kind of pod are therefore likely to fail locating the (q, t)-pareto front accurately. this is what motivates to propose an original constrained pod method (called cpod) meant to overcome the bias created by the truncation made in regular pod. more precisely, this means modifying the way of calculating the pod coefficients by imposing the integral quantities q and t based on the truncated pod to match with the actual q and t values obtained by flow simulation at the design of experiments. a detailed comparison of the pareto sets obtained from the three roms demonstrates the interest of the cpod approach.},
author = {xiao, manyu and breitkopf, piotr and {filomeno coelho}, rajan and knopf-lenoir, catherine and sidorkiewicz, maryan and villon, pierre},
doi = {10.1007/s00158-009-0434-9},
file = {:users/roy/documents/knowledge/bibliographie/2010/xiao et al.{\_}model reduction by cpod and kriging.pdf:pdf},
issn = {1615-147x},
journal = {structural and multidisciplinary optimization},
keywords = {constrained pod,model,multi-objective optimization,parallel computing,pod,reduced order},
number = {4},
pages = {555--574},
title = {{model reduction by cpod and kriging}},
volume = {41},
year = {2010}
}
@article{tripathy2016,
abstract = {uncertainty quantification (uq) tasks, such as model calibration, uncertainty propagation, and optimization under uncertainty, typically require several thousand evaluations of the underlying computer codes. to cope with the cost of simulations, one replaces the real response surface with a cheap surrogate based, e.g., on polynomial chaos expansions, neural networks, support vector machines, or gaussian processes (gp). however, the number of simulations required to learn a generic multivariate response grows exponentially as the input dimension increases. this curse of dimensionality can only be addressed, if the response exhibits some special structure that can be discovered and exploited. a wide range of physical responses exhibit a special structure known as an active subspace (as). an as is a linear manifold of the stochastic space characterized by maximal response variation. the idea is that one should first identify this low dimensional manifold, project the high-dimensional input onto it, and then link the projection to the output. if the dimensionality of the as is low enough, then learning the link function is a much easier problem than the original problem of learning a high-dimensional function. the classic approach to discovering the as requires gradient information, a fact that severely limits its applicability. furthermore, and partly because of its reliance to gradients, it is not able to handle noisy observations. the latter is an essential trait if one wants to be able to propagate uncertainty through stochastic simulators, e.g., through molecular dynamics codes. in this work, we develop a probabilistic version of as which is gradient-free and robust to observational noise. our approach relies on a novel gaussian process regression with built-in dimensionality reduction. in particular, the as is represented as an orthogonal projection matrix that serves as yet another covariance function hyper-parameter to be estimated from the data. to train the model, we design a two-step maximum likelihood optimization procedure that ensures the orthogonality of the projection matrix by exploiting recent results on the stiefel manifold, i.e., the manifold of matrices with orthogonal columns. the additional benefit of our probabilistic formulation, is that it allows us to select the dimensionality of the as via the bayesian information criterion. we validate our approach by showing that it can discover the right as in synthetic examples without gradient information using both noiseless and noisy observations. we demonstrate that our method is able to discover the same as as the classical approach in a challenging one-hundred-dimensional problem involving an elliptic stochastic partial differential equation with random conductivity. finally, we use our approach to study the effect of geometric and material uncertainties in the propagation of solitary waves in a one dimensional granular system.},
archiveprefix = {arxiv},
arxivid = {1602.04550},
author = {tripathy, rohit and bilionis, ilias and gonzalez, marcial},
doi = {10.1016/j.jcp.2016.05.039},
eprint = {1602.04550},
file = {:users/roy/documents/knowledge/bibliographie/2016/tripathy, bilionis, gonzalez{\_}gaussian processes with built-in dimensionality reduction applications to high-dimensional uncertainty prop.pdf:pdf},
issn = {10902716},
journal = {journal of computational physics},
keywords = {active subspace,dimensionality reduction,gaussian process regression,granular crystals,stiefel manifold,uncertainty quantification},
title = {{gaussian processes with built-in dimensionality reduction: applications to high-dimensional uncertainty propagation}},
year = {2016}
}
@inproceedings{keane2000,
abstract = {the design of the wings for a transonic civil trans- port aircraft is an extremely complex task. it is nor- mally undertaken over an extended time period and at a variety of levels of complexity. typically, simple empirical models are use at the earliest stages of con- cept design, followed by ever more complex methods as the design process proceeds towards the final de- tailed stages. moreover, this process is increasing- ly dominated by computational methods for both analysis (such as cfd) and synthesis (such as opti- mization). in this paper the further development of the southampton wing design environment is de- scribed. this problem solving environment (pse) combines multiple analysis methods of differing lev- els of complexity with powerful search and optimiza- tion techniques. a particular focus of the research is the use of evolutionary search strategies, such as genetic algorithms, and of data fusion between the results of various analysis codes when applied to con- cept design problems.},
author = {keane, a j and petruzzell, n},
booktitle = {8th aiaa/usaf/nasa/issmo symposium on multidisciplinary analysis and optimization},
file = {:users/roy/documents/knowledge/bibliographie/2000/keane, petruzzell{\_}aircraft wing design using ga-based multi-level strategies.pdf:pdf},
title = {{aircraft wing design using ga-based multi-level strategies}},
year = {2000}
}
@article{obrien2016,
abstract = {numerous facets of scientific research implicitly or explicitly call for the estimation of probability densities. histograms and kernel density estimates (kdes) are two commonly used techniques for estimating such information, with the kde generally providing a higher fidelity representation of the probability density function (pdf). both methods require specification of either a bin width or a kernel bandwidth. while techniques exist for choosing the kernel bandwidth optimally and objectively, they are computationally intensive, since they require repeated calculation of the kde. a solution for objectively and optimally choosing both the kernel shape and width has recently been developed by bernacchia and pigolotti (2011). while this solution theoretically applies to multidimensional kdes, it has not been clear how to practically do so. a method for practically extending the bernacchia-pigolotti kde to multidimensions is introduced. this multidimensional extension is combined with a recently-developed computational improvement to their method that makes it computationally efficient: a 2d kde on 105 samples only takes 1 s on a modern workstation. this fast and objective kde method, called the fastkde method, retains the excellent statistical convergence properties that have been demonstrated for univariate samples. the fastkde method exhibits statistical accuracy that is comparable to state-of-the-science kde methods publicly available in r, and it produces kernel density estimates several orders of magnitude faster. the fastkde method does an excellent job of encoding covariance information for bivariate samples. this property allows for direct calculation of conditional pdfs with fastkde. it is demonstrated how this capability might be leveraged for detecting non-trivial relationships between quantities in physical systems, such as transitional behavior.},
author = {o'brien, travis a. and kashinath, karthik and cavanaugh, nicholas r. and collins, william d. and o'brien, john p.},
doi = {10.1016/j.csda.2016.02.014},
file = {:users/roy/documents/knowledge/bibliographie/2016/o'brien et al.{\_}a fast and objective multidimensional kernel density estimation method fastkde.pdf:pdf},
issn = {01679473},
journal = {computational statistics and data analysis},
keywords = {ecf,empirical characteristic function,histogram,kde,kernel density estimation,multidimensional,nonuniform fft,nufft},
pages = {148--160},
publisher = {elsevier b.v.},
title = {{a fast and objective multidimensional kernel density estimation method: fastkde}},
volume = {101},
year = {2016}
}
@article{thomas2009,
author = {thomas, r. h.},
doi = {10.1093/rpd/ncp151},
file = {:users/roy/documents/knowledge/bibliographie/2009/thomas{\_}the black swan - the impact of the highly improbable.pdf:pdf},
isbn = {9780713999952},
issn = {0144-8420},
journal = {radiation protection dosimetry},
month = {aug},
number = {1},
pages = {62--65},
title = {{the black swan - the impact of the highly improbable}},
volume = {136},
year = {2009}
}
@article{saltelli2002,
abstract = {this paper deals with computations of sensitivity indices in sensitivity analysis. given a mathematical or computational model y = f(x1,x2,...,xk), where the input factors xi's are uncorrelated with one another, one can see y as the realization of a stochastic process obtained by sampling each of the xi from its marginal distribution. the sensitivity indices are related to the decomposition of the variance of y into terms either due to each xi taken singularly (first order indices), as well as into terms due to the cooperative effects of more than one xi. in this paper we assume that one has computed the full set of first order sensitivity indices as well as the full set of total-order sensitivity indices (a fairly common strategy in sensitivity analysis), and show that in this case the same set of model evaluations can be used to compute double estimates of: the total effect of two factors taken together, for all such (2k) couples, where k is the dimensionality of the model; the total effect of k-2 factors taken together, for all (2k) such (k-2) ples. we further introduce a new strategy for the computation of the full sets of first plus total order sensitivity indices that is about 50{\%} cheaper in terms of model evaluations with respect to previously published works. we discuss separately the case where the input factors xi's are not independent from each other. {\textcopyright} 2002 elsevier science b.v. all rights reserved.},
author = {saltelli, andrea},
doi = {10.1016/s0010-4655(02)00280-1},
file = {:users/roy/documents/knowledge/bibliographie/2002/saltelli{\_}making best use of model evaluations to compute sensitivity indices.pdf:pdf},
issn = {00104655},
journal = {computer physics communications},
keywords = {importance measures,sensitivity analysis,sensitivity indices,sensitivity measures},
month = {may},
number = {2},
pages = {280--297},
title = {{making best use of model evaluations to compute sensitivity indices}},
volume = {145},
year = {2002}
}
@article{talnikar2015,
address = {reston, virginia},
author = {talnikar, chaitanya a and blonigan, patrick j and bodart, julien and wang, qiqi},
doi = {10.2514/6.2015-1954},
file = {:users/roy/documents/knowledge/bibliographie/2015/talnikar et al.{\_}optimization with les – algorithms for dealing with sampling error of turbulence statistics.pdf:pdf},
isbn = {9781624103438},
journal = {53rd aiaa aerospace sciences meeting},
month = {jan},
pages = {1--11},
publisher = {american institute of aeronautics and astronautics},
title = {{optimization with les – algorithms for dealing with sampling error of turbulence statistics}},
year = {2015}
}
@phdthesis{sanjose2009,
abstract = {aeronautical gas turbines are facing growing demands on emission reductions. indeed, the quality of the air-fuel mixture directly triggers the formation of pollutants degrading the environment. large eddy simulation is an accurate numerical method to predict turbulent mixing in combustors. adding the liquid phase of the fuel in these simulations also becomes necessary to properly predict the injection process and the vaporization of the fuel in the combustion chamber. the purpose of this dissertation is to evaluate the accuracy and reliability of euler-euler les in a complex combustor configuration. the injection and vaporization processes of the fuel liquid phase are both modeled in the present les as they drive the formation of the fuel gas phase. moreover, the numerical methods that solve the continuous equations of the disperse phase must be accurate and robust in realistic combustor configurations. the simulations shown in the present study reproduce the non-reactive two-phase flow of the onera mercato test bench. the experimental set-up is equipped with an air-swirler injection system and a pressure-swirled atomizer typical of actual turboengine combustors. in the present work the fim-ur liquid injection model has been developed. it creates boundary conditions profiles for a liquid spray produced by a pressure-swirled atomizer. kerosene used in the experiments is modeled in the present numerical simulations by a single species leading to a good estimate of the vaporization rate. three numerical strategies have been tested on the mercato configuration. comparisons between experimental and les results help defining the most accurate numerical strategy. the use of the centered numerical scheme ttgc stabilized by a localized artificial viscosity operator is best when the random uncorrelated energy of droplets is also resolved. unlike an upwind numerical scheme, the selected strategy allows the user to control where and how much artificial viscosity is added. the source terms coming from the mesoscopic movement redistribute the energy in the compression or expanding zones of the disperse phase, and provide the proper distribution of fluctuations in the combustion chamber. the obtained strategy is compared with the statistics provided by a lagrangian description of the liquid spray in the same mono-disperse injection. the euler-euler approach leads to the same accuracy in the same spray dynamics of the disperse phase as in the euler-lagrange method. both unsteady flow simulations also provide the same dispersion and mixing processes in the mercato set-up. differences on the mean diameter and the fuel distribution in the combustion chamber are seen and related to the local poly-dispersion that cannot be resolved in the mono-disperse euler-euler approach and that naturally appear in the euler-lagrange method despite the mono-disperse injection.},
author = {sanjose, marl{\`{e}}ne},
file = {:users/roy/documents/knowledge/bibliographie/2009/sanjose{\_}evaluation de la m{\'{e}}thode euler-euler pour la simulation num{\'{e}}rique aux gandes {\'{e}}chelles des chambres {\`{a}} carburant liquide.pdf:pdf},
keywords = {euler-euler approach,large eddy simulation,injection,kerosene,liquid,two-phase flow,vaporization},
school = {universit{\'{e}} de toulouse},
title = {{evaluation de la m{\'{e}}thode euler-euler pour la simulation num{\'{e}}rique aux gandes {\'{e}}chelles des chambres {\`{a}} carburant liquide}},
year = {2009}
}
@article{saltelli2010,
abstract = {variance based methods have assessed themselves as versatile and effective among the various available techniques for sensitivity analysis of model output. practitioners can in principle describe the sensitivity pattern of a model y = f (x1, x2, ..., xk) with k uncertain input factors via a full decomposition of the variance v of y into terms depending on the factors and their interactions. more often practitioners are satisfied with computing just k first order effects and k total effects, the latter describing synthetically interactions among input factors. in sensitivity analysis a key concern is the computational cost of the analysis, defined in terms of number of evaluations of f (x1, x2, ..., xk) needed to complete the analysis, as f (x1, x2, ..., xk) is often in the form of a numerical model which may take long processing time. while the computational cost is relatively cheap and weakly dependent on k for estimating first order effects, it remains expensive and strictly k-dependent for total effect indices. in the present note we compare existing and new practices for this index and offer recommendations on which to use. {\textcopyright} 2009 elsevier b.v. all rights reserved.},
author = {Saltelli, Andrea and Annoni, Paola and Azzini, Ivano and Campolongo, Francesca and Ratto, Marco and Tarantola, Stefano},
doi = {10.1016/j.cpc.2009.09.018},
file = {:users/roy/documents/knowledge/bibliographie/2010/saltelli et al.{\_}variance based sensitivity analysis of model output. design and estimator for the total sensitivity index.pdf:pdf},
isbn = {0010-4655},
issn = {00104655},
journal = {Computer Physics Communications},
number = {2},
pages = {259--270},
publisher = {Elsevier b.v.},
title = {{Variance based sensitivity analysis of model output. design and estimator for the total sensitivity index}},
volume = {181},
year = {2010}
}
@inproceedings{correll2017,
author = {correll, michael and heer, jeffrey},
booktitle = {chi conference on human factors in computing systems},
doi = {10.1145/3025453.3025922},
file = {:users/roy/documents/knowledge/bibliographie/2017/correll, heer{\_}regression by eye estimating trends in bivariate visualizations.pdf:pdf},
title = {{regression by eye : estimating trends in bivariate visualizations}},
year = {2017}
}
@article{lucor2007,
abstract = {we address the sensitivity of large-eddy simulations (les) to parametric uncertainty in the subgrid-scale model.more specifically, we investigate the sensitivity of the les statistical moments of decaying homogeneous isotropic turbulence to the uncertainty in the smagorinsky model free parameter cs (i.e. the smagorinsky constant). our sensitivity methodology relies on the non-intrusive approach of the generalized polynomial chaos (gpc) method; the gpc is a spectral non-statistical numerical method well-suited to representing random processes not restricted to gaussian fields. the analysis is carried out at re$\lambda$=100 and for different grid resolutions and cs distributions. numerical predictions are also compared to direct numerical simulation evidence. we have shown that the different turbulent scales of the les solution respond differently to the variability in cs. in particular, the study of the relative turbulent kinetic energy distributions for different cs distributions indicates that small scales are mainly affected by changes in the subgrid-model parametric uncertainty.},
author = {Lucor, Didier and Meyers, Johan and Sagaut, Pierre},
doi = {10.1017/s0022112007006751},
file = {:users/roy/documents/knowledge/bibliographie/2007/lucor, meyers, sagaut{\_}sensitivity analysis of large-eddy simulations to subgrid-scale-model parametric uncertainty using polynomial chao.pdf:pdf},
issn = {0022-1120},
journal = {Journal of Fluid Mechanics},
month = {aug},
pages = {255--279},
title = {{Sensitivity analysis of large-eddy simulations to subgrid-scale-model parametric uncertainty using polynomial chaos}},
volume = {585},
year = {2007}
}
@techreport{deloach,
abstract = {this paper is a tutorial introduction to the analysis of variance (anova), intended as a reference for aerospace researchers who are being introduced to the analytical methods of the modern design of experiments (mdoe), or who may have other opportunities to apply this method. one-way and two-way fixed-effects anova, as well as random effects anova, are illustrated in practical terms that will be familiar to most practicing aerospace researchers. i.},
author = {deloach, richard},
booktitle = {aiaa},
file = {:users/roy/documents/knowledge/bibliographie/2010/deloach{\_}analysis of variance in the modern design of experiments.pdf:pdf},
pages = {1--36},
title = {{analysis of variance in the modern design of experiments}},
year = {2010}
}
@inproceedings{ahlfeld2016,
abstract = {multi-fidelity uncertainty quantification using rans and dns for turbulent flow is explored. the objective is to achieve statistical results of higher quality than obtainable through rans simulation but with significantly lower cost than by using dns. the novelty of this work is that previous research on multi-fidelity modeling for uncertainty quantification in cfd has only investigated the use of partially converged or coarser mesh solutions as lower-fidelity models. this ensured a high correlation and low bias be- tween the two models. a multi-fidelity solution combining rans and dns is potentially more beneficial but also more difficult to achieve. the use of two models with such large conceptual differences makes the correlation between the models more challenging. we therefore investigate two recent methods for model combination: a multi-grid approach of nested optimal gaussian sparse grids and weighted regression favoring high-fidelity collocation points. the results show that a beneficial connection between the two models is indeed difficult, because the correlation between the responses is overall low. however, there are quantities for which the correlation is high enough and others for which a suf- ficiently high correlation can be achieved using trust-region correction methods. 1.},
author = {ahlfeld, by r and laizet, s and montomoli, f and geraci, g and iaccarino, g},
booktitle = {center for turbulence research annual research briefs},
file = {:users/roy/documents/knowledge/bibliographie/2016/ahlfeld et al.{\_}multi-fidelity uncertainty quantification using rans and dns.pdf:pdf},
title = {{multi-fidelity uncertainty quantification using rans and dns}},
year = {2016}
}
@article{quinton2016,
abstract = {sound is potentially an effective way of analysing data and it is possible to simultaneously interpret layers of sounds and identify changes. multiple attempts to use sound with scientific data have been made, with varying levels of success. on many occasions this was done without including the end user during the development. in this study a sonified model of the 8 planets of our solar system was built and tested using an end user approach. the sonification was created for the esplora planetarium, which is currently being constructed in malta. the data requirements were gathered from a member of the planetarium staff, and 12 end users, as well as the planetarium representative tested the sonification. the results suggest that listeners were able to discern various planetary characteristics without requiring any additional information. three out of eight sound design parameters did not represent characteristics successfully. these issues have been identified and further development will be conducted},
author = {quinton, michael and mcgregor, iain and benyon, david},
doi = {10.21785/icad2016.003},
file = {:users/roy/documents/knowledge/bibliographie/2016/quinton, mcgregor, benyon{\_}sonifying the solar system.pdf:pdf},
isbn = {0-9670904-3-1},
journal = {22nd international conference on auditory display (icad-2016)},
title = {{sonifying the solar system}},
year = {2016}
}
@article{degennaro2015b,
abstract = {the formation and accretion of ice on the leading edge of an airfoil can be detrimental to aerodynamic performance. furthermore, the geometric shape of leading edge ice profiles can vary significantly depending on a wide range of physical parameters, which can translate into a wide variability in aerodynamic performance. the purpose of this work is to explore the variability in airfoil aerodynamic performance that results from variability in leading edge ice shape profile. first, we demonstrate how to identify a low-dimensional set of parameters that governs ice shape from a database of ice shapes using proper orthogonal decomposition (pod). then, we investigate the effects of uncertainty in the pod coefficients. this is done by building a global response surface surrogate using polynomial chaos expansions (pce). to construct this surrogate efficiently, we use adaptive sparse grid sampling of the pod parameter space. we then analyze the data from a statistical standpoint.},
archiveprefix = {arxiv},
arxivid = {1505.07844},
author = {degennaro, anthony m. and rowley, clarence w. and martinelli, luigi},
doi = {10.2514/1.c032698},
eprint = {1505.07844},
file = {:users/roy/documents/knowledge/bibliographie/2015/degennaro, rowley, martinelli{\_}data-driven low-dimensional modeling and uncertainty quantification for airfoil icing.pdf:pdf},
issn = {0021-8669},
journal = {arxiv preprint},
month = {may},
number = {5},
pages = {1--16},
title = {{data-driven low-dimensional modeling and uncertainty quantification for airfoil icing}},
volume = {52},
year = {2015}
}
@article{ndiaye2015,
author = {ndiaye, a and bauerheim, m. and moreau, s. and nicoud, f.},
file = {:users/roy/documents/knowledge/bibliographie/2015/ndiaye et al.{\_}uncertainty quantification of thermoacoustic instabilities in a swirled stabilized combustor.pdf:pdf},
journal = {proceedings of the asme turbo expo 2015},
title = {{uncertainty quantification of thermoacoustic instabilities in a swirled stabilized combustor.}},
year = {2015}
}

@article{hermann2008,
abstract = {sonification is still a relatively young research field and many terms such as sonification, auditory display, aural- ization, audification have been used without a precise def- inition. recent developments such as the introduction of model-based sonification, the establishment of interactive sonification and the increased interest in sonification from arts have raised the need to revisit the definitions in order to move towards a clearer terminology. this paper intro- duces a new definition for sonification and auditory display that emphasizes the necessary and sufficient conditions for organized sound to be called sonification. it furthermore suggests a taxonomy, and discusses the relation between vi- sualization and sonification. a hierarchy of closed-loop in- teractions is furthermore introduced. this paper aims to ini- tiate vivid discussion towards the establishment of a deeper theory of sonification and auditory display.},
author = {hermann, thomas},
file = {:users/roy/documents/knowledge/bibliographie/2008/hermann{\_}taxonomy and definitions for sonification and auditory display thomas hermann neuroinformatics group faculty of technology , bie.pdf:pdf},
pages = {1--8},
title = {{taxonomy and definitions for sonification and auditory display thomas hermann neuroinformatics group faculty of technology , bielefeld university , bielefeld , germany}},
year = {2008}
}
@inproceedings{guo2016,
abstract = {in aerodynamics related design, analysis and optimization problems, flow fields are simulated using computational fluid dynamics (cfd) solvers. however, cfd simulation is usu- ally a computationally expensive, memory demanding and time consuming iterative process. these drawbacks of cfd limit opportunities for design space exploration and forbid interactive design. we propose a general and flexible ap- proximation model for real-time prediction of non-uniform steady laminar flow in a 2d or 3d domain based on convo- lutional neural networks (cnns). we explored alternatives for the geometry representation and the network architec- ture of cnns. we show that convolutional neural networks can estimate the velocity field two orders of magnitude faster than a gpu-accelerated cfd solver and four orders of mag- nitude faster than a cpu-based cfd solver at a cost of a low error rate. this approach can provide immediate feedback for real-time design iterations at the early stage of design. compared with existing approximation models in the aero- dynamics domain, cnns enable an efficient estimation for the entire velocity field. furthermore, designers and engi- neers can directly apply the cnn approximation model in their design space exploration algorithms without training extra lower-dimensional surrogate models.},
author = {guo, xiaoxiao and li, wei and iorio, francesco},
booktitle = {22nd acm sigkdd international conference on knowledge discovery and data mining},
doi = {10.1145/2939672.2939738},
file = {:users/roy/documents/knowledge/bibliographie/2016/guo, li, iorio{\_}convolutional neural networks for steady flow approximation.pdf:pdf},
keywords = {compu- tational fluid dynamics,convolutional neural networks,machine learning,surrogate models},
pages = {481--490},
title = {{convolutional neural networks for steady flow approximation}},
year = {2016}
}
@article{bastine2015,
abstract = {we apply a modified proper orthogonal decomposition (pod) to large eddy simulation data of a wind turbine wake in a turbulent atmospheric boundary layer. the turbine is modeled as an actuator disk. our analysis mainly focuses on the pragmatic identification of spatial modes, which yields a low order description of the wake flow. this reduction to a few degrees of freedom is a crucial first step for the development of simplified dynamic wake models based on modal decompositions. it is shown that only a few modes are necessary to capture the basic dynamical aspects of quantities that are relevant to a turbine in the wake flow. furthermore, we show that the importance of the individual modes depends on the relevant quantity chosen. therefore, the optimal choice of modes for a possible model could in principle depend on the application of interest. we additionally present a possible interpretation of the extracted modes by relating them to the specific properties of the wake. for example, the first mode is related to the horizontal large-scale movement.},
archiveprefix = {arxiv},
arxivid = {1409.1150},
author = {bastine, david and witha, bj{\"{o}}rn and w{\"{a}}chter, matthias and peinke, joachim},
doi = {10.3390/en8020895},
eprint = {1409.1150},
file = {:users/roy/documents/knowledge/bibliographie/2015/bastine et al.{\_}towards a simplified dynamicwake model using pod analysis.pdf:pdf},
isbn = {1996-1073},
issn = {1996-1073},
journal = {energies},
keywords = {"wake,actuator disk,dynamic wake,large eddy simulations,large eddy simulations (les),les,loads,meandering,pod,proper orthogonal decomposition,proper orthogonal decomposition (pod),reduced order model,reduced order model",wake,wake model},
number = {2},
pages = {895--920},
title = {{towards a simplified dynamicwake model using pod analysis}},
volume = {8},
year = {2015}
}
@incollection{tartakovsky2016a,
abstract = {parametric uncertainty, considered broadly to include uncertainty in system parameters and driving forces (source terms and initial and boundary condi- tions), is ubiquitous in mathematical modeling. the method of distributions, which comprises pdf and cdf methods, quantifies parametric uncertainty by deriving deterministic equations for either probability density function (pdf) or cumulative distribution function (cdf) of model outputs. since it does not rely on finite-term approximations (e.g., a truncated karhunen-lo{\`{e}}ve transformation) of random parameter fields, the method of distributions does not suffer from the “curse of dimensionality.” on the contrary, it is exact for a class of nonlinear hyperbolic equations whose coefficients lack spatiotemporal correlation, i.e., exhibit an infinite number of random dimensions.},
author = {tartakovsky, daniel m and gremaud, pierre a},
booktitle = {handbook of uncertainty quantification},
doi = {10.1007/978-3-319-11259-6_27-1},
file = {:users/roy/documents/knowledge/bibliographie/2015/tartakovsky, gremaud{\_}method of distributions for uncertainty quantification.pdf:pdf},
isbn = {978-3-319-12384-4},
keywords = {colored noise,cumulative distribution function (cdf),fokker-planck equation,langevin equation,probability density function (pdf),random,stochastic,white noise},
pages = {1--22},
publisher = {springer international publishing},
title = {{method of distributions for uncertainty quantification}},
year = {2015}
}
@inproceedings{girshick2014,
abstract = {can a large convolutional neural network trained for whole-image classification on imagenet be coaxed into detecting objects in pascal? we show that the answer is yes, and that the resulting system is simple, scalable, and boosts mean average precision, relative to the venerable deformable part model, by more than 40{\%} (achieving a final map of 48{\%} on voc 2007). our framework combines powerful computer vision techniques for generating bottom-up region proposals with recent advances in learning high-capacity convolutional neural networks. we call the resulting system r-cnn: regions with cnn features. the same framework is also competitive with state-of-the-art semantic segmentation methods, demonstrating its flexibility. beyond these results, we execute a battery of experiments that provide insight into what the network learns to represent, revealing a rich hierarchy of discriminative and often semantically meaningful features.},
archiveprefix = {arxiv},
arxivid = {1311.2524},
author = {girshick, ross and donahue, jeff and darrell, trevor and malik, jitendra},
booktitle = {2014 ieee conference on computer vision and pattern recognition},
doi = {10.1109/cvpr.2014.81},
eprint = {1311.2524},
file = {:users/roy/documents/knowledge/bibliographie/2014/girshick et al.{\_}rich feature hierarchies for accurate object detection and semantic segmentation.pdf:pdf},
isbn = {978-1-4799-5118-5},
issn = {10636919},
month = {jun},
pages = {580--587},
pmid = {26656583},
publisher = {ieee},
title = {{rich feature hierarchies for accurate object detection and semantic segmentation}},
year = {2014}
}
@article{lecun1998,
archiveprefix = {arxiv},
arxivid = {1102.0183},
author = {lecun, y. and bottou, l. and bengio, y. and haffner, p.},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:users/roy/documents/knowledge/bibliographie/1998/lecun et al.{\_}gradient-based learning applied to document recognition.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {proceedings of the ieee},
number = {11},
pages = {2278--2324},
pmid = {15823584},
title = {{gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{cannamela2013,
abstract = {complex computer codes are widely used in science and engineering to model physical phenomena. furthermore, it is common that they have a large number of input parameters. global sensitivity analysis aims to identify those which have the most important impact on the output. sobol indices are a popular tool to perform such analysis. however, their estimations require an important number of simulations and often cannot be processed under reasonable time constraint. to handle this problem, a gaussian process regression model is built to surrogate the computer code and the sobol indices are estimated through it. the aim of this paper is to provide a methodology to estimate the sobol indices through a surrogate model taking into account both the estimation errors and the surrogate model errors. in particular, it allows us to derive non-asymptotic confidence intervals for the sobol index estimations. furthermore, we extend the suggested strategy to the case of multi-fidelity computer codes which can be run at different levels of accuracy. for such simulators, we use an extension of gaussian process regression models for multivariate outputs.},
archiveprefix = {arxiv},
arxivid = {1307.2223},
author = {gratiet, loic le and cannamela, claire and iooss, bertrand},
eprint = {1307.2223},
file = {:users/roy/documents/knowledge/bibliographie/2013/gratiet, cannamela, iooss{\_}a bayesian approach for global sensitivity analysis of (multi-fidelity) computer codes.pdf:pdf},
keywords = {bayesian analysis,complex,computer codes,gaussian process regression,multi-fidelity model,sensitivity analysis,sobol index},
month = {jul},
pages = {1--31},
title = {{a bayesian approach for global sensitivity analysis of (multi-fidelity) computer codes}},
year = {2013}
}
@article{stein1987,
abstract = {latin hypercube sampling (mckay, conover, and beckman 1979) is a method of sampling that can be used to produce input values for estimation of expectations of functions of output variables. the asymptotic variance of such an estimate is obtained. the estimate is also shown to be asymptotically normal. asymptotically, the variance is less than that obtained using simple random sampling, with the degree of variance reduction depending on the degree of additivity in the function being integrated. a method for producing latin hypercube samples when the components of the input variables are statistically dependent is also described. these techniques are applied to a simulation of the performance of a printer actuator.},
author = {stein, michael},
file = {:users/roy/documents/knowledge/bibliographie/1987/stein{\_}large sample properties of simulations using latin hypercube sampling.pdf:pdf},
journal = {technometrics},
keywords = {exchangeability,rank pro-cedure,sampling with dependent random variables,variance reduction},
number = {2},
title = {{large sample properties of simulations using latin hypercube sampling}},
volume = {29},
year = {1987}
}
@article{liu2012,
abstract = {anomalies are data points that are few and different. as a result of these properties, we show that, anomalies are susceptible to a mechanism called isolation. this article proposes a method called isolation forest (iforest), which detects anomalies purely based on the concept of isolation without employing any distance or density measure-fundamentally different from all existing methods. as a result, iforest is able to exploit subsampling (i) to achieve a low linear time-complexity and a small memory-requirement and (ii) to deal with the effects of swamping and masking effectively. our empirical evaluation shows that iforest outperforms orca, one-class svm, lof and random forests in terms of auc, processing time, and it is robust against masking and swamping effects. iforest also works well in high dimensional problems containing a large number of irrelevant attributes, and when anomalies are not available in training sample. {\textcopyright} 2012 acm 1556-4681/2012/03-art3 {\$}10.00.},
author = {liu, fei tony and ting, kai ming and zhou, zhi-hua},
doi = {10.1145/2133360.2133363},
file = {:users/roy/documents/knowledge/bibliographie/2012/liu, ting, zhou{\_}isolation-based anomaly detection.pdf:pdf},
isbn = {1556-4681},
issn = {15564681},
journal = {acm transactions on knowledge discovery from data},
number = {1},
pages = {1--39},
title = {{isolation-based anomaly detection}},
volume = {6},
year = {2012}
}

@article{evensen1994,
author = {evensen, geir},
file = {:users/roy/documents/knowledge/bibliographie/1994/evensen{\_}sequential data assimilation with a nonlinear quasi-geostrophic model using monte carlo methods to forecast error statistics.pdf:pdf},
journal = {journal of geophysical research},
number = {c5},
pages = {143--162},
title = {{sequential data assimilation with a nonlinear quasi-geostrophic model using monte carlo methods to forecast error statistics}},
volume = {99},
year = {1994}
}
@article{forrester2006,
abstract = {efficient methods for global aerodynamic optimization using computational fluid dynamics simulations should aim to reduce both the time taken to evaluate design concepts and the number of evaluations needed for optimization. this paper investigates methods for improving such efficiency through the use of partially converged computational fluid dynamics results. these allow surrogate models to be built in a fraction of the time required for models based on converged results. the proposed optimization methodologies increase the speed of convergence to a global optimum while the computer resources expended in areas of poor designs are reduced. a strategy which combines a global approximation built using partially converged simulations with expected improvement updates of converged simulations is shown to outperform a traditional surrogate-based optimization.},
author = {Forrester, Alexander I.J and Bressloff, Neil W. and Keane, Andy J.},
doi = {10.1098/rspa.2006.1679},
file = {:users/roy/documents/knowledge/bibliographie/2006/forrester, bressloff, keane{\_}optimization using surrogate models and partially converged computational fluid dynamics simulations.pdf:pdf},
isbn = {1364-5021},
issn = {1364-5021},
journal = {Proceedings of the Royal Society A: mathematical, physical and engineering sciences},
keywords = {computational fluid dynamics,data fusion,design of experiment,kriging},
number = {2071},
pages = {2177 --2204},
title = {{Optimization using surrogate models and partially converged computational fluid dynamics simulations}},
volume = {462},
year = {2006}
}
@article{poore1999,
abstract = {seifried provides an overview of cryptography and an$\backslash$nintroduction to open source cryptographic software.},
author = {poore, ralph spencer},
doi = {10.1201/1086/43305.8.2.19990601/31062.6},
file = {:users/roy/documents/knowledge/bibliographie/1999/poore{\_}crypto' 101.pdf:pdf},
issn = {1065-898x},
journal = {information systems security},
month = {jun},
number = {2},
pages = {26--26},
title = {{crypto' 101}},
volume = {8},
year = {1999}
}
@article{niederreiter1988,
abstract = {we generalize and improve earlier constructions of low-discrepancy sequences by sobol', faure, and the author, thus obtaining sequences in the s-dimensional unit cube with the smallest discrepancy that is currently known. the construction is based on the theory of (t, s)-sequences. it is also shown that the dispersion of the sequences constructed here has the smallest possible order of magnitude among any sequences in the s-dimensional unit cube. ?? 1988.},
author = {niederreiter, harald},
doi = {10.1016/0022-314x(88)90025-x},
file = {:users/roy/documents/knowledge/bibliographie/1988/niederreiter{\_}low-discrepancy and low-dispersion sequences.pdf:pdf},
issn = {0022314x},
journal = {journal of number theory},
month = {sep},
number = {1},
pages = {51--70},
title = {{low-discrepancy and low-dispersion sequences}},
volume = {30},
year = {1988}
}
@inproceedings{granados-ortiz2016,
abstract = {the aim of cfd simulations is to model and compute the ideal performance of a flow under some particular conditions. a classic approach is to perform simulations with fixed parameters and boundary conditions. however, this is not accurate enough due to the fact that under realistic conditions, some parameters may be uncertain. in recent years, the interest of undertaking the simulations under uncertainty is increasing, but is not yet a common rule and 'incomplete' simulations are still taking place. this procedure could be missing information such as whether mechanical tolerances are influential in dramatic parts of the flow or the relevancy in accurate tunning of turbulence models. taking this knowledge into consideration, non-intrusive uncertainty quantification (uq) has been applied to 3d rans simulations of an under-expanded jet, in order to understand the impact of input uncertainties. results show that some regions of the jet plume are very sensitive to a combination of both physical and turbulence model variance. these regions are in fact corresponding to the parts of the jet where screech and shock-cell noise is generated, so this can be an indicator of a relevant impact of uncertainties in jet noise emmission, what can guide to future research and more robust developments in aircraft industry.},
address = {reston, virginia},
author = {granados-ortiz, francisco j. and {perez arroyo}, carlos and lai, choi-hong and puigt, guillaume and airiau, christophe},
booktitle = {46th aiaa fluid dynamics conference},
doi = {10.2514/6.2016-4091},
file = {:users/roy/documents/knowledge/bibliographie/2016/granados-ortiz et al.{\_}uncertainty quantification and sensitivity analysis applied to an under-expanded single jet.pdf:pdf},
isbn = {978-1-62410-436-7},
month = {jun},
organization = {aiaa},
publisher = {american institute of aeronautics and astronautics},
title = {{uncertainty quantification and sensitivity analysis applied to an under-expanded single jet}},
year = {2016}
}
@article{krige1989,
abstract = {the main developments during four decades of geostatistical applications on the witwatersrand-type gold deposits are reviewed and critically compared with a view to highlighting the progress made to date. the basic techniques - from the preliminary borehole valuation of mining properties to the block estimating of ore reserves and foreseeable recoveries are analysed and the prospects for methodological improvements are discussed. the techniques examined include the first elementary kriging procedure of regression (krige, 1952), first spatial structure modelling through the variance-size of area analysis (krige, 1952), and all the main subsequently developed kriging and variography techniques applied to the witwatersrand deposits. data from the original 91 boreholes covering the major part of the orange free state gold field as well as sampling data from loraine mine (over 24 000 items) provide the information base for the analysis. the achievements are measured and compared using error variances and considering the total relative profits to be realised from the selective mining of an orebody. 1.},
author = {krige, d g and guarascio, m and camisani-calzolari, f a},
file = {:users/roy/documents/knowledge/bibliographie/1989/krige, guarascio, camisani-calzolari{\_}early south african geostatistical techniques in today's perspective.pdf:pdf},
isbn = {9401568464},
journal = {geostatistics},
pages = {1--19},
title = {{early south african geostatistical techniques in today's perspective}},
volume = {1},
year = {1989}
}
@article{jones1998,
abstract = {inmany engineering optimization problems, the number of function evaluations is severely limited by time or cost. these problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. one way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. these surfaces can then be used for visualization, tradeoff analysis, and optimization. in this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. we then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. the key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
author = {jones, donald r and schonlau, matthias and william, j},
doi = {10.1023/a:1008306431147},
file = {:users/roy/documents/knowledge/bibliographie/1998/jones, schonlau, william{\_}efficient global optimization of expensive black-box functions.pdf:pdf},
isbn = {0925-5001},
issn = {09255001},
journal = {journal of global optimization},
keywords = {bayesian global optimization,kriging,process,random function,response surface,stochastic,visualization},
pages = {455--492},
pmid = {21858987},
title = {{efficient global optimization of expensive black-box functions}},
volume = {13},
year = {1998}
}

@article{ribes2015,
author = {Rib{\'{e}}s, Alejandro and Pouderoux, Joachim and Popelin, Anne-Laure and Iooss, Bertrand},
file = {:users/roy/documents/knowledge/bibliographie/2015/rib{\'{e}}s et al.{\_}understanding ensembles by joint interactive statistical analysis of curve datasets and their principal component analysis.pdf:pdf},
keywords = {curve visualization,ensemble visualization,exploratory statistics,uncertainty,visual clutter,visualization},
title = {{Understanding ensembles by joint interactive statistical analysis of curve datasets and their principal component analysis}},
year = {2015}
}
@article{ferretti2016,
abstract = {the majority of published sensitivity analyses (sas) are either local or one factor-at-a-time (oat) analyses, relying on unjustified assumptions of model linearity and additivity. global approaches to sensitivity analyses (gsa) which would obviate these shortcomings, are applied by a minority of researchers. by reviewing the academic literature on sa, we here present a bibliometric analysis of the trends of different sa practices in last decade. the review has been conducted both on some top ranking journals (nature and science) and through an extended analysis in the elsevier's scopus database of scientific publications. after correcting for the global growth in publications, the amount of papers performing a generic sa has notably increased over the last decade. even if oat is still the most largely used technique in sa, there is a clear increase in the use of gsa with preference respectively for regression and variance-based techniques. even after adjusting for the growth of publications in the sole modelling field, to which sa and gsa normally apply, the trend is confirmed. data about regions of origin and discipline are also briefly discussed. the results above are confirmed when zooming on the sole articles published in chemical modelling, a field historically proficient in the use of sa methods.},
author = {ferretti, federico and saltelli, andrea and tarantola, stefano},
doi = {10.1016/j.scitotenv.2016.02.133},
file = {:users/roy/documents/knowledge/bibliographie/2016/ferretti, saltelli, tarantola{\_}trends in sensitivity analysis practice in the last decade.pdf:pdf},
issn = {00489697},
journal = {science of the total environment},
keywords = {bibliometric analysis,chemical modelling,global sensitivity analysis,sensitivity analysis},
month = {oct},
pages = {666--670},
pmid = {26934843},
publisher = {british geological survey, nerc},
title = {{trends in sensitivity analysis practice in the last decade}},
volume = {568},
year = {2016}
}
@article{brodlie2012,
abstract = {most visualization techniques have been designed on the assumption that the data to be represented are free from uncertainty. yet this is rarely the case. recently the visualization community has risen to the challenge of incorporating an indication of uncertainty into visual representations, and in this article we review their work. we place the work in the context of a reference model for data visualization, that sees data pass through a pipeline of processes. this allows us to distinguish the visualization of uncertainty —which considers how we depict uncertainty specified with the data—and the uncertainty of visualization —which considers how much inaccuracy occurs as we process data through the pipeline. it has taken some time for uncertain visualization methods to be developed, and we explore why uncertainty visualization is hard—one explanation is that we typically need to find another display dimension and we may have used these up already! to organize the material we return to a typology developed by one of us in the early days of visualization, and make use of this to present a catalog of visualization techniques describing the research that has been done to extend each method to handle uncertainty. finally we note the responsibility on us all to incorporate any known uncertainty into a visualization, so that integrity of the discipline is maintained.},
archiveprefix = {arxiv},
arxivid = {arxiv:gr-qc/9809069v1},
author = {Brodlie, Ken and {Allendes Osorio}, Rodolfo and Lopes, Adriano},
doi = {10.1007/978-1-4471-2804-5_6},
eprint = {9809069v1},
file = {:users/roy/documents/knowledge/bibliographie/2012/brodlie, allendes osorio, lopes{\_}a review of uncertainty in data visualization.pdf:pdf},
isbn = {978-1-4471-2804-5},
issn = {0717-6163},
journal = {Expanding the Frontiers of Visual Analytics and Visualization},
keywords = {errors,uncertainty,visualization},
pages = {81--109},
pmid = {15003161},
primaryclass = {arxiv:gr-qc},
title = {{A review of Uncertainty in Data Visualization}},
year = {2012}
}
@article{pronzato2012,
abstract = {when setting up a computer experiment, it has become a standard practice to select the inputs spread out uniformly across the available space. these so-called space-filling designs are now ubiquitous in corresponding publications and conferences. the statistical folklore is that such designs have superior properties when it comes to prediction and estimation of emulator functions. in this paper we want to review the circumstances under which this superiority holds, provide some new arguments and clarify the motives to go beyond space-filling. an overview over the state of the art of space-filling is introducing and complementing these results.},
author = {pronzato, luc and m{\"{u}}ller, werner g.},
doi = {10.1007/s11222-011-9242-3},
file = {:users/roy/documents/knowledge/bibliographie/2012/pronzato, m{\"{u}}ller{\_}design of computer experiments space filling and beyond.pdf:pdf},
isbn = {1122201192},
issn = {0960-3174},
journal = {statistics and computing},
keywords = {design of experiments,entropy,kriging,maximin design,minimax design,space-filling,sphere packing},
month = {may},
number = {3},
pages = {681--701},
title = {{design of computer experiments: space filling and beyond}},
volume = {22},
year = {2012}
}
@article{gramacy2012,
abstract = {optimization of complex functions, such as the output of computer$\backslash$nsimulators, is a difficult task that has received much attention$\backslash$nin the literature. a less studied problem is that of optimization$\backslash$nunder unknown constraints, i.e., when the simulator must be invoked$\backslash$nboth to determine the typical real-valued response and to determine$\backslash$nif a constraint has been violated, either for physical or policy$\backslash$nreasons. we develop a statistical approach based on gaussian processes$\backslash$nand bayesian learning to both approximate the unknown function and$\backslash$nestimate the probability of meeting the constraints. a new integrated$\backslash$nimprovement criterion is proposed to recognize that responses from$\backslash$ninputs that violate the constraint may still be informative about$\backslash$nthe function, and thus could potentially be useful in the optimization.$\backslash$nthe new criterion is illustrated on synthetic data, and on a motivating$\backslash$noptimization problem from health care policy.},
archiveprefix = {arxiv},
arxivid = {1004.4027},
author = {gramacy, robert b. and lee, herbert k h and holmes, christopher and osborne, m.},
doi = {10.1093/acprof:oso/9780199694587.003.0008},
eprint = {1004.4027},
file = {:users/roy/documents/knowledge/bibliographie/2012/gramacy et al.{\_}optimization under unknown constraints.pdf:pdf},
isbn = {9780191731921},
journal = {bayesian statistics 9},
keywords = {constrained optimization,expected improvement,gaussian process,sequential design,surrogate model},
pages = {1--19},
title = {{optimization under unknown constraints}},
year = {2012}
}
@article{rude2016,
abstract = {over the past two decades the field of computational science and engineering (cse) has penetrated both basic and applied research in academia, industry, and laboratories to advance discovery, optimize systems, support decision-makers, and educate the scientific and engineering workforce. informed by centuries of theory and experiment, cse performs computational experiments to answer questions that neither theory nor experiment alone is equipped to answer. cse provides scientists and engineers of all persuasions with algorithmic inventions and software systems that transcend disciplines and scales. carried on a wave of digital technology, cse brings the power of parallelism to bear on troves of data. mathematics-based advanced computing has become a prevalent means of discovery and innovation in essentially all areas of science, engineering, technology, and society; and the cse community is at the core of this transformation. however, a combination of disruptive developments---including the architectural complexity of extreme-scale computing, the data revolution that engulfs the planet, and the specialization required to follow the applications to new frontiers---is redefining the scope and reach of the cse endeavor. this report describes the rapid expansion of cse and the challenges to sustaining its bold advances. the report also presents strategies and directions for cse research and education for the next decade.},
archiveprefix = {arxiv},
arxivid = {1610.02608},
author = {r{\"{u}}de, ulrich and willcox, karen and mcinnes, lois curfman and {de sterck}, hans and biros, george and bungartz, hans and corones, james and cramer, evin and crowley, james and ghattas, omar and gunzburger, max and hanke, michael and harrison, robert and heroux, michael and hesthaven, jan and jimack, peter and johnson, chris and jordan, kirk e. and keyes, david e. and krause, rolf and kumar, vipin and mayer, stefan and meza, juan and m{\o}rken, knut martin and oden, j. tinsley and petzold, linda and raghavan, padma and shontz, suzanne m. and trefethen, anne and turner, peter and voevodin, vladimir and wohlmuth, barbara and woodward, carol s.},
eprint = {1610.02608},
file = {:users/roy/documents/knowledge/bibliographie/2016/r{\"{u}}de et al.{\_}research and education in computational science and engineering.pdf:pdf},
month = {oct},
number = {september},
title = {{research and education in computational science and engineering}},
year = {2016}
}

@article{saltelli1999a,
author = {saltelli, andrea},
doi = {10.1029/1998jd100042},
file = {:users/roy/documents/knowledge/bibliographie/1999/saltelli{\_}sensitivity analysis could better methods be used.pdf:pdf},
pages = {3789--3793},
title = {{sensitivity analysis : could better methods be used ?}},
volume = {104},
year = {1999}
}
@article{welch2013a,
abstract = {many scientific phenomena are now investigated by complex computer models or codes. given the input values, the code produces one or more outputs via a complex mathematical model. often the code is expensive to run, and it may be necessary to build a computationally cheaper predictor to enable, for example, optimization of the inputs. if there are many input factors, an initial step in building a predictor is identifying (screening) the active factors. we model the output of the computer code as the realization of a stochastic process. this model has a number of advantages. first, it provides a statistical basis, via the likelihood, for a stepwise algorithm to determine the important factors. second, it is very flexible, allowing nonlinear and interaction effects to emerge without explicitly modeling such effects. third, the same data are used for screening and building the predictor, so expensive runs are efficiently used. we illustrate the methodology with two examples, both having 20 input variables. in these examples, we identify the important variables, detect curvature and in- teractions, and produce a useful predictor with 30-50 runs of the computer code.},
author = {welch, william j and buck, robert j and sacks, jerome and wynn, henry p and toby, j and morris, max d and mitchell, toby j},
file = {:users/roy/documents/knowledge/bibliographie/1992/welch et al.{\_}screening, predicting, and computer experiments.pdf:pdf},
keywords = {computer,data-adaptive,dimension,electronic-,maximum,simulation,stochastic,circuit,code,likelihood,modeling,process,reduction,simulation},
number = {1},
pages = {15--25},
title = {{screening, predicting, and computer experiments}},
volume = {34},
year = {1992}
}
@article{degennaro2015a,
abstract = {the purpose of this study is twofold -- first, to introduce the application of high-order discontinuous galerkin methods to buoyancy-driven cargo hold fire simulations, second, to explore statistical variation in the fluid dynamics of a cargo hold fire given parameterized uncertainty in the fire source location and temperature. cargo hold fires represent a class of problems that require highly-accurate computational methods to simulate faithfully. hence, we use an in-house discontinuous galerkin code to treat these flows. cargo hold fires also exhibit a large amount of uncertainty with respect to the boundary conditions. thus, the second aim of this paper is to quantify the resulting uncertainty in the flow, using tools from the uncertainty quantification community to ensure that our efforts require a minimal number of simulations. we expect that the results of this study will provide statistical insight into the effects of fire location and temperature on cargo fires, and also assist in the optimization of fire detection system placement.},
archiveprefix = {arxiv},
arxivid = {1512.04962},
author = {degennaro, anthony m. and lohry, mark w. and martinelli, luigi and rowley, clarence w.},
eprint = {1512.04962},
file = {:users/roy/documents/knowledge/bibliographie/2015/degennaro et al.{\_}uncertainty quantification for cargo hold fires.pdf:pdf},
isbn = {9781624103926},
month = {dec},
number = {january},
pages = {10},
title = {{Uncertainty quantification for cargo hold fires}},
year = {2015}
}
@article{hyndman2009,
author = {Hyndman, Rob J and Shang, Han Lin},
file = {:users/roy/documents/knowledge/bibliographie/2009/hyndman, shang{\_}rainbow plots, bagplots and boxplots for functional data.pdf:pdf},
journal = {Journal of Computational and Graphical Statistics},
keywords = {highest density regions,kernel density estima-,outlier detection,robust principal component analysis,s halfspace depth,tion,tukey},
pages = {29--45},
title = {{Rainbow plots , bagplots and boxplots for functional data}},
volume = {19},
year = {2009}
}
@inproceedings{vignat2017,
author = {vignat, guillaume and taliercio, guillaume and lamouroux, jean and savary, nicolas and duchaine, patrick},
booktitle = {proceedings of the asme turbo expo 2017},
file = {:users/roy/documents/knowledge/bibliographie/2017/vignat et al.{\_}analysis of performance sensitivity to geometrical variations of a modern helicopter engine combustor using les simulation.pdf:pdf},
pages = {1--11},
title = {{analysis of performance sensitivity to geometrical variations of a modern helicopter engine combustor using les simulations}},
year = {2017}
}
@article{auder2012,
abstract = {to perform uncertainty, sensitivity or optimization analysis on scalar variables calculated by a cpu time expensive computer code, a widely accepted methodology consists in first identifying the most influential uncertain inputs (by screening techniques), and then in replacing the cpu time expensive model by a cpu inexpensive mathematical function, called a metamodel. this paper extends this methodology to the functional output case, for instance when the model output variables are curves. the screening approach is based on the analysis of variance and principal component analysis of output curves. the functional metamodeling consists in a curve classification step, a dimension reduction step, then a classical metamodeling step. an industrial nuclear reactor application (dealing with uncertain- ties in the pressurized thermal shock analysis) illustrates all these steps.},
author = {auder, benjamin and {de crecy}, agn{\`{e}}s and iooss, bertrand and marqu{\`{e}}s, michel},
doi = {10.1016/j.ress.2011.10.017},
file = {:users/roy/documents/knowledge/bibliographie/2012/auder et al.{\_}screening and metamodeling of computer experiments with functional outputs. application to thermal–hydraulic computations.pdf:pdf},
issn = {09518320},
journal = {reliability engineering {\&} system safety},
month = {nov},
pages = {122--131},
title = {{screening and metamodeling of computer experiments with functional outputs. application to thermal–hydraulic computations}},
volume = {107},
year = {2012}
}
@article{box1961,
author = {box, g. e. p. and hunter, j. s.},
file = {:users/roy/documents/knowledge/bibliographie/1961/box, hunter{\_}the 2k fractional factorial designs part i .pdf:pdf},
journal = {technometrics},
number = {3},
pages = {311--351},
title = {{the 2k " fractional factorial designs * part i .}},
volume = {3},
year = {1961}
}
@article{adams2015,
abstract = {the dakota (design analysis kit for optimization and terascale applications) toolkit provides a flexible and extensible interface between simulation codes and iterative analysis methods. dakota contains algorithms for optimization with gradient and nongradient-based methods; uncertainty quantification with sampling, reliabil- ity, and stochastic expansion methods; parameter estimation with nonlinear least squares methods; and sensitiv- ity/variance analysis with design of experiments and parameter study methods. these capabilities may be used on their own or as components within advanced strategies such as surrogate-based optimization, mixed integer nonlinear programming, or optimization under uncertainty. by employing object-oriented design to implement abstractions of the key components required for iterative systems analyses, the dakota toolkit provides a flexible and extensible problem-solving environment for design and performance analysis of computational models on high performance computers. this report serves as a theoretical manual for selected algorithms implemented within the dakota software. it is not intended as a comprehensive theoretical treatment, since a number of existing texts cover general optimization theory, statistical analysis, and other introductory topics. rather, this manual is intended to summarize a set of dakota-related research publications in the areas of surrogate-based optimization, uncertainty quantification, and optimization under uncertainty that provide the foundation for many of dakota's iterative analysis capabilities.},
author = {trucano, t.g. and swiler, l.p. and igusa, t. and oberkampf, w.l. and pilch, m.},
doi = {10.1016/j.ress.2005.11.031},
file = {:users/roy/documents/knowledge/bibliographie/2006/trucano et al.{\_}dakota, a multilevel parallel object-oriented framework for design optimization, parameter estimation, uncertainty quanti.pdf:pdf},
issn = {09518320},
journal = {reliability engineering {\&} system safety},
month = {oct},
number = {10-11},
pages = {1331--1357},
title = {{dakota, a multilevel parallel object-oriented framework for design optimization, parameter estimation, uncertainty quantification, and sensitivity analysis}},
volume = {91},
year = {2006}
}

@book{forrester2008a,
author = {Forrester, Alexander I. J. and S{\'{o}}bester, Andr{\'{a}}s and Keane, Andy J.},
file = {:users/roy/documents/knowledge/bibliographie/2008/forrester, s{\'{o}}bester, keane{\_}engineering design via surrogate modelling.pdf:pdf},
isbn = {9780470060681},
pages = {238},
title = {{Engineering design via surrogate modelling}},
year = {2008}
}
@article{benner2013,
abstract = {numerical simulation of large-scale dynamical systems plays a fundamental role in studying a wide range of complex physical phenomena; however, the inherent large-scale nature of the models leads to unmanageable demands on computational resources. model reduction aims to reduce this computational burden by generating reduced models that are faster and cheaper to simulate, yet accurately represent the original large-scale system behavior. model reduction of linear, non-parametric dynamical systems has reached a considerable level of maturity, as reflected by several survey papers and books. however, parametric model reduction has emerged only more recently as an important and vibrant research area, with several recent advances making a survey paper timely. thus, this paper aims to provide a resource that draws together recent contributions in different communities to survey state-of-the-art in parametric model reduction methods. parametric model reduction targets the broad class of problems for which the equations gov- erning the system behavior depend on a set of parameters. examples include parameterized partial differential equations and large-scale systems of parameterized ordinary differential equations. the goal of parametric model reduction is to generate low cost but accurate mod- els that characterize system response for different values of the parameters. this paper surveys state-of-the-art methods in parametric model reduction, describing the different approaches within each class of methods for handling parametric variation and providing a comparative discussion that lend insights to potential advantages and disadvantages in applying each of the methods. we highlight the important role played by parametric model reduction in design, control, optimization, and uncertainty quantification—settings that require repeated model evaluations over a potentially large range of parameter values. keywords.},
author = {benner, peter and gugercin, s. and willcox, k.},
file = {:users/roy/documents/knowledge/bibliographie/2013/benner, gugercin, willcox{\_}a survey of model reduction methods for parametric systems.pdf:pdf},
journal = {mpi magdeburg preprints},
number = {13-14},
pages = {1--36},
title = {{a survey of model reduction methods for parametric systems}},
volume = {mpimd},
year = {2013}
}
@article{feikema1995,
author = {feikema, douglas a.},
file = {:users/roy/documents/knowledge/bibliographie/1995/feikema{\_}atomization characteristics of swirl injector sprays.pdf:pdf},
title = {{atomization characteristics of swirl injector sprays}},
year = {1995}
}
@misc{schonlau1998,
author = {schonlau, matthias and welch, william j and jones, donald r},
booktitle = {new developments and applications in experimental design},
file = {:users/roy/documents/knowledge/bibliographie/1998/schonlau, welch, jones{\_}global versus local search in constrained optimization of computer models.pdf:pdf},
pages = {11--25},
title = {{global versus local search in constrained optimization of computer models}},
volume = {34},
year = {1998}
}
@article{moritz2017a,
abstract = {pangloss implements " optimistic visualization " , a method that gives analysts confidence to use approximate results for exploratory data analysis. in this paper, we outline how analysts' experience with an approximate visualization system did not match their in-tuitions. these observations have implications for the design of future data exploration systems that expose uncertainty. we also describe requirements for approximate query engines to enable the next generation of exploratory visualization systems.},
author = {moritz, dominik and fisher, danyel},
doi = {10.1145/3077257.3077258},
file = {:users/roy/documents/knowledge/bibliographie/2017/moritz, fisher{\_}what users don't expect about exploratory data analysis on approximate query processing systems.pdf:pdf},
isbn = {9781450350297},
keywords = {keywords approximate query processing,user experience,visualization},
title = {{what users don't expect about exploratory data analysis on approximate query processing systems}},
year = {2017}
}
@article{mandelbrot1967,
author = {mandelbrot, b b},
file = {:users/roy/documents/knowledge/bibliographie/1967/mandelbrot{\_}how long is the coast of britain statistical self-similarity and fractional dimension.pdf:pdf},
journal = {science},
number = {156},
pages = {636--638},
title = {{how long is the coast of britain ? statistical self-similarity and fractional dimension}},
year = {1967}
}
@inproceedings{ng2012,
address = {reston, virigina},
author = {ng, leo wai-tsun and eldred, michael},
booktitle = {53rd aiaa/asme/asce/ahs/asc structures, structural dynamics and materials conference{\textless}br{\textgreater}20th aiaa/asme/ahs adaptive structures conference{\textless}br{\textgreater}14th aiaa},
doi = {10.2514/6.2012-1852},
file = {:users/roy/documents/knowledge/bibliographie/2012/ng, eldred{\_}multifidelity uncertainty quantification using non-intrusive polynomial chaos and stochastic collocation.pdf:pdf},
isbn = {978-1-60086-937-2},
month = {apr},
number = {april},
pages = {1--17},
publisher = {american institute of aeronautics and astronautics},
title = {{multifidelity uncertainty quantification using non-intrusive polynomial chaos and stochastic collocation}},
year = {2012}
}
@article{dodson2008,
abstract = {suppose we have a function f : x → r that we with to minimize on some domain x ⊆ x . that is, we wish to x * = arg min x∈x f (x). in numerical analysis, this problem is typically called (global) optimization and has been the subject of decades of study. we draw a distinction between global optimization, where we seek the absolute optimum in x, and local optimization, where we seek to a local optimum in the neighborhood of a given initial point x 0 . a common approach to optimization problems is to make some assumptions about f . for example, when the objective function f is known to be convex and the domain x is also convex, the problem is known as convex optimization and has been widely studied. convex optimization is a common tool used across machine learning. if an exact functional form for f is not available (that is, f behaves as a " black box "), what can we do? bayesian optimization proceeds by maintaining a probabilistic belief about f and designing a so-called acquisition function to determine where to evaluate the function next. bayesian optimization is particularly well-suited to global optimization problems where f is an expensive black-box function; for example, evaluating f might require running an expensive simulation. bayesian optimization has recently become popular for training expensive machine-learning models whose behavior depend in a complicated way on their parameters (e.g., convolutional neural networks). this is an example of the " automl " paradigm. although not strictly required, bayesian optimization almost always reasons about f by choosing an appropriate gaussian process prior:},
author = {dodson, john},
file = {:users/roy/documents/knowledge/bibliographie/2008/dodson{\_}bayesian optimization.pdf:pdf},
journal = {october},
pages = {1--4},
title = {{bayesian optimization}},
year = {2008}
}
@article{denning2013,
abstract = {this paper presents meshgit, a practical algorithm for diffing and merging polygonal meshes typically used in subdivision modeling workflows. inspired by version control for text editing, we introduce the mesh edit distance as a measure of the dissimilarity between meshes. this distance is defined as the minimum cost of matching the vertices and faces of one mesh to those of another. we propose an iterative greedy algorithm to approximate the mesh edit distance, which scales well with model complexity, providing a practical solution to our problem. we translate the mesh correspondence into a set of mesh editing operations that transforms the first mesh into the second. the editing operations can be displayed directly to provide a meaningful visual difference between meshes. for merging, we compute the difference between two versions and their common ancestor, as sets of editing operations. we robustly detect conflicting operations, automatically apply non-conflicting edits, and allow the user to choose how to merge the conflicting edits. we evaluate meshgit by diffing and merging a variety of meshes and find it to work well for all.},
author = {denning, jonathan d and pellacini, fabio},
doi = {10.1145/2461912.2461942},
file = {:users/roy/documents/knowledge/bibliographie/2013/denning, pellacini{\_}meshgit.pdf:pdf},
isbn = {0730-0301},
issn = {07300301},
journal = {acm transactions on graphics},
month = {jul},
number = {4},
pages = {1},
title = {{meshgit}},
volume = {32},
year = {2013}
}

@article{campolongo2007,
abstract = {in 1991 morris proposed an effective screening sensitivity measure to identify the few important factors in models with many factors. the method is based on computing for each input a number of incremental ratios, namely elementary effects, which are then averaged to assess the overall importance of the input. despite its value, the method is still rarely used and instead local analyses varying one factor at a time around a baseline point are usually employed. in this piece of work we propose a revised version of the elementary effects method, improved in terms of both the definition of the measure and the sampling strategy. in the present form the method shares many of the positive qualities of the variance-based techniques, having the advantage of a lower computational cost, as demonstrated by the analytical examples. the method is employed to assess the sensitivity of a chemical reaction model for dimethylsulphide (dms), a gas involved in climate change. results of the sensitivity analysis open up the ground for model reconsideration: some model components may need a more thorough modelling effort while some others may need to be simplified. {\textcopyright} 2006 elsevier ltd. all rights reserved.},
author = {campolongo, francesca and cariboni, jessica and saltelli, andrea},
doi = {10.1016/j.envsoft.2006.10.004},
file = {:users/roy/documents/knowledge/bibliographie/2007/campolongo, cariboni, saltelli{\_}an effective screening design for sensitivity analysis of large models.pdf:pdf},
isbn = {13648152 (issn)},
issn = {13648152},
journal = {environmental modelling {\&} software},
keywords = {dimethylsulphide (dms),effective sampling strategy,model-free methods,screening problem,sensitivity analysis},
month = {oct},
number = {10},
pages = {1509--1518},
title = {{an effective screening design for sensitivity analysis of large models}},
volume = {22},
year = {2007}
}

@article{un2011,
author = {sun, ying and genton, marc g.},
doi = {10.1198/jcgs.2011.09224},
file = {:users/roy/documents/knowledge/bibliographie/2011/sun, genton{\_}functional boxplots.pdf:pdf},
journal = {journal of computational and graphical statistics},
keywords = {depth,functional data,growth data,precipitation data,space-time data,visualization,data,depth,functional data,growth data,precipitation data,space,time,visualization},
number = {2},
pages = {316--334},
title = {{functional boxplots}},
volume = {20},
year = {2011}
}
@inproceedings{touati2016,
address = {r{\'{e}}union island},
author = {touati, taieb},
booktitle = {8th international conference on sensitivity analysis of model output,},
file = {:users/roy/documents/knowledge/bibliographie/2016/touati{\_}confidence intervals for sobol' indices.pdf:pdf},
title = {{confidence intervals for sobol' indices}},
year = {2016}
}
@article{raissi2016,
abstract = {we develop a novel multi-fidelity framework that goes far beyond the classical ar(1) co-kriging scheme of kennedy and o'hagan (2000). our method can handle general discontinuous cross-correlations among systems with different levels of fidelity. a combination of multi-fidelity gaussian processes (ar(1) co-kriging) and deep neural networks enables us to construct a method that is immune to discontinuities. we demonstrate the effectiveness of the new technology using standard benchmark problems designed to resemble the outputs of complicated high- and low-fidelity codes.},
archiveprefix = {arxiv},
arxivid = {1604.07484},
author = {Raissi, Maziar and Karniadakis, George},
eprint = {1604.07484},
file = {:users/roy/documents/knowledge/bibliographie/2016/raissi, karniadakis{\_}deep multi-fidelity gaussian processes.pdf:pdf},
keywords = {co-kriging,deep nets,discontinuous correlations,machine learning,manifold},
month = {apr},
number = {1},
pages = {1--14},
title = {{Deep multi-fidelity gaussian processes}},
year = {2016}
}
@inproceedings{marmin2017,
author = {marmin, s},
booktitle = {mascot-num 2017},
file = {:users/roy/documents/knowledge/bibliographie/2017/marmin{\_}non-stationary gaussian process modelling and sequential design of experiments for exploration of high variation regions.pdf:pdf},
title = {{non-stationary gaussian process modelling and sequential design of experiments for exploration of high variation regions}},
year = {2017}
}


@article{Alexander2014,
author = {Alexander, Robert L and O'Modhrain, Sile and Roberts, D Aaron and Gilbert, Jason A and Zurbuchen, Thomas H},
doi = {10.1002/2014JA020025},
journal = {Journal of Geophysical Research: Space Physics},
month = {jul},
number = {7},
pages = {5259--5271},
title = {{The bird's ear view of space physics: Audification as a tool for the spectral analysis of time series data}},
volume = {119},
year = {2014}
}

@article{AnindyaChatterjee2000,
author = {{Anindya Chatterjee}},
journal = {Current Science},
number = {7},
title = {{An introduction to the proper orthogonal decomposition}},
volume = {78},
year = {2000}
}

@article{Anscombe1973,
author = {Anscombe, F J},
doi = {10.2307/2682899},
journal = {The American Statistician},
number = {1},
pages = {17--21},
title = {{Graphs in Statistical Analysis}},
volume = {27},
year = {1973}
}

@book{AshBocquetNodet2017,
  TITLE = {{Data assimilation: methods, algorithms, and applications}},
  AUTHOR = {Asch, Mark and Bocquet, Marc and Nodet, Ma{\"e}lle},
  PUBLISHER = {{SIAM}},
  SERIES = {Fundamentals of Algorithms},
  PAGES = {xviii + 306},
  YEAR = {2016},
  KEYWORDS = { ensemble methods ;  adjoint methods ; data assimilation ;  inverse problems ;  Kalman filters},
  HAL_ID = {hal-01402885},
  HAL_VERSION = {v1},
}

@article{Belia2005,
author = {Belia, Sarah and Fidler, Fiona and Williams, Jennifer and Cumming, Geoff},
doi = {10.1037/1082-989X.10.4.389},
journal = {Psychological Methods},
number = {4},
pages = {389--396},
pmid = {16392994},
title = {{Researchers Misunderstand Confidence Intervals and Standard Error Bars}},
volume = {10},
year = {2005}
}

@article{Bliznyuk2008,
author = {Bliznyuk, Nikolay and Ruppert, David and Shoemaker, Christine and Rommel, Regis and Wild, Stefan and Mugunthan, Pradeep},
doi = {10.1198/106186008X320681},
journal = {Journal of Computational and Graphical Statistics},
number = {2},
pages = {270--294},
title = {{Bayesian Calibration of Computationally Expensive Models Using Optimization and Radial Basis Function Approximation}},
volume = {17},
year = {2008}
}

@article{Bonneau2014,
author = {Bonneau, Georges Pierre and Hege, Hans Christian and Johnson, Chris R. and Oliveira, Manuel M. and Potter, Kristin and Rheingans, Penny and Schultz, Thomas},
doi = {10.1007/978-1-4471-6497-5_1},
journal = {Mathematics and Visualization},
pages = {3--27},
title = {{Overview and state-of-the-art of uncertainty visualization}},
volume = {37},
year = {2014}
}

@article{Brodlie2012,
author = {Brodlie, Ken and {Allendes Osorio}, Rodolfo and Lopes, Adriano},
doi = {10.1007/978-1-4471-2804-5_6},
journal = {Expanding the Frontiers of Visual Analytics and Visualization},
pages = {81--109},
title = {{A Review of Uncertainty in Data Visualization}},
year = {2012}
}

@article{Claessen2011,
author = {Claessen, Jarry H.T. and {Van Wijk}, Jarke J.},
doi = {10.1109/TVCG.2011.201},
journal = {IEEE Transactions on Visualization and Computer Graphics},
number = {12},
pages = {2310--2316},
title = {{Flexible linked axes for multivariate data visualization}},
volume = {17},
year = {2011}
}

@article{Ehlschlaeger1997,
author = {Ehlschlaeger, Charles R and Shortridge, Ashton M and Goodchild, Michael F},
journal = {Computers {\&} Geosciences},
number = {4},
pages = {387--395},
title = {{Visualizing spatial data uncertainty animation using}},
volume = {23},
year = {1997}
}

@article{Gigerenzer1995,
author = {Gigerenzer, Gerd and Hoffrage, Ulrich},
doi = {10.1037/0033-295X.102.4.684},
journal = {Psychological Review},
number = {4},
pages = {684--704},
title = {{How to Improve Bayesian Reasoning Without Instruction : Frequency Formats}},
volume = {102},
year = {1995}
}

@article{Hackstadt1994,
author = {Hackstadt, S T and Malony, A D and Mohr, B},
doi = {10.1109/SHPCC.1994.296663},
journal = {Scalable High-Performance Computing Conf.},
keywords = {computer graphics. parallel languages. parallel pr},
pages = {342--349},
title = {{Scalable performance visualization for data-parallel programs}},
year = {1994}
}

@book{HandbookUQ,
editor = {Ghanem, Roger and Higdon, David and Owhadi, Houman},
booktitle = {Handbook of Uncertainty Quantification},
title = {Handbook of Uncertainty Quantification},
isbn = {978-2-7598-1269-1},
year = {2017}
}

@article{Hullman2015,
author = {Hullman, Jessica and Resnick, Paul and Adar, Eytan},
doi = {10.1371/journal.pone.0142444},
pages = {1--25},
journal = {PloS ONE},
title = {Hypothetical Outcome Plots Outperform Error Bars and Violin Plots for Inferences about Reliability of Variable Ordering},
year = {2015}
}

@article{Hughes2003,
author = {Hughes, Scott A.},
doi = {10.1016/S0003-4916(02)00025-8},
journal = {Annals of Physics},
number = {1},
pages = {142--178},
title = {{Listening to the universe with gravitational-wave astronomy}},
volume = {303},
year = {2003}
}

@article{Hyndman2009,
author = {Hyndman, Rob J and Shang, Han Lin},
journal = {Journal of Computational and Graphical Statistics},
pages = {29--45},
title = {{Rainbow plots , bagplots and boxplots for functional data}},
volume = {19},
year = {2009}
}

@article{Inselberg1985,
author = {Inselberg, Alfred},
booktitle = {The Visual Computer},
doi = {10.1007/bf01898350},
journal = {The Visual Computer},
month = aug,
number = 2,
pages = {69--91},
title = {{The Plane with Parallel Coordinates}},
volume = 1,
year = 1985
}

@book{Inselberg2009,
author = {Inselberg, Alfred},
doi = {10.1007/978-0-387-68628-8},
publisher = {Springer New York},
title = {{Parallel Coordinates}},
year = {2009}
}

@article{Kale2018,
author = {Kale, Alex and Nguyen, Francis and Kay, Matthew and Hullman, Jessica},
journal = {Transactions on Visualization and Computer Graphics},
title = {{Hypothetical Outcome Plots Help Untrained Observers Judge Trends in Ambiguous Data}},
year = {2018}
}

@book{Montomoli2015,
author = {Montomoli, Francesco and Carnevale, Mauro and D'Ammaro, Antonio and Massini, Michela and Salvadori, Simone},
booktitle = {Springer International Publishing},
doi = {10.1007/978-3-319-00885-1},
isbn = {9783319146805},
title = {{Uncertainty Quantification in Computational Fluid Dynamics and Aircraft Engines}},
volume = {92},
year = {2015}
}

@inproceedings{Moreland2016,
author = {Moreland, Kenneth},
booktitle = {Proceedings of Human Vision and Electronic Imaging (HVEI)},
doi = {10.2352/ISSN.2470-1173.2016.16.HVEI-133},
title = {{Why We Use Bad Color Maps and What You Can Do About It}},
year = {2016}
}

@inproceedings{Popelin2013,
archivePrefix = {arXiv},
arxivId = {1402.7141},
author = {Popelin, Anne-laure and Iooss, Bertrand},
booktitle = {SNA + MC 2013 - Joint International Conference on Supercomputing in Nuclear Applications + Monte Carlo},
doi = {10.1051/snamc/201403403},
isbn = {978-2-7598-1269-1},
month = {jun},
pages = {03403},
title = {{Visualization tools for uncertainty and sensitivity analyses on thermal-hydraulic transients}},
year = {2013}
}

@article{Potter2012a,
author = {Potter, Kristin and Rosen, Paul and Johnson, Chris R.},
doi = {10.1007/978-3-642-32677-6_15},
journal = {IFIP Advances in Information and Communication Technology},
pages = {226--247},
title = {{From quantification to visualization: A taxonomy of uncertainty visualization approaches}},
volume = {377 AICT},
year = {2012}
}

@article{Raidou2016,
author = {Raidou, Renata Georgia and Eisemann, Martin and Breeuwer, Marcel and Eisemann, Elmar and Vilanova, Anna},
doi = {10.1109/TVCG.2015.2467872},
journal = {IEEE Transactions on Visualization and Computer Graphics},
number = {1},
pages = {589--598},
title = {{Orientation-Enhanced Parallel Coordinate Plots}},
volume = {22},
year = {2016}
}

@article{Ren2017,
author = {Ren, B. and Bacallado, S. and Favaro, S. and Holmes, S. and Trippa, L.},
doi = {10.1080/01621459.2017.1288631},
journal = {Journal of the American Statistical Association},
number = {520},
pages = {1430--1442},
title = {{Bayesian non Parametric Ordination for the Analysis of Microbial Communities}},
volume = {112},
year = {2017}
}

@article{Ribes2015,
author = {Rib{\'{e}}s, Alejandro and Pouderoux, Joachim and Popelin, Anne-laure and Iooss, Bertrand},
title = {{Understanding Ensembles by Joint Interactive Statistical Analysis of Curve Datasets and their Principal Component Analysis}},
year = {2015}
}

@article{Sacks1989,
author = {Sacks, Jerome and Welch, Williams J. and Mitchell, Toby J. and Wynn, Henry P.},
doi = {10.1214/ss/1177012413},
journal = {Statistical Science},
number = {4},
pages = {409--423},
title = {{Design and Analysis of Computer Experiments}},
volume = {4},
year = {1989}
}

@article{Scholkopf1999,
author = {Sch{\"{o}}lkopf, Bernhard and Platt, John C. and Shawe-Taylor, John and Smola, Alex J and Williamson, Robert C},
doi = {10.1162/089976601750264965},
journal = {Neural Computation},
month = {jul},
number = {7},
pages = {1443--1471},
title = {{Estimating the Support of a High-Dimensional Distribution}},
volume = {13},
year = {2001}
}

@incollection{Scott2015,
author = {Scott, David W},
booktitle = {Multivariate Density Estimation},
chapter = {7},
doi = {10.1002/9781118575574},
publisher = {John Wiley {\&} Sons, Inc},
series = {Wiley Series in Probability and Statistics},
title = {{Multivariate Density Estimation}},
year = {2015}
}

@book{Sullivan2015,
author = {Sullivan, T.J.},
booktitle = {Introduction to Uncertainty Quantification},
title = {Introduction to Uncertainty Quantification},
doi = {10.1007/978-3-319-23395-6},
series = {Texts in Applied Mathematics},
volume = {63},
year = {2015}
}

@article{Sun2011,
author = {Sun, Ying and Genton, Marc G.},
doi = {10.1198/jcgs.2011.09224},
journal = {Journal of Computational and Graphical Statistics},
number = {2},
pages = {316--334},
title = {{Functional Boxplots}},
volume = {20},
year = {2011}
}

@article{VanSomeren2016,
author = {{Van Someren}, Eus J. W. and Dekker, Kim and {Te Lindert}, Bart H. W. and Benjamins, Jeroen S. and Moens, Sarah and Migliorati, Filippo and Aarts, Emmeke and van der Sluis, Sophie},
doi = {10.1080/23328940.2015.1130519},
journal = {Temperature},
number = {1},
pages = {59--76},
title = {{The experienced temperature sensitivity and regulation survey}},
volume = {3},
year = {2016}
}

@book{Wand1995,
address = {Boston, MA},
author = {Wand, M. P. and Jones, M. C.},
doi = {10.1007/978-1-4899-4493-1},
title = {{Kernel Smoothing}},
year = {1995}
}

@article{Whitaker2013,
author = {Whitaker, Ross T and Mirzargar, Mahsa and Kirby, Robert M},
doi = {10.1109/TVCG.2013.143},
journal = {IEEE Transactions on Visualization and Computer Graphics},
number = {12},
pages = {2713--2722},
title = {{Contour boxplots: A method for characterizing uncertainty in feature sets from simulation ensembles}},
volume = {19},
year = {2013}
}


@article{Androulakis2016,
author = {Androulakis, E. and Drosou, K. and Koukouvinos, C. and Zhou, Y. D.},
doi = {10.1080/03610926.2014.966843},
journal = {Communications in Statistics - Theory and Methods},
number = {13},
pages = {3782--3806},
title = {{Measures of uniformity in experimental designs: A selective overview}},
volume = {45},
year = {2016}
}

@article{Balachandar2010-A,
   author = {Balachandar, S. and Eaton, J. K.},
   title = {Turbulent dispersed multiphase flow},
   journal = {Annual Review Fluid Mechanics},
   volume = {42},
   number = {},
   pages = {111--133},
   year = {2010}
}

@article{Baudin2015,
arxivId = {1501.05242},
author = {Baudin, Micha{\"{e}}l and Dutfoy, Anne and Iooss, Bertrand and Popelin, Anne-Laure},
month = {jun},
title = {{OpenTURNS: An industrial software for uncertainty quantification in simulation}},
year = {2015}
}

@article{Beran2017-A,
   author = {Beran, P. and Stanford, B. and Schrock, C.},
   title = {Uncertainty quantification in aeroelasticity},
   journal = {Annual Review Fluid Mechanics},
   volume = {49},
   number = {},
   pages = {361--386},
   year = {2017}
}

@incollection{Cavazzuti2013,
author = {Cavazzuti, Marco},
booktitle = {Optimization Methods: From Theory to Design},
doi = {10.1007/978-3-642-31187-1_2},
pages = {13--42},
publisher = {Springer Berlin Heidelberg},
title = {{Design of Experiments}},
year = {2013}
}

@article{Caporaloni1975-A,
   author = {Caporaloni, M. and Tampieri, F. and Trombetti, F. and Vittori, O.},
   title = {Transfer of particles in nonisotropic air turbulence},
   journal = {Journal of the Atmospheric Sciences},
   volume = {32},
   number = {},
   pages = {565--568},
   year = {1975}
}

@article{Cha2007,
author = {Cha, Sung-hyuk},
doi = {10.1007/s00167-009-0884-z},
journal = {International Journal of Mathematical Models and Methods in Applied Sciences},
keywords = {distance,histogram,probability density function},
number = {4},
pages = {300--307},
title = {{Comprehensive Survey on Distance / Similarity Measures between Probability Density Functions}},
volume = {1},
year = {2007}
}

@article{Chernatynskiy2013-A,
   author = {Chernatynskiy, A. and Phillpot, S. R. and LeSar, R.},
   title = {Uncertainty quantification in multiscale simulation of materials: a prospective},
   journal = {Annual Review Fluid Mechanics},
   volume = {43},
   number = {},
   pages = {157--182},
   year = {2013}
}

@article{Crombecq2011,
author = {Crombecq, K. and Laermans, E. and Dhaene, T.},
doi = {10.1016/j.ejor.2011.05.032},
journal = {European Journal of Operational Research},
number = {3},
pages = {683--696},
title = {{Efficient space-filling and non-collapsing sequential design strategies for simulation-based modeling}},
volume = {214},
year = {2011}
}

@article{Damblin2013,
author = {Damblin, Guillaume and Couplet, Mathieu and Iooss, Bertrand and Damblin, Guillaume and Couplet, Mathieu and Iooss, Bertrand},
journal = {Journal of Simulation},
pages = {276--289},
title = {{Numerical studies of space filling designs : optimization of Latin Hypercube Samples and subprojection properties}},
year = {2013}
}

@article{Esmaily2018-A,
   author = {Esmaily, M. and Jofre, L. and Mani, A. and Iaccarino, G.},
   title = {A scalable geometric multigrid solver for nonsymmetric elliptic systems with application to variable-density flows},
   journal = {Journal of Computational Physics},
   volume = {357},
   number = {},
   pages = {142--158},
   year = {2018}
}

@book{Fang2006,
author = {Fang, Kai-Tai and Li, R Z and Sudjianto, A},
editor = {Lafferty, John and Madigan, David and Murtagh, Fionn and Smyth, Padhraic},
isbn = {978-1-58488-546-7},
pages = {290},
publisher = {Chapman {\&} Hall/CRC},
title = {{Design and modeling for computer experiments}},
year = {2006}
}

@article{Franco2009,
author = {Franco, J. and Vasseur, O. and Corre, B. and Sergent, M.},
doi = {10.1016/j.chemolab.2009.03.011},
journal = {Chemometrics and Intelligent Laboratory Systems},
number = {2},
pages = {164--169},
title = {{Minimum Spanning Tree: A new approach to assess the quality of the design of computer experiments}},
volume = {97},
year = {2009}
}

@article{Frankel2016-A,
   author = {Frankel, A. and Pouransari, H. and Coletti, F. and Mani, A.},
   title = {Settling of heated particles in homogeneous turbulence},
   journal = {Journal of Fluid Mechanics},
   volume = {792},
   number = {},
   pages = {869--893},
   year = {2016}
}

@article{Frankel2017-A,
   author = {Frankel, A. and Iaccarino, G.},
   title = {Efficient control variates for uncertainty quantification of radiation transport},
   journal = {Journal of Quantitative Spectroscopy \& Radiative Transfer},
   volume = {189},
   number = {},
   pages = {398--406},
   year = {2017}
}

@article{Garud2017,
author = {Garud, Sushant S and Karimi, Iftekhar A and Kraft, Markus},
doi = {10.1016/j.compchemeng.2017.05.010},
file = {:Users/roy/Documents/Knowledge/Bibliographie/2017/Garud et al.{\_}Design of Computer Experiments A Review.pdf:pdf},
issn = {00981354},
journal = {Computers {\&} Chemical Engineering},
month = {nov},
number = {182},
pages = {71--95},
title = {{Design of computer experiments: A review}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0098135417302090},
volume = {106},
year = {2017}
}

@article{Hastings1979,
author = {Hastings, W K},
doi = {10.2307/2334940},
journal = {Biometrika},
month = {apr},
number = {1},
pages = {97},
title = {{Monte Carlo Sampling Methods Using Markov Chains and Their Applications}},
volume = {57},
year = {1970}
}

@article{Ho2014-A,
   author = {Ho, C. K. and Iverson, B. D.},
   title = {Review of high-temperature central receiver designs
for concentrating solar power},
   journal = {Renewable and Sustainable Energy Reviews},
   volume = {29},
   number = {},
   pages = {835--846},
   year = {2014}
}

@article{Ho2017-A,
   author = {Ho, C. K.},
   title = {Advances in central receivers for concentrating solar applications},
   journal = {Solar Energy},
   volume = {152},
   number = {},
   pages = {38--56},
   year = {2017}
}

@article{Hoffman2011,
arxivId = {1111.4246},
author = {Hoffman, Matthew D. and Gelman, Andrew},
issn = {15337928},
number = {2008},
pages = {1--30},
title = {{The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo}},
year = {2011}
}

@article{Jofre2017-A,
   author = {Jofre, L. and Geraci, G. and Fairbanks, H. R. and Doostan, A. and Iaccarino, G.},
   title = {Multi-fidelity uncertainty quantification of irradiated particle-laden turbulence},
   journal = {Annual Research Briefs, Center for Turbulence Research, Stanford University},
   pages = {21--34},
   year = {2017}
}

@article{Jofre2018-A,
   author = {Jofre, L. and Domino, S. P. and Iaccarino, G.},
   title = {A framework for characterizing structural uncertainty in large-eddy simulation closures},
   journal = {Flow Turbulence and Combustion},
   volume = {100},
   number = {2},
   pages = {341--363},
   year = {2018}
}

@article{Joseph2015,
author = {Joseph, V. Roshan and Gul, Evren and Ba, Shan},
doi = {10.1093/biomet/asv002},
journal = {Biometrika},
number = {2},
pages = {371--380},
title = {{Maximum projection designs for computer experiments}},
volume = {102},
year = {2015}
}

@article{Kucherenko2015,
author = {Kucherenko, Sergei and Albrecht, Daniel and Saltelli, Andrea},
doi = {10.1016/j.ress.2017.04.003},
journal = {The 8th IMACS Seminar on Monte Carlo Methods},
keywords = {high dimensional integration,latin hypercube sampling,monte carlo,quasi monte carlo,sequences,sobol},
pages = {1--32},
title = {{Exploring multi-dimensional spaces: a Comparison of Latin Hypercube and Quasi Monte Carlo Sampling Techniques}},
year = {2015}
}

@article{Lekivetz2015,
author = {Lekivetz, Ryan and Jones, Bradley},
doi = {10.1002/qre.1640},
journal = {Quality and Reliability Engineering International},
month = {jul},
number = {5},
pages = {829--837},
title = {{Fast Flexible Space-Filling Designs for Nonrectangular Regions}},
volume = {31},
year = {2015}
}

@article{Liu2018,
author = {Liu, Haitao and Ong, Yew Soon and Cai, Jianfei},
doi = {10.1007/s00158-017-1739-8},
journal = {Structural and Multidisciplinary Optimization},
number = {1},
pages = {393--416},
title = {{A survey of adaptive sampling for global metamodeling in support of simulation-based complex engineering design}},
volume = {57},
year = {2018}
}

@article{Mckay1979,
author = {Mckay, M. D. and Beckman, R .J. and Conover, W. J.},
journal = {Technometrics},number = {2},
pages = {239--245},
title = {{A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code}},
volume = {21},
year = {1979}
}
@online{Mira-O,
   author = {Mira-ALCF},
   title = {{M}ira, {A}rgonne {L}eadership {C}omputing {F}acility},
   %year = {2018},
   url = {https://www.alcf.anl.gov/mira}
}
@article{Najm2009-A,
   author = {Najm, H. N.},
   title = {Uncertainty quantification and polynomial chaos techniques in computational fluid dynamics},
   journal = {Annual Review Fluid Mechanics},
   volume = {41},
   number = {},
   pages = {35--52},
   year = {2009}
}

@article{Owen1998,
author = {Owen, Art B.},
doi = {10.1006/jcom.1998.0487},
journal = {Journal of Complexity},
number = {4},
pages = {466--489},
title = {{Scrambling Sobol' and Niederreiter-Xing points}},
volume = {14},
year = {1998}
}

@article{Pouransari2017-A,
   author = {Pouransari, H. and Mani, A.},
   title = {Effects of preferential concentration on heat transfer in particle-based solar receivers},
   journal = {Journal of Solar Energy Engineering},
   volume = {139},
   number = {},
   pages = {021008},
   year = {2017}
}

@article{Pronzato2017,
author = {Pronzato, Luc},
journal = {Journal de la Soci{\'{e}}t{\'{e}} Fran{\c{c}}aise de Statistique},
number = {1},
pages = {7--36},
title = {{Minimax and maximin space-filling designs : some properties and methods for construction}},
volume = {158},
year = {2017}
}

@article{Rahmani2018-A,
   author = {Rahmani, M. and Geraci, G. and Iaccarino, G. and Mani, A.},
   title = {Effects of particle polydispersity on radiative heat transfer in particle-laden turbulent flows},
   journal = {International Journal of Multiphase Flow},
   volume = {104},
   pages = {42--59},
   year = {2018}
}

@article{Roache1997-A,
   author = {Roache, P. J.},
   title = {Quantification of uncertainty in computational fluid dynamics},
   journal = {Annual Review Fluid Mechanics},
   volume = {29},
   number = {},
   pages = {123--160},
   year = {1997}
}

@article{Sacks1989,
author = {Sacks, Jerome and Welch, Williams J. and Mitchell, Toby J. and Wynn, Henry P.},
doi = {10.1214/ss/1177012413},
journal = {Statistical Science},
month = {nov},
number = {4},
pages = {409--423},
title = {{Design and Analysis of Computer Experiments}},
volume = {4},
year = {1989}
}

@book{Saltelli2007,
author = {Saltelli, Andrea and Ratto, Marco and Andres, Terry and Campolongo, Francesca and Cariboni, Jessica and Gatelli, Debora and Saisana, Michaela and Tarantola, Stefano},
booktitle = {Global Sensitivity Analysis. The Primer},
doi = {10.1002/9780470725184},
isbn = {9780470725184},
month = {dec},
pages = {237--275},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Global Sensitivity Analysis. The Primer}},
year = {2007}
}

@article{Saltelli2010,
author = {Saltelli, Andrea and Annoni, Paola and Azzini, Ivano and Campolongo, Francesca and Ratto, Marco and Tarantola, Stefano},
doi = {10.1016/j.cpc.2009.09.018},
journal = {Computer Physics Communications},
number = {2},
pages = {259--270},
title = {{Variance based sensitivity analysis of model output. Design and estimator for the total sensitivity index}},
volume = {181},
year = {2010}
}

@article{Saltelli2019,
author = {Saltelli, Andrea and Aleksankina, Ksenia and Becker, William and Fennell, Pamela and Ferretti, Federico and Holst, Niels and Li, Sushan and Wu, Qiongli},
doi = {10.1016/j.envsoft.2019.01.012},
file = {:Users/roy/Documents/Knowledge/Bibliographie/2019/Saltelli et al.{\_}Why so many published sensitivity analyses are false A systematic review of sensitivity analysis practices.pdf:pdf},
issn = {1364-8152},
journal = {Environmental Modelling and Software},
number = {January},
pages = {29--39},
publisher = {Elsevier},
title = {{Why so many published sensitivity analyses are false : A systematic review of sensitivity analysis practices}},
url = {https://doi.org/10.1016/j.envsoft.2019.01.012},
volume = {114},
year = {2019}
}

@article{Sheikholeslami2017,
author = {Sheikholeslami, Razi and Razavi, Saman},
doi = {10.1016/j.envsoft.2017.03.010},
journal = {Environmental Modelling {\&} Software},
pages = {109--126},
title = {{Progressive Latin Hypercube Sampling: An efficient approach for robust sampling-based analysis of environmental models}},
volume = {93},
year = {2017}
}

@article{Sobol1967,
author = {Sobol', I. M.},
doi = {10.1016/0041-5553(67)90144-9},
journal = {USSR Computational Mathematics and Mathematical Physics},
number = {4},
pages = {86--112},
pmid = {4925774259},
title = {{On the distribution of points in a cube and the approximate evaluation of integrals}},
volume = {7},
year = {1967}
}
@article{Subramaniam2013-A,
   author = {Subramaniam, S.},
   title = {{L}agrangian-{E}ulerian methods for multiphase flows},
   journal = {Progress in Energy and Combustion Science},
   volume = {39},
   number = {},
   pages = {215--245},
   year = {2013}
}

@book{Wand1995,
address = {Boston, MA},
author = {Wand, M. P. and Jones, M. C.},
doi = {10.1007/978-1-4899-4493-1},
isbn = {978-0-412-55270-0},
publisher = {Springer US},
title = {{Kernel Smoothing}},
year = {1995}
}
@article{Zamansky2014-A,
   author = {Zamansky, R. and Coletti, F. and Massot, M. and Mani, A.},
   title = {Radiation induces turbulence in particle-laden fluids},
   journal = {Physics of Fluids},
   volume = {26},
   number = {},
   pages = {071701},
   year = {2014}
}


@PHDTHESIS{Segui2017b,
author = {Segui-Troth, L. },
title = {Multi-physics coupled simulations of gas turbines},
year = {2017},
type = {PhD Thesis},
month = {11},
school = {Universite de Toulouse, INP Toulouse - Ecole doctorale MEGeP},
}

@article{Nicoud1999,
    Annote = {OC},
    Author = {F. Nicoud and F. Ducros},
    Date-Added = {2008-11-21 14:49:30 +0100},
    Date-Modified = {2013-09-30 09:23:41 +0000},
    Journal = {Flow Turb. and Combustion},
    Keywords = {LES},
    Number = {3},
    Pages = {183-200},
    Title = {Subgrid-scale stress modelling based on the square of the velocity gradient},
    Volume = {62},
    Year = {1999}}

@inproceedings{Segui2017a,
	author = {Segui, L.M. and Gicquel, L.Y.M. and Duchaine, F. and de Laborderie, J.},
	Title = {LES of the LS89 cascade: influence of inflow turbulence on the flow predictions},
	Year = {2017},
	series = {European Turbomachinery Conference}
}


@article{Emmons1951,
	author = {Emmons, H W},
	journal = {Journal of the Aeronautical Sciences},
	number = {7},
	title = {{The laminar-turbulent transition in a boundary layer - {\{}P{\}}art I}},
	volume = {18},
	year = {1951}
}

@article{Gicquel2011,
    Author = {L.Y.M. Gicquel and N. Gourdain and J.-F. Boussuge and H. Deniau and G. Staffelbach and P. Wolf and T. Poinsot},
    Journal = {Comptes Rendus M\'ecanique},
    Keywords = {AVBP, elsA, High Performance Computing},
    Number = {2-3},
    Pages = {104 - 124},
    Title = {High performance parallel computing of flows in complex geometries},
    Volume = {339},
    Year = {2011}}

@inproceedings{Quartapelle1993,
    Author = {L. Quartapelle and V. Selmin},
    series = {8th International conference, Finite elements in fluids: new trends and applications},
    Title = {High-order {T}aylor-{G}alerkin methods for non-linear multidimensional problems.},
    Year = {1993}}

@inproceedings{Wheeler2015,
    Address = {Montreal, Canada},
    Author = {Wheeler, A.P.S. and Sandberg, R.D. and Sandham, N.D. and Pichler, R. and Michelassi, V. and Laskowski, G.},
    series = {ASME Turbo Expo 2015},
    Pages = {GT2015-43133},
    Title = {Direct numerical Simulations of a High Pressure Turbine Vane},
    Year = {2015}}

@inproceedings{Gourdain2010,
    Address = {Glasgow, UK},
    Author = {N. Gourdain and F. Duchaine and E. Collado and L. Gicquel},
    series = {ASME Turbo expo},
    Date-Added = {2011-12-27 16:21:55 +0100},
    Date-Modified = {2013-09-30 09:23:44 +0000},
    Editor = {GT2010-22793},
    Keywords = {Turbomachinery, Heat transfer},
    Month = {June},
    Title = {Advanced Numerical Simulation Dedicated to the Prediction of Heat Transfer in a Highly Loaded Turbine Guide Vane.},
    Year = {2010}}

@phdthesis{Fontaneto2014,
    Author = {F. Fontaneto},
    School = {University of Bergamo},
    Type = {{PhD thesis}},
    Title = {Aero-thermal performance of a film-cooled high pressure turbine blade/vane: a test case for numerical codes validation},
    Year = {2014}}

@inproceedings{Pichler2016,
    Address = {Seoul, South Korea},
    Author = {Pichler, R. and Kopriva, J. and Laskowski, G. and Michelassi, V. and Sandberg, R.},
    series = {ASME Turbo Expo 2016},
    Publisher = {GT2016-57189},
    Title = {Highly resolved LES of a Linear HPT Vane Cascade using structured and unstructured codes},
    Year = {2016}}

@techreport{arts1990,
author = {Arts, Tony and {Lambert de Rouvroit}, M and Rutherford, A W},
institution = {von Karman Institute for Fluid Dynamics},
keywords = {Aerodynamics,Euler/Navier Stokes codes,Heat transfer,Transonic turbines},
number = {174},
title = {{Aero-Thermal Investigation of a Highly Loaded Transonic Linear Turbine Guide Vane Cascade}},
year = {1990}
}

@article{baudin2015,
archivePrefix = {arXiv},
arxivId = {1501.05242},
author = {Baudin, Micha{\"{e}}l and Dutfoy, Anne and Iooss, Bertrand and Popelin, Anne-Laure},
keywords = {C++ library,OpenTURNS,estima-,genericity,multivariate distri- bution,open source,probability,propagation,python module,quantification,random vectors,sensitivity,simulation,statistics,tion,transparency,uncertainty},
title = {{OpenTURNS: An industrial software for uncertainty quantification in simulation}},
year = {2015}
}

@inproceedings{baudin2016,
address = {R{\'{e}}union Island},
author = {Baudin, Micha{\"{e}}l and Boumhaout, Khalid and Delage, Thibault and Iooss, Bertrand and Martinez, Jean-Marc},
series = {8th International Conference on Sensitivity Analysis of Model Output,},
title = {{Numerical stability of Sobol' indices estimation formula}},
year = {2016}
}

@article{braconnier2011,
author = {Braconnier, T. and Ferrier, M. and Jouhaud, J.-C. and Montagnac, M. and Sagaut, P.},
doi = {10.1016/j.compfluid.2010.09.002},
journal = {Computers {\&} Fluids},
keywords = {proper orthogonal decomposition,singular value decomposition},
number = {1},
pages = {195--209},
title = {{Towards an adaptive POD/SVD surrogate model for aeronautic design}},
volume = {40},
year = {2011}
}

@incollection{cavazzuti2013,
author="Cavazzuti, Marco",
title="Design of Experiments",
bookTitle="Optimization Methods: From Theory to  Design Scientific and Technological Aspects in Mechanics",
year="2013",
publisher="Springer Berlin Heidelberg",
pages="13--42",
isbn="978-3-642-31187-1",
doi="10.1007/978-3-642-31187-1_2",
}

@article{chatterjee2000,
author = {{Anindya Chatterjee}},
journal = {Current Science},
number = {7},
title = {{An introduction to the proper orthogonal decomposition}},
volume = {78},
year = {2000}
}

@book{clarke1992,
author = {Clarke, Bertrand and Fokoue, Ernest and Zhang, Hao Helen},
doi = {10.1007/978-1-4612-4380-9},
editor = {Kotz, Samuel and Johnson, Norman L.},
isbn = {978-0-387-94039-7},
pages = {1--694},
publisher = {Springer New York},
series = {Springer Series in Statistics},
title = {{Breakthroughs in Statistics}},
year = {1992}
}

@article{damblin2013,
author = {Damblin, Guillaume and Couplet, Mathieu and Iooss, Bertrand and Damblin, Guillaume and Couplet, Mathieu and Iooss, Bertrand},
journal = {Journal of Simulation},
keywords = {Latin Hypercube Sampling,computer experiment,discrepancy,optimal design},
pages = {276--289},
title = {{Numerical studies of space filling designs : optimization of Latin Hypercube Samples and subprojection properties}},
year = {2013}
}

@article{degennaro2015,
author = {{DeGennaro}, Anthony M. and Rowley, Clarence W. and Martinelli, Luigi},
doi = {10.2514/1.C032698},
journal = {Journal of Aircraft},
month = {sep},
number = {5},
pages = {1404--1411},
title = {{Uncertainty Quantification for Airfoil Icing Using Polynomial Chaos Expansions}},
volume = {52},
year = {2015}
}

@article{dette2010,
author = {Dette, Holger and Pepelyshev, Andrey},
doi = {10.1198/TECH.2010.09157},
journal = {Technometrics},
keywords = {arc-sine distribution,logarithmic potential,space-filling designs,uniform designs},
number = {4},
pages = {421--429},
title = {{Generalized Latin Hypercube Design for Computer Experiments}},
volume = {52},
year = {2010}
}

@article{draper1995,
author = {Draper, David},
isbn = {00359246},
journal = {Journal of the Royal Statistical Society B},
keywords = {BAYES FACTORS,CALIBRATION,FORECASTINO,HIERARCHICAL MODELS,INFERENCE,MODEL SPECIFICATION,OVERFITTING,PREDICTION,ROBUSTNESS,SENSITIVITY ANALYSIS,UNCERTAINTY ASSESSMENT},
number = {1},
pages = {45--97},
title = {{Assessment and Propagation of Model Uncertainty}},
volume = {57},
year = {1995}
}

@article{duchaine2009,
author = {Duchaine, F and Morel, T and {M. Gicquel}, L. Y.},
doi = {10.2514/1.37808},
journal = {AIAA Journal},
month = {mar},
number = {3},
pages = {631--645},
title = {{Computational-Fluid-Dynamics-Based Kriging Optimization Tool for Aeronautical Combustion Chambers}},
volume = {47},
year = {2009}
}

@inproceedings{emory2016,
author = {Emory, Michael and Iaccarino, Gianluca and Ma, Lynn},
series = {Turbomachinery Technical Conference and Exposition},
pages = {1--10},
title = {{Uncertainty Quantification in Turbomachinery Simulations}},
year = {2016}
}

@incollection{fang2006,
author = {Fang, Kai-Tai and Li, R Z and Sudjianto, A},
editor = {Lafferty, John and Madigan, David and Murtagh, Fionn and Smyth, Padhraic},
isbn = {978-1-58488-546-7},
publisher = {Chapman {\&} Hall/CRC},
title = {{Design and modeling for computer experiments}},
booktitle={Computer Science and Data Analysis Series},
year = {2006}
}

@article{ferretti2016,
author = {Ferretti, Federico and Saltelli, Andrea and Tarantola, Stefano},
doi = {10.1016/j.scitotenv.2016.02.133},
journal = {Science of The Total Environment},
keywords = {Bibliometric analysis,Chemical modelling,Global sensitivity analysis,Sensitivity analysis},
month = {oct},
pages = {666--670},
publisher = {British Geological Survey, NERC},
title = {{Trends in sensitivity analysis practice in the last decade}},
volume = {568},
year = {2016}
}

@article{forrester2007,
author = {Forrester, Alexander I.J. and S{\'{o}}bester, Andr{\'{a}}s and Keane, Andy J},
doi = {10.1098/rspa.2007.1900},
journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
month = {dec},
number = {2088},
pages = {3251--3269},
title = {{Multi-fidelity optimization via surrogate modelling}},
volume = {463},
year = {2007}
}

@article{forrester2009,
author = {Forrester, Alexander I J and Keane, Andy J.},
doi = {10.1016/j.paerosci.2008.11.001},
journal = {Progress in Aerospace Sciences},
number = {1-3},
pages = {50--79},
title = {{Recent advances in surrogate-based optimization}},
volume = {45},
year = {2009}
}

@book{hastie2009,
address = {New York, NY},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
booktitle = {Journal of Computational Science},
doi = {10.1007/978-0-387-84858-7},
isbn = {978-0-387-84857-0},
publisher = {Springer New York},
series = {Springer Series in Statistics},
title = {{The Elements of Statistical Learning}},
volume = {2},
year = {2009}
}

@article{iooss2010,
author = {Iooss, B. and Boussouf, L. and Feuillard, V. and Marrel, A.},
journal = {International Journal on Advances in Systems and Measurements},
number = {1},
pages = {11--21},
title = {{Numerical studies of the metamodel fitting and validation processes}},
volume = {3},
year = {2010}
}

@incollection{iooss2016,
author = {Iooss, B. and Saltelli, A.},
booktitle = {Handbook of Uncertainty Quantification},
doi = {10.1007/978-3-319-11259-6\_31-1},
keywords = {Computer experiments,Impact assessment,Risk assessment,Sensitivity analysis,Sensitivity auditing,Uncertainty analysis},
pages = {1--20},
publisher = {Springer International Publishing},
title = {{Introduction to Sensitivity Analysis}},
year = {2016}
}

@article{ishigami1990,
author = {Ishigami, T. and Homma, T.},
doi = {10.1109/ISUMA.1990.151285},
journal = {IEEE},
pages = {398--403},
title = {{An importance quantification technique in uncertainty analysis for computer models}},
year = {1990}
}

@inproceedings{kohavi1995,
author = {Kohavi, Ron},
series = {International Joint Conference on Artificial Intelligence},
title = {{A Study of Cross-Validation and Bootstrap for Accuracy Estimaiton and Model Selection}},
year = {1995}
}

@article{krige1989,
author = {Krige, D G and Guarascio, M and Camisani-Calzolari, F A},
journal = {Geostatistics},
pages = {1--19},
title = {{Early South African geostatistical techniques in today's perspective}},
volume = {1},
year = {1989}
}

@article{kucherenko2016,
author = {Kucherenko, Sergei and Song, Shugfang},
doi = {10.1007/978-3-319-33507-0_23},
journal = {Springer Proceedings in Mathematics and Statistics},
keywords = {Derivative based global measures,Global sensitivity analysis,Monte carlo methods,Morris method,Quasi monte carlo methods,Sobol' sensitivity indices},
pages = {455--469},
title = {{Derivative-based global sensitivity measures and their link with sobol' sensitivity indices}},
volume = {163},
year = {2016}
}

@incollection{legratiet2016,
author="Gratiet, Lo{\"i}c Le
and Marelli, Stefano
and Sudret, Bruno",
editor="Ghanem, Roger
and Higdon, David
and Owhadi, Houman",
title="Metamodel-Based Sensitivity Analysis: Polynomial Chaos Expansions and Gaussian Processes",
bookTitle="Handbook of Uncertainty Quantification",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="1--37",
isbn="978-3-319-11259-6",
doi="10.1007/978-3-319-11259-6_38-1"
}

@article{mckay1979,
author = {Mckay, M. D. and Beckman, R .J. and Conover, W. J.},
journal = {Technometrics},
number = {2},
pages = {239--245},
title = {{A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code}},
volume = {21},
year = {1979}
}

@article{margheri2016,
author = {Margheri, Luca and Sagaut, Pierre},
doi = {10.1016/j.jcp.2016.07.036},
journal = {Journal of Computational Physics},
pages = {137--173},
publisher = {Elsevier Inc.},
title = {{A hybrid anchored-ANOVA-POD/Kriging method for uncertainty quantification in unsteady high-fidelity CFD simulations}},
volume = {324},
year = {2016}
}

@article{marrel2009,
title = {Calculations of Sobol indices for the Gaussian process metamodel},
journal = {Reliability Engineering \& System Safety},
volume = {94},
number = {3},
pages = {742 - 751},
year = {2009},
doi = {http://dx.doi.org/10.1016/j.ress.2008.07.008},
author = {Amandine Marrel and Bertrand Iooss and Beatrice Laurent and Olivier Roustant}
}

@article{marrel2012,
author = {Marrel, Amandine and Iooss, Bertrand and {Da Veiga}, S{\'{e}}bastien and Ribatet, Mathieu},
doi = {10.1007/s11222-011-9274-8},
journal = {Statistics and Computing},
month = {may},
number = {3},
pages = {833--847},
title = {{Global sensitivity analysis of stochastic computer models with joint metamodels}},
volume = {22},
year = {2012}
}

@incollection{marrel2015,
author = {Marrel, Amandine and Saint-Geours, Nathalie and {De Lozzo}, Matthias},
booktitle = {Handbook of Uncertainty Quantification},
doi = {10.1007/978-3-319-11259-6\_39-1},
isbn = {978-3-319-11259-6},
pages = {1--31},
publisher = {Springer International Publishing},
title = {{Sensitivity Analysis of Spatial and/or Temporal Phenomena}},
year = {2015}
}

@article{martin2005,
author = {Martin, Jay D. and Simpson, Timothy W.},
doi = {10.2514/1.8650},
journal = {AIAA Journal},
number = {4},
pages = {853--863},
title = {{Use of Kriging Models to Approximate Deterministic Computer Models}},
volume = {43},
year = {2005}
}

@inproceedings{masquelet2017,
author = {Masquelet, Matthieu and Yann, Jin and Dord, Anne and Laskowski, Gregory and Lee, Shunn and Jofre, Lluis and Iaccarino, Gianluca},
series = {ASME Turbo Expo 2017: Turbine Technical Conference and Exposition},
pages = {1--11},
title = {{Uncertainty Quantification in Large Eddy Simulations of a Rich-Dome Aviation Gas Turbine}},
year = {2017}
}

@article{molga2005,
author = {Molga, Marcin and Smutnicki, Czes{\l}aw},
number = {c},
pages = {1--43},
title = {{Test functions for optimization needs}},
year = {2005}
}

@article{najm2009,
author = {Najm, Habib N},
doi = {10.1146/annurev.fluid.010908.165248},
journal = {Annual Review of Fluid Mechanics},
keywords = {cfd,pc,polynomial chaos,uq},
month = {jan},
number = {1},
pages = {35--52},
title = {{Uncertainty Quantification and Polynomial Chaos Techniques in Computational Fluid Dynamics}},
volume = {41},
year = {2009}
}

@article{owen2017,
author = {Owen, N E and Challenor, P. and Menon, P. P. and Bennani, S.},
doi = {10.1137/15M1046812},
issn = {2166-2525},
journal = {SIAM/ASA Journal on Uncertainty Quantification},
month = {jan},
number = {1},
pages = {403--435},
title = {{Comparison of Surrogate-Based Uncertainty Quantification Methods for Computationally Expensive Simulators}},
volume = {5},
year = {2017}
}

@book{rasmussen2006,
author = {Rasmussen, C.E. and Williams, C},
publisher = {MIT Press},
title = {{Gaussian processes for machine learning}},
year = {2006}
}

@unpublished{roy2016, author = {Roy, Pamphile T.}, title = {Uncertainty Quantification applied to Turbine Design},
year = {2016},
institution = {ISAE-SUPAERO CERFACS},
type = {Unpublished results},
addendum={Unpublished results},
address = {Toulouse, France},
url= {http://cerfacs.fr/wp-content/uploads/2016/09/CFD_Roy_rapport_stage.pdf}
}

@article{sacks1989,
author = {Sacks, Jerome and Welch, Williams J. and Mitchell, Toby J. and Wynn, Henry P.},
doi = {10.1214/ss/1177012413},
journal = {Statistical Science},
month = {nov},
number = {4},
pages = {409--423},
title = {{Design and Analysis of Computer Experiments}},
volume = {4},
year = {1989}
}



@phdthesis{scheidt2006,
author = {Scheidt, C{\'{e}}line},
school = {Louis Pasteur},
title = {{Analyse statistique d'exp{\'{e}}riences simul{\'{e}}es : Mod{\'{e}}lisation adaptative de r{\'{e}}ponses non-r{\'{e}}guli{\`{e}}res par krigeage et plans d'exp{\'{e}}riences, Application {\`{a}} la quantification des incertitudes en ing{\'{e}}nierie des r{\'{e}}servoirs p{\'{e}}troliers}},
year = {2006}
}

@article{schoebi2015,
author = {Schoebi, R. and Sudret, B. and Wiart, J.},
journal = {International Journal for Uncertainty Quantification},
title = {{Polynomial-Chaos-based Kriging}},
volume = {5},
number = {2},
pages = {171--193},
year = {2015}
}

@article{sheikholeslami2017,
author = {Sheikholeslami, Razi and Razavi, Saman},
doi = {10.1016/j.envsoft.2017.03.010},
journal = {Environmental Modelling {\&} Software},
pages = {109--126},
title = {{Progressive Latin Hypercube Sampling: An efficient approach for robust sampling-based analysis of environmental models}},
volume = {93},
year = {2017}
}

@article{sirovich1987,
author = {Sirovich, Lawrence},
journal = {Quarterly of Applied Mathematics},
number = {3},
pages = {561--571},
title = {{Turbulence and the dynamics of coherent structures part i: coherent structures}},
volume = {XLV},
year = {1987}
}

@inproceedings{smith1998,
author = {Smith, Warren D. and Wormald, Nick},
series = {39th Annual Symposium on Foundations of Computer Science},
doi = {10.1109/SFCS.1998.743449},
title = {{Geometric Separator Theorems and Applications}},
year = {1998}
}

@article{sobol1993,
author = {Sobol', I.M},
journal = {Mathematical Modeling and Computational Experiment},
keywords = {Anova,Sobol' sensitivity indices,Sobol' variance decomposition},
number = {4},
pages = {407--414},
title = {{Sensitivity analysis for nonlinear mathematical models}},
volume = {1},
year = {1993}
}

@article{sudret2008,
author = {Sudret, B.},
title = {Global Sensitivity analysis using polynomial chaos expansions},
journal = {Reliability Engineering and System Safety},
year = {2008},
volume = {93},
number = {7},
doi = {10.1016/j.ress.2007.04.002},
pages = {964--979}
}

@book{wand1995,
address = {Boston, MA},
author = {Wand, M. P. and Jones, M. C.},
doi = {10.1007/978-1-4899-4493-1},
isbn = {978-0-412-55270-0},
publisher = {Springer US},
title = {{Kernel Smoothing}},
year = {1995}
}

@article{wales1997,
author = {Wales, David J. and Doye, Jonathan P. K.},
doi = {10.1021/jp970984n},
journal = {The Journal of Physical Chemistry A},
number = {28},
pages = {5111--5116},
title = {{Global Optimization by Basin-Hopping and the Lowest Energy Structures of Lennard-Jones Clusters Containing up to 110 Atoms}},
volume = {101},
year = {1997}
}



@Article{ELSheikh2013,
author="ELSheikh, A. H.
and Pain, C. C.
and Fang, F.
and Gomes, J. L. M. A.
and Navon, I. M.",
title="Parameter estimation of subsurface flow models using iterative regularized ensemble Kalman filter",
journal="Stochastic Environmental Research and Risk Assessment",
year="2013",
volume="27",
number="4",
pages="877--897"
}

@Article{Ciriello2013,
author="Ciriello, V.
and Di Federico, V.
and Riva, M.
and Cadini, F.
and De Sanctis, J.
and Zio, E.
and Guadagnini, A.",
title="Polynomial chaos expansion for global sensitivity analysis applied to a model of radionuclide migration in a randomly heterogeneous aquifer",
journal="Stochastic Environmental Research and Risk Assessment",
year="2013",
volume="27",
number="4",
pages="945--954"
}

@Article{Marrel2015,
author="Marrel, A.
and Perot, G.
and Mottet, C.",
title="Development of a surrogate model and sensitivity analysis for spatio-temporal numerical simulators",
journal="Stochastic Environmental Research and Risk Assessment",
year="2015",
volume="29",
number="3",
pages="959--974"
}

@phdthesis{barthelemy2015,
	author = {Barth\'el\'emy, S.},
	school = {Institut National Polytechnique de Toulouse},
	title = {Assimilation de donn\'ee ensembliste et couplage de mod\`eles hydrauliques 1D-2D pour la pr\'evision des crues en temps r\'eel: application au r\'eseau hydraulique Adour Maritime.},
	year = {2015}
}
@phdthesis{elmocaydphd,
	author = {El Mo\c{c}ayd, N.},
	school = {Institut National Polytechnique de Toulouse},
	title = {La d\'ecomposition en polyn\^omes du chaos pour l\'am\'elioration de l\'assimilation de donn\'ees ensembliste en hydraulique fluviale.},
	year = {2017},
}
@article{baudin2015,
archivePrefix = {arXiv},
arxivId = {1501.05242},
author = {Baudin, M. and Dutfoy, A. and Iooss, B. and Popelin, A.-L.},
keywords = {C++ library,OpenTURNS,estima-,genericity,multivariate distri- bution,open source,probability,propagation,python module,quantification,random vectors,sensitivity,simulation,statistics,tion,transparency,uncertainty},
title = {{OpenTURNS: An industrial software for uncertainty quantification in simulation}},
year = {2015}
}

@inproceedings{baudin2016,
address = {R{\'{e}}union Island},
author = {Baudin, M. and Boumhaout, K. and Delage, T. and Iooss, B. and Martinez, J.-M.},
booktitle = {8th International Conference on Sensitivity Analysis of Model Output,},
title = {{Numerical stability of Sobol' indices estimation formula}},
year = {2016}
}

@phdthesis{berveiller2005,
author = {Berveiller, M.},
title = {El\'ements finis stochastiques : approches intrusive et non intrusive pour des
analyses de fiabilit\'e},
school = { Universit\'e Blaise Pascal, Clermont-Ferrand},
year = {2005}
}

@article{besnard2011,
	Author = {Besnard, A. and Goutal, N.},
	Journal = {La Houille Blanche},
	Volume = {3},
	Pages = {42--47},
	Publisher = {Soci\'{e}t\'{e} Hydrotechnique de France},
	Title = {{Comparaison de mod\`{e}les 1D \`{a} casiers et 2D pour la mod\'{e}lisation hydraulique d'une plaine d'inondation--Cas de la Garonne entre Tonneins et La R\'{e}ole}},
	Year = {2011}
}
    
@article{birolleau2014,
author = {Birolleau, A. and Poette, G. and Lucor, D.},
journal = {Commun. Comput. Phys.},
pages = {1--34},
title = {{Adaptive Bayesian inference for discontinuous inverse problems, application to hyperbolic conservation laws}},
volume = {16},
year = {2014}
}

@phdthesis{blatman2009phd,
author = {Blatman, G.},
title = {{Adaptative sparse Polynomial Chaos expansions for uncertainty propagation and sensitivity analysis}},
school = {Universit\'e Blaise Pascal, Clermont-Ferrand},
year = {2009}
}

@article{bozzi2015,
	Author = {Bozzi, S. and Passoni, G. and Bernardara, P. and Goutal, N. and Arnaud, A.},
	Doi = {10.1007/s10666-014-9430-6},
	Journal = {Environmental Modeling \& Assessment},
	Volume = {4},
	Title = {{Roughness and Discharge Uncertainty in 1D Water Level Calculations}},
	Year = {2014},
}

@article{chatterjee2000,
author = {Chatterjee, A.},
journal = {Current Science},
number = {7},
title = {{An introduction to the proper orthogonal decomposition}},
volume = {78},
year = {2000}
}

@book{clarke1992,
author = {Clarke, B. and Fokoue, E. and Zhang, H. H.},
doi = {10.1007/978-1-4612-4380-9},
editor = {Kotz, Samuel and Johnson, Norman L.},
isbn = {978-0-387-94039-7},
pages = {1--694},
publisher = {Springer New York},
series = {Springer Series in Statistics},
title = {{Breakthroughs in Statistics}},
year = {1992}
}

@article{cloke2009,
	Author = {Cloke, H.L. and Pappenberger, F.},
  	Title = {{Ensemble flood forecasting: A review}},
	Doi = {10.1016/j.jhydrol.2009.06.005},
	Isbn = {0022-1694},
	Issn = {00221694},
	Journal = {Journal of Hydrology},
	Pages = {613--626},
	Volume = {375},
	Year = {2009}
}

@article{damblin2013,
author = {Damblin, G. and Couplet, M. and Iooss, B.},
journal = {Journal of Simulation},
keywords = {Latin Hypercube Sampling,computer experiment,discrepancy,optimal design},
pages = {276--289},
title = {{Numerical studies of space filling designs : optimization of Latin Hypercube Samples and subprojection properties}},
year = {2013}
}
  
@article{dechant2011,
	Author = {Dechant, C.M. and Moradkhani, H.},
	Title = {{Improving the characterization of initial condition for ensemble streamflow prediction using data assimilation}},
    Doi = {10.5194/hess-15-3399-2011},
	Journal = {Hydrology and Earth System Sciences},
	Pages = {3399--3410},
	Volume = {15},
	Year = {2011}
}

@article{deman2015,
author = {Deman, G. and Konakli, K. and Sudret, B. and Kerrou, J. and Perrochet, P. and Benabderrahmane, H.},
title = {Using sparse polynomial chaos expansions for the global sensitivity analysis of groundwater lifetime expectancy in a multi-layered hydrogeological model},
journal = {Reliability Engineering and System Safety},
year = {2015},
volume = {147},
pages = {156-169}
}

@inbook{despres2013,
author={Despr{\'e}s, B. and Poette, G. and Lucor, G.},
editor={Bijl, H. and Lucor, D. and Mishra, S. and Schwab, C.},
title={Robust Uncertainty Propagation in Systems of Conservation Laws with the Entropy Closure Method},
bookTitle={Uncertainty Quantification in Computational Fluid Dynamics},
year={2013},
publisher={Springer International Publishing},
address={Cham},
pages={105--149},
doi={10.1007/978-3-319-00885-1\_3}
}

@article{dubreuil2014,
author = {S. Dubreuil and M. Berveiller and F. Petitjean and M. Salaün},
doi = {10.1016/j.ress.2013.09.011},
title = {Construction of bootstrap confidence intervals on sensitivity indices computed by polynomial chaos expansion},
journal = {Reliability Engineering and System Safety},
volume = {121},
pages = {263-275},
year = {2014}
}

@article{durand2008,
	Author = {Durand, M. and Andreadis, K.M. and Alsdorf, D.E. and Lettenmaier, D.P. and Moller, D. and Wilson, M.},
	Doi = {10.1029/2008GL034150},
	Journal = {Geophysical Research Letters},
	Pages = {1--5},
	Title = {{Estimation of bathymetric depth and slope from data assimilation of swath altimetry into a hydrodynamic model}},
	Volume = {35},
	Year = {2008}
}

@article{durand2010,
author={M. Durand and E. Rodriguez and D. E. Alsdorf and M. Trigg},
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
title={Estimating River Depth From Remote Sensing Swath Interferometry Measurements of River Height, Slope, and Width},
year={2010},
volume={3},
number={1},
pages={20-31},
doi={10.1109/JSTARS.2009.2033453},
month={March}
}

@InProceedings{dutka2009,
title = {Implementation of a polynomial chaos toolbox in OpenTurns with test-case application},
year = {2009},
author = {Dutka-Malen, I. and Lebrun, R. and Saassouh, B. and Sudret, B.},
Booktitle = { Conference: Proc. 10th~Int. Conf. Struct. Safety and Reliability (ICOSSAR'2009), Osaka, Japan }
}

@unpublished{elmocaydEMA,
	Author = {El Mo\c{c}ayd, N. and Ricci, S. and Goutal, N. and Rochoux, M.C. and Boyaval, S. and Goeury, C. and Lucor, D. and Thual, O.},
	Journal = {Under final review to Environmental Modeling Assessment },
    year = {2017},
	Title = {{Polynomial surrogate model for open-channel flows in steady state}}
}

@article{evensen1994,
	Author = {Evensen, G.},
	Doi = {10.1029/94JC00572},
	Journal = {Journal of Geophysical Research},
	Number = {C5},
	Pages = {10143--10162},
	Pmid = {13},
	Title = {{Sequential data assimilation with a nonlinear quasi-geostrophic model using Monte Carlo methods to forecast error statistics}},
	Volume = {99},
	Year = {1994}
}

@article{Forrester2009,
author = {Forrester, A. I. J. and Keane, A. J.},
doi = {10.1016/j.paerosci.2008.11.001},
journal = {Progress in Aerospace Sciences},
number = {1-3},
pages = {50--79},
title = {{Recent advances in surrogate-based optimization}},
volume = {45},
year = {2009}
}

@Article{ge2008,
author = {Liang, G. and Kwok Fai, C. and Kobayashi, M. H.},
title = {Stochastic Solution for Uncertainty Propagation in Nonlinear Shallow-Water Equations},
journal = {Journal of Hydraulic Engineering},
year = {2008},
doi = {10.1061/(ASCE)0733-9429(2008)134:12(1732)},
volume = {134},
number = {12},
pages = {1732--1743}
}

@article{goutal2002,
	Author = {Goutal, N. and Maurel, F.},
	Journal = {International Journal for Numerical Methods in Fluids},
	Number = {1},
	Pages = {1--19},
	Publisher = {Wiley Online Library},
	Title = {{A finite volume solver for 1D shallow-water equations applied to an actual river}},
	Volume = {38},
	Year = {2002}
}
    
@article{habert2016,
	Author = {Habert, J. and Ricci, S. and {Le Pape}, E. and Thual, O. and Piacentini, A. and Goutal, N. and Jonville, G. and Rochoux, M.},
	Title = {{Reduction of the uncertainties in the water level-discharge relation of a 1D hydraulic model in the context of operational flood forecasting}},
    Doi = {10.1016/j.jhydrol.2015.11.023},
	Journal = {Journal of Hydrology},
	Volume = {532},
	Pages = {52-64},
	Year = {2016}
}

@book{hastie2009,
address = {New York, NY},
author = {Hastie, T. and Tibshirani, R. and Friedman, J.},
booktitle = {Journal of Computational Science},
doi = {10.1007/978-0-387-84858-7},
isbn = {978-0-387-84857-0},
publisher = {Springer New York},
series = {Springer Series in Statistics},
title = {{The Elements of Statistical Learning}},
volume = {2},
year = {2009}
}

@article{higdon2004,
author = {D. Higdon and M. Kennedy and J. C. Cavendish and J. A. Cafeo and R. D. Ryne},
title = {Combining Field Data and Computer Simulations for Calibration and Prediction},
journal = {SIAM Journal on Scientific Computing},
volume = {26},
number = {2},
pages = {448-466},
year = {2004},
doi = {10.1137/S1064827503426693}
}

@article{horritt2002,
author = {Horritt, M.S. and Bates, P.D.},
title = {Evaluation of 1D and 2D numerical models for predicting river flood inundation},
journal = {Journal of Hydrology},
year = {2002},
volume = {268},
pages = {87--99}
}

@inproceedings{hosder2006,
author = {Hosder, S. Perez, R. and Walters, R.W.},
title = {A non-intrusive polynomial chaos method for uncertainty propagation in CFD simulations},
booktitle = {48th AIAA Aerospace Sciences Meeting and Exhibit},
year = {2006},
number = {AIAA-2010-0129},
publisher = {The American Institute of Aeronautics and Astronautics, Inc},
}

@incollection{iooss2016,
author = {Iooss, B. and Saltelli, A.},
booktitle = {Handbook of Uncertainty Quantification},
doi = {10.1007/978-3-319-11259-6\_31-1},
keywords = {Computer experiments,Impact assessment,Risk assessment,Sensitivity analysis,Sensitivity auditing,Uncertainty analysis},
pages = {1--20},
publisher = {Springer International Publishing},
title = {{Introduction to Sensitivity Analysis}},
year = {2016}
}

@article{iooss2010,
author = {Iooss, B. and Boussouf, L. and Feuillard, V. and Marrel, A.},
journal = {International Journal on Advances in Systems and Measurements},
number = {1},
pages = {11--21},
title = {{Numerical studies of the metamodel fitting and validation processes}},
volume = {3},
year = {2010}
}

@article{ishigami1990,
author = {Ishigami, T. and Homma, T.},
doi = {10.1109/ISUMA.1990.151285},
isbn = {0-8186-2107-9},
journal = {IEEE},
pages = {398--403},
title = {{An importance quantification technique in uncertainty analysis for computer models}},
year = {1990}
}

@article{krause2005,
author = {Krause, P and Boyle, D P and B{\"{a}}se, F.},
doi = {10.5194/adgeo-5-89-2005},
journal = {Advances in Geosciences},
number = {89},
pages = {89--97},
title = {{Comparison of different efficiency criteria for hydrological model assessment}},
volume = {5},
year = {2005}
}

@article{lamboni2011,
author = {Lamboni, M. and Monod, H. and Makowski, D.},
doi = {10.1016/j.ress.2010.12.002},
journal = {Reliability Engineering and System Safety},
keywords = {Dynamic model,Factorial design,Latin hypercube sampling,Principal components analysis,RKHS,Sensitivity analysis,Sobol' decomposition},
number = {4},
pages = {450--459},
title = {{Multivariate sensitivity analysis to measure global contribution of input factors in dynamic models}},
volume = {96},
year = {2011}
}

@incollection{legratiet2017,
author = {{Le Gratiet}, L. and Marelli, S. and Sudret, B.},
booktitle = {Handbook of Uncertainty Quantification},
doi = {10.1007/978-3-319-11259-6\_38-1},
keywords = {Error estimation,Gaussian process regression,Kriging,Model selection,Polynomial chaos expansions, Sobol' indices},
pages = {1--37},
publisher = {Springer International Publishing},
title = {{Metamodel-Based Sensitivity Analysis: Polynomial Chaos Expansions and Gaussian Processes}},
year = {2017}
}

@article{legratiet2014,
author = {Le Gratiet, L. and Cannamela, C. and Iooss, B.},
title = {A Bayesian Approach for Global Sensitivity Analysis of (Multifidelity) Computer Codes},
journal = {SIAM/ASA Journal on Uncertainty Quantification},
volume = {2},
number = {1},
pages = {336-363},
year = {2014},
doi = {10.1137/130926869}
}

@book{lemaitreknio2010,
author = {Le Maitre, O. and Knio, O.},
title = {Spectral Methods for Uncertainty Quantification},
publisher = {Springer},
year = {2010}
}

@article{lixiu2008,
author = {Li, J. and Xiu, D.},
title = {On numerical properties of the ensemble {K}alman filter for data assimilation},
journal = {Comput. Methods Appl. Mech. Engrg.},
year = {2008},
volume = {197},
pages = {3574-3583}
}

@article{lixiu2009,
title = {A generalized polynomial chaos based ensemble Kalman filter with high accuracy},
journal = {Journal of Computational Physics},
volume = {228},
number = {15},
pages = {5454--5469},
year = {2009},
doi = {10.1016/j.jcp.2009.04.029},
author = {Li, J. and Xiu, D.}
}

@article{lockwood2012,
author = {Lockwood, B.A. and Anitescu, M.},
journal = {Nuclear Science and Engineering},
keywords = {derivative,gaussian process,nuclear engi-,uncertainty quantification,universal kriging},
pages = {1--32},
title = {{Gradient-enhanced universal kriging for uncertainty propagation}},
year = {2012}
}

@article{lucor2007,
author = {Lucor, D. and Meyers, J. and Sagaut, P.},
doi = {10.1017/S0022112007006751},
journal = {Journal of Fluid Mechanics},
pages = {255--279},
title = {{Sensitivity analysis of large-eddy simulations to subgrid-scale-model parametric uncertainty using polynomial chaos}},
volume = {585},
year = {2007}
}

@article{marrel2009,
title = {Calculations of Sobol indices for the Gaussian process metamodel},
journal = {Reliability Engineering \& System Safety},
volume = {94},
number = {3},
pages = {742 - 751},
year = {2009},
doi = {http://dx.doi.org/10.1016/j.ress.2008.07.008},
author = {A. Marrel and B. Iooss and B. Laurent and O. Roustant}
}

@article{matgen2010,
	Author = {Matgen, P. and Montanari, M. and Hostache, R. and Pfister, L. and Hoffmann, L. and Plaza, D. and Pauwels, V.R.N. and {De Lannoy}, G.J.M. and {De Keyser}, R. and Savenije, H.H.G.},
	Doi = {10.5194/hess-14-1773-2010},
	Journal = {Hydrology and Earth System Sciences},
	Pages = {1773--1785},
	Title = {{Towards the sequential assimilation of SAR-derived water stages into hydraulic models using the Particle Filter: Proof of concept}},
	Volume = {14},
	Year = {2010}
}

@article{migliorati2013,
author = {Migliorati, G. and Nobile, F. and Von Schwerin, E. and Tempone, R.},
title = {{Approximation of quantities of interest in stochastic PDEs by the random Discret L2 Projection on polynomial spaces}},
journal = {SIAM J. Sci Comput.},
year = {2013},
volume = {35},
number = {3},
pages = {A1440-A1460}
}

@article{molga2005,
author = {Molga, M. and Smutnicki, C.},
number = {c},
pages = {1--43},
title = {{Test functions for optimization needs}},
year = {2005}
}

@article{moradkhani2005,
	Author = {Moradkhani, H. and Sorooshian, S. and Gupta, H.V. and Houser, P.R.},
	Title = {{Dual state-parameter estimation of hydrological models using ensemble Kalman filter}},
    Doi = {10.1016/j.advwatres.2004.09.002},
	Journal = {Advances in Water Resources},
	Pages = {135--147},
	Volume = {28},
	Year = {2005}
}

@article{moradkhani2008,
	Author = {Moradkhani, H.},
	Doi = {10.3390/s8052986},
	Journal = {Sensors},
	Keywords = {Data assimilation,Remote sensing,Snow,Soil moisture},
	Pages = {2986--3004},
	Title = {{Hydrologic remote sensing and land surface data assimilation}},
	Volume = {8},
	Year = {2008}
}

@article {oakley2004,
author = {Oakley, J.E. and O'Hagan, A.},
title = {Probabilistic sensitivity analysis of complex models: a Bayesian approach},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {66},
number = {3},
publisher = {Blackwell Publishing},
doi = {10.1111/j.1467-9868.2004.05304.x},
pages = {751--769},
year = {2004}
}

@article{owen2015,
author = {Owen, N.E. and Challenor, P. and Menon, P. P. and Bennani, S.},
pages = {403--435},
title = {{Comparison of surrogate-based uncertainty quantification methods for computationally expensive simulators}},
journal = {SIAM/ASA J. Uncertainty Quantification},
volume = {5},
number = {1},
doi = {10.1137/15M1046812},
year = {2017}
}

@article{pajonk2013,
title = {Sampling-free linear Bayesian updating of model state and parameters using a square root approach },
journal = {Computers \& Geosciences},
volume = {55},
pages = {70--83},
year = {2013},
note = {Ensemble Kalman filter for data assimilation },
doi = {10.1016/j.cageo.2012.05.017},
author = {Pajonk, O. and Rosic, B.V. and Matthies, H.G.}
}

@article{parrish2012,
	Author = {Parrish, M.A. and Moradkhani, H. and Dechant, C.M.},
	Doi = {10.1029/2011WR011116},
	Journal = {Water Resources Research},
	Number = {W03519},
	Pages = {1--18},
	Title = {{Toward reduction of model uncertainty: Integration of Bayesian model averaging and data assimilation}},
	Volume = {48},
	Year = {2012}
}

@article{pedregosa2011,
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, {\'{E}}.},
eprint = {1201.0490},
isbn = {9781783281930},
journal = {Journal of Machine Learning Research},
number = {2825-2830},
title = {{Scikit-learn: Machine Learning in Python}},
volume = {12},
year = {2012}
}

@book{rasmussen2006,
author = {Rasmussen, C.E. and Williams, C},
publisher = {MIT Press},
title = {{Gaussian processes for machine learning}},
year = {2006}
}

@article{rochoux2014,
author = {Rochoux, M.C. and Ricci, S. and Lucor, D.and  Cuenot, B. and Trouv\'e, A.},
title = {{Towards predictive data-driven simulations of wildfire spread - Part 1: Reduced-cost Ensemble Kalman Filter based on a Polynomial Chaos surrogate model for parameter estimation}},
journal = {Nat. Hazards and Earth Syst. Sci.},
year = {2014},
volume = {14},
number = {11},
pages = {2951-2973}
}

@phdthesis{rochoux2014phd,
	Author = {Rochoux, M.},
    School = {Ecole Centrale Paris},
	Title = {{Vers une meilleure pr\'{e}vision de la propagation d'incendies de for\^{e}t : \'{e}valuation de mod\`{e}les et assimilation de donn\'{e}es}},
	Year = {2014}
}


@phdthesis{saad2007phd,
	Author = {Saad, G.A.},
    School = {Faculty of the Graduate
School, University of Southern California},
	Title = {{Stochastic Data Assimilation with Application to Multi-Phase Flow and Health Monitoring Problems}},
	Year = {2007}
}

@article{sirovich1987,
author = {Sirovich, L.},
journal = {Quarterly of Applied Mathematics},
number = {3},
pages = {561--571},
title = {{Turbulence and the dynamics of coherent structures part i: coherent structures}},
volume = {XLV},
year = {1987}
}

@article{smirnov1939,
  title={Estimate of difference between empirical distribution curves in two independent samples},
  author={Smirnov, NV},
  journal={Byull. Mosk. Gos. Univ},
  volume={2},
  number={2},
  year={1939}
}

@article{Sobol1993,
author = {Sobol′, I.M},
journal = {Mathematical Modeling and Computational Experiment},
keywords = {Anova,Sobol' sensitivity indices,Sobol' variance decomposition},
number = {4},
pages = {407--414},
title = {{Sensitivity analysis for nonlinear mathematical models}},
volume = {1},
year = {1993}
}

@article{storlie2009,
author = {Storlie, C.B. and Swiler, L.P. and Helton, J.C. and Sallaberry, C.J.},
doi = {10.1016/j.ress.2009.05.007},
journal = {Reliability Engineering {\&} System Safety},
keywords = {Bootstrap,Confidence intervals,Meta-model,Nonparametric regression,Sensitivity analysis,Surrogate model,Uncertainty analysis,Variance decomposition},
month = {nov},
number = {11},
pages = {1735--1763},
title = {{Implementation and evaluation of nonparametric regression procedures for sensitivity analysis of computationally demanding models}},
volume = {94},
year = {2009}
}

@article{sudret2008,
author = {Sudret, B.},
title = {Global Sensitivity analysis using polynomial chaos expansions},
journal = {Reliability Engineering and System Safety},
year = {2008},
volume = {93},
number = {7},
doi = {10.1016/j.ress.2007.04.002},
pages = {964--979}
}

@book{thual2010,
	Author = {Thual, O.},
	Publisher = {Ecole polytechnique},
	Title = {{Hydrodynamique de l'environnement}},
	Year = {2010}
}
    
@article{weerts2011,
	Author = {Weerts, A.H. and Winsemius, H.C. and Verkade, J.S.},
	Title = {{Estimation of predictive hydrological uncertainty using quantile regression: examples from the National Flood Forecasting System (England and Wales)}},
    Doi = {10.5194/hess-15-255-2011},
	Journal = {Hydrology and Earth System Sciences},
	Pages = {255--265},
	Volume = {15},
	Year = {2011}
}

@book{xiu2010,
Author = {Xiu, D.},
title = {Numerical Methods for Stochastic Computations: A Spectral Method Approach},
publisher = {Princeton University Press},
year = {2010}
}

@article{xiu2002,
author = {Xiu, D. and Karniadakis, G.E.},
title = {The Wiener--Askey Polynomial Chaos for Stochastic Differential Equations},
journal = {SIAM Journal on Scientific Computing},
volume = {24},
number = {2},
pages = {619-644},
year = {2002},
doi = {10.1137/S1064827501387826}
}

@article{goutal2012,
author = {Goutal, N. and  Lacombe, J.-M. and Zaoui, F. and El-Kadi-Adberrezzak K.},
title = {MASCARET: a 1-D open souces software for flow hydrodynamic and water quality in open channel networks},
journal = {River Flow},
pages = {1169-1174},
year = {2012},
}
