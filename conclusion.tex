%\& Perspectives
\chapter*{Conclusion}

% Quick summary
\lettrine{T}{his thesis} work proposes some methodological improvements related to Uncertainty Quantification (UQ). Apart from the novelty of these methods, they proved to be useful in various costly numerical experiments.\\

% First part: literature
The first part of the document reviews the literature on various aspects of UQ. The reader is walked through all the steps required to perform a UQ study in a costly numerical environment: from the definition of a Design of Experiments (DoE) to construct a surrogate model, to the use of this surrogate to compute statistics and ultimately visualize the results. The current advances and limitations of DoE are presented in~\cref{sec:doe}. OLHS and \emph{Sobol'} sequences are both considered as best practice although they exhibit some limitations such as the iterative properties and the randomization, respectively. Using this sample, one can perform a UQ analysis with methods detailed in~\cref{sec:uq}. Global Sensitivity Analysis (GSA) is recommended over local SA. Looking at the method themselves, variance-based SA is still predominant but moment-based methods appear as good complement in a study. In a costly numerical environment, it is not tractable to directly perform a UQ and the usage of a surrogate model comes as a solution. \Cref{sec:surrogate} exposes two of the most used methods: Gaussian Process (GP) and Polynomial Chaos (PC). Finally, \cref{sec:visu} introduces the problem of visualizing the uncertainties. This is a fairly new field with only a limited corpus of study. There is currently no clear recommendations about how to visualize uncertainties. From this review, \cref{chap:questions} details the three questions which this thesis aims to answer:
\begin{itemize}
\item \emph{How to construct a DoE in a high-dimensional parameter space?}
\item \emph{How to resample a DoE by considering the QoI of already sampled experiments?}
\item \emph{How to visualize uncertainties in high-dimensional cases?}
\end{itemize}

% Second part: novel methods
The following part seeks to answer these questions. It begins with \cref{chap:dissemination} outlining all the methodological contributions of this thesis. A new UQ tool is presented in~\cref{chap:batman}: Batman. Batman is an open-source Python library dedicated to UQ which has been developed during this thesis. It is based on state-of-the-art statistical libraries, such as OpenTURNS, and it allows to easily perform an uncertainty analysis. All further methodological developments of this work have been consolidated through this tool. \Cref{chap:doe} presents a new iterative and versatile sampling method named KDOE. It relies on a Kernel Density Estimation (KDE) that serves as an exclusion field which allows constraining the samples not to be too close to each other. This procedure is used to iteratively fill the parameter space uniformly. It is shown that this method provides good space filling properties, especially for high-dimensional cases. \Cref{chap:resample} answers the resampling problematic when QoI have been computed. Two novel methods based on both the Leave-One-Out (LOO) error and the variance of the surrogate model constructed by GP are presented: LOO-$\sigma$ and LOO-\emph{Sobol'}. It is shown that an improvement of the quality (over classical methods) of the surrogate model is guaranteed in high-dimensional cases. Lastly, \cref{chap:visu} introduces novel visualization strategies to visualize uncertainty. The methods are designed to help UQ practitioners by allowing the visualization of both input parameters space and  Quantity of Interest (QoI) on the same canvas by means of a 3-dimensional version of the Kiviat plot (also called spider plot). In case of functional output data, the use of sound is proposed to convey this information. The functional data is analysed using the Highest Density Region (HDR) technique, and distance from the most probable output is mapped through sound.\\
 
% Third part: cases
These methods are applied on various complex configurations in the last part. First of all, \cref{chap:mascaret} compares GP and PC in a hydraulic context with the 1-dimensional code: MASCARET. Both surrogate showed their ability to correctly substitute the simulator with a low number of numerical simulations. In an operational context, replacing a numerical simulator with  a surrogate model is of interest even if the original model is not expensive to evaluate. It allows to produce more frequently, and more accurately, predictions using less expensive hardware. \Cref{chap:ls89} illustrates the benefits of the novel resampling methods on a configuration known in the literature to be difficult to reproduce: the aerothermal flow around the LS89 blade cascade. It is reminded that this is the first uncertainty quantification study performed using Large Eddy Simulation (LES). The code AVBP was used. The difficulty to correctly predict the flow features even for such a high-fidelity simulation, points towards the uncertainty of the boundary conditions. The turbulence intensity and the angle of attack are known to affect the aerothermal flow. This quantity is of great interest for industrial partners to determine the life-cycle of the turbine components. The study allowed to find a spatial sensitivity of the flow to the input parameters. Following, \cref{chap:swirler} presents the use of HDR in order to reduce the dimensionality of the input parameter space for a complex case: a swirler geometry. Using LES and Adaptative Mesh Refinements (AMR) the physic is captured with precision allowing assessing manufacturing dispersion coming from Additive Manufacturing (AM). This study allowed to find that the manufacturing tolerances could be reduced as the impact of additive manufacturing on the QoI was controlled. On an industrial point of view, such analysis can help reduce cost if the precision can be lowered. In~\cref{chap:optim}, the whole toolchain is used to perform a global optimization of the geometry of a heat exchanger---consisting in a ribbed tube. The Efficient Global Optimization (EGO) method was used to lower the computational cost of the study. The obtained surrogate model was then used to perform a UQ analysis. One of the main conclusions from this analysis is that the interaction effect between the pitch of the ribs and the emptiness ratio between the ribs was important. %Finally, \cref{chap:psaap} briefly introduces a work in progress where the new sampling method is being used.\\

% Perspectives
%\newpage

% Models
The perspectives of this work are numerous. They are presented under three subsections: \emph{(i)} surrogate models, \emph{(ii)} uncertainty tools and \emph{(iii)} visualization.

\begin{itemize}
\item \emph{Surrogate models,}\hfill\\
As outlined in the literature review, lots of work has already been devoted to the definition of surrogate models and (re)sampling strategies. New approaches are still being proposed as in~\cite{raissi2016} where a deep learning approach is used to construct deep GP models or in~\cite{Schoebi2015} where PC and GP are used in the same framework.

Considering the dimensionality of the DoE and QoI, a common practice is to use a Proper Orthogonal Decomposition (POD) in order to reduce the dimensionality. While this method produces good results---as presented in this work---, other reduction techniques such as Linear Discriminant Analysis (LDA) or Autoencoder could be investigated~\cite{Goodfellow2016}. It becomes even more challenging to take into account a temporal high dimensional phenomenon. A parallel with Long Short Term Memory (LSTM) methods from deep learning can be made.

Another challenge is to take into account bifurcations in the physics. A bifurcation is a qualitative change of the behaviour of a system resulting from a small smooth change (contrary to an impulsion or Dirac) made to the parameter values. The identification of classes in the input space that lead to bifurcations in the output space can be achieved by expert knowledge or with  machine learning algorithms as proposed in~\cite{Dupuis2018}. Properly separating the input parameter space depending on the physics is a difficult task as the dimensionality increases and the use of automatic tools comes at hand. Still, there are lots of possibilities and finding the right criterion for a given problem still requires some expertise.

Efforts are also concentrated on other aspects such as the cost of construction of the models and how to frame the problem. The optimization of weights is a key step in deep learning algorithms. During the past years, there has been an incentive toward Bayesian optimization to solve this problem. But deep learning makes use of large dataset which is a concern with Gaussian Process. Some effort are being done to improve the scalability~\cite{Wilson2015} (going from $\mathcal{O}(n^3)$ to $\mathcal{O}(n)$). An interest in Bayesian model is foreseen, not only on an applicative point-of-view but also on more theoretical aspects.

\item \emph{Uncertainty tools,}\hfill\\
Concerning the statistical tools to assess uncertainties, from the literature review it was shown that variance based methods have become a standard. Some improvements are still possible---as shown by~\cite{Saltelli2017} where the proposed algorithm requires less samples to converge while being iterative. \emph{Sobol'} indices are still demanding in terms of computational ressources and iterative strategies seek to control the cost of such analysis~\cite{Terraz2017}. However, these methods still require a group of $d + 2$ (with $d$ the number of input parameters) coherent simulations at each iteration to proceed. In case of a large number of parameters and a complex simulation procedure, some sample might fail to return a result. In such case, the procedure is not robust and lots of samples are lost---for 1 sample missing, $d + 1$ samples are lost. Hence the need of robust strategies.

Moving on to other sensitivity indices, more work is expected concerning moment-based indices. There are a lot of metrics that can be used to discriminate the conditional and unconditional probability density functions but no clear recommendation and comparison is available.

There is a wide range of mature solutions available and the challenge, as outlined in~\cite{Saltelli2019}, is toward educating people to use them. Going further, the whole community would benefit from common and updated guidelines on how to handle uncertainties. This is one of the goal of the \emph{COST action} proposal: \emph{Model Auditing and Sensitivity Analysis}. This project seek to federate the community and create tools and teaching materials which would be updated as per the latest research.

\item \emph{Visualization.}\hfill\\
As for the visualization of uncertainty, and more generally the way to convey it in a useful way, it is still an open question. The uncertainty community is lacking interest on the matter as opposed to the \emph{data visualization} community. Interesting work around uncertainty are being published but a link between the two community is still required. Linking uncertainty visualization with deep learning is even more challenging due to the curse-of-dimensionality.

Complex visualization schemes are being proposed but there is still reluctance about these new representations. To paraphrase Sheelagh Carpendale, alphabet itself is an incredibly complex visual system which requires years to learn. Once mastered, it reveals to be a powerful communication tools. Uncertainty visualization is today limited as to be quickly understandable which constrain these innovations to be adopted. Another limit is found with the incentive not to use 3-dimensional visualizations or the need to find equivalent representations in 2-dimensions for printing purpose for instance. In that regard, the literature around 3-dimension and interactive uncertainty is slim not to say inexistant.
\end{itemize}

% Impact
This work has proven the feasibility of UQ in a LES context and more generally in a high computational environment. Distributing Batman openly, and thus all the novelty of this thesis work, has allowed a multitude of international collaborations with both institutional and industrial partners. It alleviate the usage of UQ in complex configuration and also permit a collaboration around a common tool. On a methodological aspect, this work proposes new directions to explore. In every aspect, an influence of the data science community is foreseen. From a bus arrival prediction to impact and risk assessment at the European Commission level, the treatment of uncertainties has become an important societal and engineering preoccupation. This thesis paves the way, not to a less uncertain world, but to a population more aware and empowered with uncertainty.








